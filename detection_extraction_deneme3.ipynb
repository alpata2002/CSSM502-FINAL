{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qJRFk0UJN0I",
        "outputId": "7db099eb-02f4-433c-b254-53b62c07605b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# %%\n",
        "import torch\n",
        "torch.cuda.is_available()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# (Colab only) upload your XLSX\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "dHJ47DH9Q71R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "!pip -q install transformers accelerate evaluate openpyxl scikit-learn pandas numpy torch\n"
      ],
      "metadata": {
        "id": "bVZHNlZbQ-PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# If you haven't installed these in this environment, uncomment\n",
        "!pip -q install \"transformers>=4.38\"\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "S28chX9ORBUu",
        "outputId": "ff13d6b7-9b98-4d75-95e9-5ede1f3bdca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "PATH_XLSX = \"evrensel_isci_sendika_2024_dec2025_clean_fin_uncorrupted_real.xlsx\"  # <-- change this\n",
        "df = pd.read_excel(PATH_XLSX)\n",
        "\n",
        "required_cols = [\"EVENT_RELEVANT\", \"EVENT_ID\", \"title\", \"content\",\"date\"]\n",
        "missing = [c for c in required_cols if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns in XLSX: {missing}\")\n",
        "\n",
        "df.shape, df.columns.tolist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoBcuJtqRDK_",
        "outputId": "d7c5141b-521f-48eb-dff2-b18f0fc74c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9186, 15),\n",
              " ['title',\n",
              "  'date',\n",
              "  'link',\n",
              "  'content',\n",
              "  'EVENT_RELEVANT',\n",
              "  'EVENT_ID',\n",
              "  'Unnamed: 6',\n",
              "  'error',\n",
              "  'Unnamed: 8',\n",
              "  'Unnamed: 9',\n",
              "  'Unnamed: 10',\n",
              "  'Unnamed: 11',\n",
              "  'Unnamed: 12',\n",
              "  650,\n",
              "  160])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "def normalize_text(x):\n",
        "    \"\"\"Light cleanup: keep Turkish characters, remove weird spaces, collapse whitespace.\"\"\"\n",
        "    if pd.isna(x):\n",
        "        return \"\"\n",
        "    x = str(x).replace(\"\\u00A0\", \" \")  # non-breaking space\n",
        "    x = re.sub(r\"\\s+\", \" \", x).strip()\n",
        "    return x\n",
        "\n",
        "df[\"title\"] = df[\"title\"].apply(normalize_text)\n",
        "df[\"content\"] = df[\"content\"].apply(normalize_text)\n",
        "\n",
        "# final text fed into the model\n",
        "df[\"text\"] = (df[\"title\"].astype(str) + \"\\n\\n\" + df[\"content\"].astype(str)).str.strip()\n",
        "df[[\"title\",\"content\",\"text\"]].head(2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "OyuFejx-RFxn",
        "outputId": "9af33768-174b-4a22-99dd-78db10671ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  \\\n",
              "0  Bartın'da Hema'ya ait maden ocağında vagonları...   \n",
              "1                         Bu soygun düzeni değişmeli   \n",
              "\n",
              "                                             content  \\\n",
              "0  Bartın'ın Amasra ilçesindeki Hema Enerji şirke...   \n",
              "1  Pendik Marmara Eğitim ve Araştırma Hastanesind...   \n",
              "\n",
              "                                                text  \n",
              "0  Bartın'da Hema'ya ait maden ocağında vagonları...  \n",
              "1  Bu soygun düzeni değişmeli\\n\\nPendik Marmara E...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-97548fca-d7da-46cf-90f4-ffb6e6d2fd06\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bartın'da Hema'ya ait maden ocağında vagonları...</td>\n",
              "      <td>Bartın'ın Amasra ilçesindeki Hema Enerji şirke...</td>\n",
              "      <td>Bartın'da Hema'ya ait maden ocağında vagonları...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bu soygun düzeni değişmeli</td>\n",
              "      <td>Pendik Marmara Eğitim ve Araştırma Hastanesind...</td>\n",
              "      <td>Bu soygun düzeni değişmeli\\n\\nPendik Marmara E...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97548fca-d7da-46cf-90f4-ffb6e6d2fd06')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-97548fca-d7da-46cf-90f4-ffb6e6d2fd06 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-97548fca-d7da-46cf-90f4-ffb6e6d2fd06');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df[[\\\"title\\\",\\\"content\\\",\\\"text\\\"]]\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Bu soygun d\\u00fczeni de\\u011fi\\u015fmeli\",\n          \"Bart\\u0131n'da Hema'ya ait maden oca\\u011f\\u0131nda vagonlar\\u0131n aras\\u0131na s\\u0131k\\u0131\\u015fan i\\u015f\\u00e7i hayat\\u0131n\\u0131 kaybetti\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Pendik Marmara E\\u011fitim ve Ara\\u015ft\\u0131rma Hastanesinden bir sa\\u011fl\\u0131k emek\\u00e7isi Devlet memurlar\\u0131n\\u0131n \\u015fu s\\u0131ra en b\\u00fcy\\u00fck g\\u00fcndemlerinden birisi vergi dilimi meselesi. \\u00c7\\u00fcnk\\u00fc ekonomik ko\\u015fullar daha da k\\u00f6t\\u00fcye giderken, her g\\u00fcn al\\u0131m g\\u00fcc\\u00fcm\\u00fcz d\\u00fc\\u015ferken, bu ya\\u015fam standartlar\\u0131nda maa\\u015flar\\u0131m\\u0131z\\u0131n insanca ya\\u015fayabilecek bir d\\u00fczeyden uzak olmas\\u0131 yetmezmi\\u015f gibi, artan vergi kesintileriyle kayb\\u0131m\\u0131z daha da b\\u00fcy\\u00fcyor. \\u00d6nceki d\\u00f6nemlerde de \\u00e7ok da adaletli olmayan bu vergi sistemi, bug\\u00fcn\\u00fcn ekonomik ko\\u015fullar\\u0131nda biz emek\\u00e7iler i\\u00e7in tam bir kambur haline geldi. H\\u00fck\\u00fcmetin izledi\\u011fi ekonomi politikalar\\u0131 sonucunda olu\\u015fan a\\u00e7\\u0131k, halihaz\\u0131rda ge\\u00e7inmekte zorlanan biz emek\\u00e7ilerden \\u00e7\\u0131kar\\u0131lacak. \\u00c7\\u00fcnk\\u00fc sermayeye asla dokunamaz, dokunmazlar. Mecliste torba yasalarla sermaye gruplar\\u0131 her t\\u00fcrl\\u00fc ihaleleri al\\u0131rken, devletten ald\\u0131klar\\u0131 taahh\\u00fctlerle kasalar\\u0131n\\u0131 doldururken, milyarl\\u0131k vergi bor\\u00e7lar\\u0131 silinirken, AKP iktidar\\u0131n\\u0131n krizin faturas\\u0131n\\u0131 biz emek\\u00e7ilerden \\u00e7\\u0131karmamas\\u0131n\\u0131 beklemek abes olur. H\\u00fck\\u00fcmetin T\\u00dc\\u0130K\\u2019in yalan enflasyon rakamlar\\u0131yla alaca\\u011f\\u0131m\\u0131z zamlar\\u0131 \\u00fc\\u00e7 kuru\\u015fa d\\u00fc\\u015f\\u00fcrerek k\\u00fcfreder gibi hak etti\\u011fimizden \\u00e7ok uzak rakamlarla \\u00e7al\\u0131\\u015ft\\u0131rmas\\u0131 yetmezmi\\u015f gibi 3 kuru\\u015fluk zamm\\u0131n yar\\u0131s\\u0131na da elimize ge\\u00e7meden el koyuluyor. 2024 b\\u00fct\\u00e7esinin b\\u00fcy\\u00fck bir k\\u0131sm\\u0131 biz emek\\u00e7ilerden toplanaca\\u011f\\u0131 ilan edilmi\\u015fken, \\u2018K\\u00f6t\\u00fc g\\u00fcnleri geride b\\u0131rakt\\u0131k, s\\u0131rada daha k\\u00f6t\\u00fc g\\u00fcnler var\\u2019 repli\\u011fi ger\\u00e7e\\u011fe b\\u00fcr\\u00fcnm\\u00fc\\u015ft\\u00fcr. K\\u00f6t\\u00fc g\\u00fcnleri yaratanlar, bizlere insanca ya\\u015fayacak bir \\u00fccreti \\u00e7ok g\\u00f6renler, televizyonlarda ve gazetelerde \\u015fu kadar zam yapaca\\u011f\\u0131z deyip maa\\u015f elimize ge\\u00e7meden mafya gibi \\u00e7\\u00f6kenler bu soygun d\\u00fczenini de\\u011fi\\u015ftirmeyecekler, tabi bizler h\\u00fck\\u00fcmeti bunu yapmaya zorlamazsak. \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor...\",\n          \"Bart\\u0131n'\\u0131n Amasra il\\u00e7esindeki Hema Enerji \\u015firketine ait maden oca\\u011f\\u0131nda vagonlar\\u0131n aras\\u0131na s\\u0131k\\u0131\\u015fan i\\u015f\\u00e7i Ferdi \\u00d6zg\\u00fcn (33) Ankara'da tedavi g\\u00f6rd\\u00fc\\u011f\\u00fc hastanede 1 hafta sonra hayat\\u0131n\\u0131 kaybetti. \\u0130\\u015f cinayeti, 26 Aral\\u0131k'ta, merkeze ba\\u011fl\\u0131 Tarlaa\\u011fz\\u0131 k\\u00f6y\\u00fcndeki Hema Enerji \\u015firketine ait maden oca\\u011f\\u0131nda meydana geldi. Saat 16.00- 24.00 vardiyas\\u0131nda eksi 500 kotunda \\u00e7al\\u0131\\u015fan Ferdi \\u00d6zg\\u00fcn, k\\u00f6m\\u00fcr galerisine mekanize direkler ta\\u015f\\u0131nd\\u0131\\u011f\\u0131 s\\u0131rada vagonlar\\u0131n aras\\u0131na s\\u0131k\\u0131\\u015ft\\u0131. Mesai arkada\\u015flar\\u0131, durumu 112 Acil Sa\\u011fl\\u0131k ekiplerine haber verdi. \\u0130lk m\\u00fcdahalesinin ard\\u0131ndan ambulansla Bart\\u0131n Devlet Hastanesi'ne kald\\u0131r\\u0131lan \\u00d6zg\\u00fcn, buradan da Ankara Bilkent \\u015eehir Hastanesi'ne sevk edildi. Ferdi \\u00d6zg\\u00fcn, hastanede bu sabah hayat\\u0131n\\u0131 kaybetti.(DHA) \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... Elif Ekin Salt\\u0131k \\u0130\\u00e7erik y\\u00fckleniyor...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Bu soygun d\\u00fczeni de\\u011fi\\u015fmeli\\n\\nPendik Marmara E\\u011fitim ve Ara\\u015ft\\u0131rma Hastanesinden bir sa\\u011fl\\u0131k emek\\u00e7isi Devlet memurlar\\u0131n\\u0131n \\u015fu s\\u0131ra en b\\u00fcy\\u00fck g\\u00fcndemlerinden birisi vergi dilimi meselesi. \\u00c7\\u00fcnk\\u00fc ekonomik ko\\u015fullar daha da k\\u00f6t\\u00fcye giderken, her g\\u00fcn al\\u0131m g\\u00fcc\\u00fcm\\u00fcz d\\u00fc\\u015ferken, bu ya\\u015fam standartlar\\u0131nda maa\\u015flar\\u0131m\\u0131z\\u0131n insanca ya\\u015fayabilecek bir d\\u00fczeyden uzak olmas\\u0131 yetmezmi\\u015f gibi, artan vergi kesintileriyle kayb\\u0131m\\u0131z daha da b\\u00fcy\\u00fcyor. \\u00d6nceki d\\u00f6nemlerde de \\u00e7ok da adaletli olmayan bu vergi sistemi, bug\\u00fcn\\u00fcn ekonomik ko\\u015fullar\\u0131nda biz emek\\u00e7iler i\\u00e7in tam bir kambur haline geldi. H\\u00fck\\u00fcmetin izledi\\u011fi ekonomi politikalar\\u0131 sonucunda olu\\u015fan a\\u00e7\\u0131k, halihaz\\u0131rda ge\\u00e7inmekte zorlanan biz emek\\u00e7ilerden \\u00e7\\u0131kar\\u0131lacak. \\u00c7\\u00fcnk\\u00fc sermayeye asla dokunamaz, dokunmazlar. Mecliste torba yasalarla sermaye gruplar\\u0131 her t\\u00fcrl\\u00fc ihaleleri al\\u0131rken, devletten ald\\u0131klar\\u0131 taahh\\u00fctlerle kasalar\\u0131n\\u0131 doldururken, milyarl\\u0131k vergi bor\\u00e7lar\\u0131 silinirken, AKP iktidar\\u0131n\\u0131n krizin faturas\\u0131n\\u0131 biz emek\\u00e7ilerden \\u00e7\\u0131karmamas\\u0131n\\u0131 beklemek abes olur. H\\u00fck\\u00fcmetin T\\u00dc\\u0130K\\u2019in yalan enflasyon rakamlar\\u0131yla alaca\\u011f\\u0131m\\u0131z zamlar\\u0131 \\u00fc\\u00e7 kuru\\u015fa d\\u00fc\\u015f\\u00fcrerek k\\u00fcfreder gibi hak etti\\u011fimizden \\u00e7ok uzak rakamlarla \\u00e7al\\u0131\\u015ft\\u0131rmas\\u0131 yetmezmi\\u015f gibi 3 kuru\\u015fluk zamm\\u0131n yar\\u0131s\\u0131na da elimize ge\\u00e7meden el koyuluyor. 2024 b\\u00fct\\u00e7esinin b\\u00fcy\\u00fck bir k\\u0131sm\\u0131 biz emek\\u00e7ilerden toplanaca\\u011f\\u0131 ilan edilmi\\u015fken, \\u2018K\\u00f6t\\u00fc g\\u00fcnleri geride b\\u0131rakt\\u0131k, s\\u0131rada daha k\\u00f6t\\u00fc g\\u00fcnler var\\u2019 repli\\u011fi ger\\u00e7e\\u011fe b\\u00fcr\\u00fcnm\\u00fc\\u015ft\\u00fcr. K\\u00f6t\\u00fc g\\u00fcnleri yaratanlar, bizlere insanca ya\\u015fayacak bir \\u00fccreti \\u00e7ok g\\u00f6renler, televizyonlarda ve gazetelerde \\u015fu kadar zam yapaca\\u011f\\u0131z deyip maa\\u015f elimize ge\\u00e7meden mafya gibi \\u00e7\\u00f6kenler bu soygun d\\u00fczenini de\\u011fi\\u015ftirmeyecekler, tabi bizler h\\u00fck\\u00fcmeti bunu yapmaya zorlamazsak. \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor...\",\n          \"Bart\\u0131n'da Hema'ya ait maden oca\\u011f\\u0131nda vagonlar\\u0131n aras\\u0131na s\\u0131k\\u0131\\u015fan i\\u015f\\u00e7i hayat\\u0131n\\u0131 kaybetti\\n\\nBart\\u0131n'\\u0131n Amasra il\\u00e7esindeki Hema Enerji \\u015firketine ait maden oca\\u011f\\u0131nda vagonlar\\u0131n aras\\u0131na s\\u0131k\\u0131\\u015fan i\\u015f\\u00e7i Ferdi \\u00d6zg\\u00fcn (33) Ankara'da tedavi g\\u00f6rd\\u00fc\\u011f\\u00fc hastanede 1 hafta sonra hayat\\u0131n\\u0131 kaybetti. \\u0130\\u015f cinayeti, 26 Aral\\u0131k'ta, merkeze ba\\u011fl\\u0131 Tarlaa\\u011fz\\u0131 k\\u00f6y\\u00fcndeki Hema Enerji \\u015firketine ait maden oca\\u011f\\u0131nda meydana geldi. Saat 16.00- 24.00 vardiyas\\u0131nda eksi 500 kotunda \\u00e7al\\u0131\\u015fan Ferdi \\u00d6zg\\u00fcn, k\\u00f6m\\u00fcr galerisine mekanize direkler ta\\u015f\\u0131nd\\u0131\\u011f\\u0131 s\\u0131rada vagonlar\\u0131n aras\\u0131na s\\u0131k\\u0131\\u015ft\\u0131. Mesai arkada\\u015flar\\u0131, durumu 112 Acil Sa\\u011fl\\u0131k ekiplerine haber verdi. \\u0130lk m\\u00fcdahalesinin ard\\u0131ndan ambulansla Bart\\u0131n Devlet Hastanesi'ne kald\\u0131r\\u0131lan \\u00d6zg\\u00fcn, buradan da Ankara Bilkent \\u015eehir Hastanesi'ne sevk edildi. Ferdi \\u00d6zg\\u00fcn, hastanede bu sabah hayat\\u0131n\\u0131 kaybetti.(DHA) \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... Elif Ekin Salt\\u0131k \\u0130\\u00e7erik y\\u00fckleniyor...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ----------------------------\n",
        "# Cell 4 (fixed) — Define labeled rows (accept 0/1 as floats OR strings)\n",
        "# ----------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def normalize_label(x):\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "\n",
        "    # float/integer case (your current situation: 0.0 / 1.0)\n",
        "    if isinstance(x, (int, np.integer, float, np.floating)):\n",
        "        if x == 0 or x == 0.0:\n",
        "            return 0\n",
        "        if x == 1 or x == 1.0:\n",
        "            return 1\n",
        "        return np.nan\n",
        "\n",
        "    # string case\n",
        "    s = str(x).strip().lower()\n",
        "    if s in {\"0\", \"0.0\", \"no\", \"n\", \"false\"}:\n",
        "        return 0\n",
        "    if s in {\"1\", \"1.0\", \"yes\", \"y\", \"true\"}:\n",
        "        return 1\n",
        "\n",
        "    return np.nan\n",
        "\n",
        "df[\"LABEL_CLEAN\"] = df[\"EVENT_RELEVANT\"].apply(normalize_label)\n",
        "labeled_mask = df[\"LABEL_CLEAN\"].notna()\n",
        "\n",
        "print(\"Total rows:\", len(df))\n",
        "print(\"Labeled rows:\", int(labeled_mask.sum()))\n",
        "df.loc[labeled_mask, [\"EVENT_RELEVANT\",\"LABEL_CLEAN\"]].head(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "hF8N57dtRH3B",
        "outputId": "68c9f2b0-4bc1-40ad-b9a1-fba9ca64e281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows: 9186\n",
            "Labeled rows: 738\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   EVENT_RELEVANT  LABEL_CLEAN\n",
              "0             0.0          0.0\n",
              "1             0.0          0.0\n",
              "2             0.0          0.0\n",
              "3             0.0          0.0\n",
              "4             0.0          0.0\n",
              "5             0.0          0.0\n",
              "6             0.0          0.0\n",
              "7             0.0          0.0\n",
              "8             0.0          0.0\n",
              "9             0.0          0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-092422fd-d660-46f6-b60e-ef4e5c864b70\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EVENT_RELEVANT</th>\n",
              "      <th>LABEL_CLEAN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-092422fd-d660-46f6-b60e-ef4e5c864b70')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-092422fd-d660-46f6-b60e-ef4e5c864b70 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-092422fd-d660-46f6-b60e-ef4e5c864b70');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"EVENT_RELEVANT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LABEL_CLEAN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ----------------------------\n",
        "# Cell 5 — Train/validation split (using LABEL_CLEAN from Cell 4)\n",
        "# ----------------------------\n",
        "\n",
        "# Keep only labeled rows (LABEL_CLEAN is 0/1, NaN otherwise)\n",
        "df_labeled = df.loc[labeled_mask].copy()\n",
        "\n",
        "# This is what the Trainer will learn on\n",
        "df_labeled[\"label\"] = df_labeled[\"LABEL_CLEAN\"].astype(int)\n",
        "\n",
        "# Stratified split so class balance is preserved in train/val\n",
        "train_df, val_df = train_test_split(\n",
        "    df_labeled,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df_labeled[\"label\"]\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(train_df), \"Val size:\", len(val_df))\n",
        "train_df[\"label\"].value_counts(normalize=True), val_df[\"label\"].value_counts(normalize=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL1Y1hfnRJ_1",
        "outputId": "810aaa94-d689-4a63-fe13-72b13025a4d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 590 Val size: 148\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(label\n",
              " 0    0.783051\n",
              " 1    0.216949\n",
              " Name: proportion, dtype: float64,\n",
              " label\n",
              " 0    0.783784\n",
              " 1    0.216216\n",
              " Name: proportion, dtype: float64)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Inspect what EVENT_RELEVANT really looks like\n",
        "s = df[\"EVENT_RELEVANT\"]\n",
        "\n",
        "print(\"dtype:\", s.dtype)\n",
        "print(\"non-null count:\", s.notna().sum())\n",
        "\n",
        "# show a sample of unique raw values (as-is)\n",
        "u = s.dropna().unique()\n",
        "print(\"unique values sample (up to 50):\", u[:50])\n",
        "\n",
        "# show stringified + stripped sample too\n",
        "u_str = pd.Series(u).astype(str).str.strip()\n",
        "print(\"stringified sample (up to 50):\", u_str.head(50).tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tIjHYTNRMfK",
        "outputId": "ac161bcd-3af4-4e28-cc5e-03a238b62022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dtype: float64\n",
            "non-null count: 738\n",
            "unique values sample (up to 50): [0. 1.]\n",
            "stringified sample (up to 50): ['0.0', '1.0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "MODEL_NAME = \"dbmdz/bert-base-turkish-cased\"  # BERTurk\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "MAX_LEN = 384  # 512 is allowed but slower\n",
        "\n",
        "class TextClsDataset(Dataset):\n",
        "    def __init__(self, texts, labels=None, tokenizer=None, max_len=384):\n",
        "        self.texts = list(texts)\n",
        "        self.labels = None if labels is None else list(labels)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        enc = self.tokenizer(\n",
        "            self.texts[i],\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=False\n",
        "        )\n",
        "        item = {k: torch.tensor(v) for k, v in enc.items()}\n",
        "        if self.labels is not None:\n",
        "            item[\"labels\"] = torch.tensor(int(self.labels[i]))\n",
        "        return item\n",
        "\n",
        "train_ds = TextClsDataset(train_df[\"text\"], train_df[\"label\"], tokenizer, MAX_LEN)\n",
        "val_ds   = TextClsDataset(val_df[\"text\"],   val_df[\"label\"],   tokenizer, MAX_LEN)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5aNYnNZRO6a",
        "outputId": "32c787a0-5193-43d3-8062-ae2724a4e7aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(DEVICE)\n",
        "\n",
        "pos = int(train_df[\"label\"].sum())\n",
        "neg = int(len(train_df) - pos)\n",
        "\n",
        "# More weight to positive class if positives are rare\n",
        "class_weights = torch.tensor([1.0, (neg / max(pos, 1))], dtype=torch.float, device=DEVICE)\n",
        "class_weights\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYt2MAEnRQnL",
        "outputId": "5ebcac8b-04df-40e2-e4b7-f730376091ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 3.6094], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ----------------------------\n",
        "# Cell 8 (updated) — Trainer setup + fine-tune (version compatible)\n",
        "# ----------------------------\n",
        "import inspect\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()[:, 1]\n",
        "    return {\n",
        "        \"roc_auc\": float(roc_auc_score(labels, probs)) if len(np.unique(labels)) > 1 else float(\"nan\")\n",
        "    }\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Weighted loss\n",
        "def custom_loss_fn(model, inputs, return_outputs=False):\n",
        "    labels = inputs.pop(\"labels\")\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "    loss = loss_fct(logits, labels)\n",
        "    return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Some transformers versions support compute_loss in Trainer; fallback safely\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        return custom_loss_fn(model, inputs, return_outputs=return_outputs)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"berturk_event_detect\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8 if DEVICE==\"cuda\" else 4,\n",
        "    per_device_eval_batch_size=16 if DEVICE==\"cuda\" else 8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"roc_auc\",\n",
        "    greater_is_better=True,\n",
        "    fp16=(DEVICE==\"cuda\"),\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "sN-f877sRSre",
        "outputId": "192c063f-6c05-4538-b753-78dd8fd3dcee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3381214140.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = WeightedTrainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='222' max='222' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [222/222 00:23, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Roc Auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.713000</td>\n",
              "      <td>0.674348</td>\n",
              "      <td>0.856681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.647700</td>\n",
              "      <td>0.344386</td>\n",
              "      <td>0.939925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.256300</td>\n",
              "      <td>0.312034</td>\n",
              "      <td>0.939655</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=222, training_loss=0.48938286411869636, metrics={'train_runtime': 23.8399, 'train_samples_per_second': 74.245, 'train_steps_per_second': 9.312, 'total_flos': 349279925990400.0, 'train_loss': 0.48938286411869636, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "val_out = trainer.predict(val_ds)\n",
        "val_logits = val_out.predictions\n",
        "val_labels = val_out.label_ids\n",
        "\n",
        "val_probs = torch.softmax(torch.tensor(val_logits), dim=-1).numpy()[:, 1]\n",
        "val_preds = (val_probs >= 0.5).astype(int)\n",
        "\n",
        "print(\"ROC-AUC:\", roc_auc_score(val_labels, val_probs) if len(np.unique(val_labels)) > 1 else \"NA\")\n",
        "print(classification_report(val_labels, val_preds, digits=3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "Mi1D-8EuRURM",
        "outputId": "c482d690-49b8-4c96-a3ee-230afde62984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC: 0.9399245689655173\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.972     0.914     0.942       116\n",
            "           1      0.744     0.906     0.817        32\n",
            "\n",
            "    accuracy                          0.912       148\n",
            "   macro avg      0.858     0.910     0.880       148\n",
            "weighted avg      0.923     0.912     0.915       148\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ---- Run model on full df (prediction) ----\n",
        "full_ds = TextClsDataset(df[\"text\"], labels=None, tokenizer=tokenizer, max_len=MAX_LEN)\n",
        "full_out = trainer.predict(full_ds)\n",
        "full_logits = full_out.predictions\n",
        "full_probs = torch.softmax(torch.tensor(full_logits), dim=-1).numpy()[:, 1]\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "def pick_threshold_min_recall(y_true, y_prob, min_recall=0.85):\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
        "    # precision_recall_curve returns thresholds of length n-1\n",
        "    # recall/precision have length n\n",
        "    best_t = 0.0\n",
        "    best_p = -1.0\n",
        "\n",
        "    for t, p, r in zip(thresholds, precision[1:], recall[1:]):\n",
        "        if r >= min_recall and p > best_p:\n",
        "            best_p = p\n",
        "            best_t = float(t)\n",
        "\n",
        "    # fallback if target recall unattainable\n",
        "    if best_p < 0:\n",
        "        # choose threshold that maximizes recall (usually very low)\n",
        "        best_t = float(thresholds[np.argmax(recall[1:])])\n",
        "\n",
        "    return best_t\n",
        "\n",
        "# --- use on your validation predictions ---\n",
        "BEST_T = pick_threshold_min_recall(val_labels, val_probs, min_recall=0.80)\n",
        "print(\"Chosen threshold (min_recall=0.90):\", BEST_T)\n",
        "\n",
        "df[\"EVENT_PRED\"] = (full_probs >= BEST_T).astype(int)\n",
        "df[\"EVENT_PRED\"].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "ywBVpGDeRWrZ",
        "outputId": "d909cfd8-7e18-4bb7-df49-26047d93b3be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen threshold (min_recall=0.90): 0.5911701917648315\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EVENT_PRED\n",
              "0    7885\n",
              "1    1301\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EVENT_PRED</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1301</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# If needed\n",
        "!pip -q install spacy\n"
      ],
      "metadata": {
        "id": "gVvh6oLORZUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ----------------------------\n",
        "# Cell S2 (updated) — Parse Turkish publication date into PUB_DATE\n",
        "# ----------------------------\n",
        "\n",
        "MONTHS_TR = {\n",
        "    \"ocak\":1, \"şubat\":2, \"subat\":2, \"mart\":3, \"nisan\":4, \"mayıs\":5, \"mayis\":5,\n",
        "    \"haziran\":6, \"temmuz\":7, \"ağustos\":8, \"agustos\":8, \"eylül\":9, \"eylul\":9,\n",
        "    \"ekim\":10, \"kasım\":11, \"kasim\":11, \"aralık\":12, \"aralik\":12\n",
        "}\n",
        "\n",
        "def parse_tr_pub_date(x):\n",
        "    if pd.isna(x):\n",
        "        return pd.NaT\n",
        "    s = str(x).lower().strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "\n",
        "    # common formats:\n",
        "    # \"4 aralık 2025 10:25 Güncelleme: 10:31\"\n",
        "    m = re.search(r\"(\\d{1,2})\\s+([a-zçğıöşü]+)\\s+(\\d{4})\", s)\n",
        "    if not m:\n",
        "        return pd.NaT\n",
        "    day = int(m.group(1))\n",
        "    mon = MONTHS_TR.get(m.group(2), None)\n",
        "    year = int(m.group(3))\n",
        "    if mon is None:\n",
        "        return pd.NaT\n",
        "    try:\n",
        "        return pd.Timestamp(year=year, month=mon, day=day)\n",
        "    except Exception:\n",
        "        return pd.NaT\n",
        "\n",
        "df[\"PUB_DATE\"] = df[\"date\"].apply(parse_tr_pub_date)\n",
        "df[[\"date\",\"PUB_DATE\"]].head(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "7J3v9oiyRcgm",
        "outputId": "62414c69-16b8-4237-e735-472a10a6393c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                date   PUB_DATE\n",
              "0            2 Ocak 2024 11:02 — — Güncelleme: 10:13 2024-01-02\n",
              "1                                  2 Ocak 2024 04:30 2024-01-02\n",
              "2                                  1 Ocak 2024 10:36 2024-01-01\n",
              "3                                  1 Ocak 2024 03:00 2024-01-01\n",
              "4                               31 Aralık 2023 23:07 2023-12-31\n",
              "5                               31 Aralık 2023 15:18 2023-12-31\n",
              "6  31 Aralık 2023 06:34 — — Güncelleme: 1 Ocak 20... 2023-12-31\n",
              "7         31 Aralık 2023 05:34 — — Güncelleme: 11:21 2023-12-31\n",
              "8                               30 Aralık 2023 17:58 2023-12-30\n",
              "9                               30 Aralık 2023 17:40 2023-12-30"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-679f1178-5a01-47a1-a52f-0f7a1440c16d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>PUB_DATE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2 Ocak 2024 11:02 — — Güncelleme: 10:13</td>\n",
              "      <td>2024-01-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2 Ocak 2024 04:30</td>\n",
              "      <td>2024-01-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1 Ocak 2024 10:36</td>\n",
              "      <td>2024-01-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1 Ocak 2024 03:00</td>\n",
              "      <td>2024-01-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31 Aralık 2023 23:07</td>\n",
              "      <td>2023-12-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>31 Aralık 2023 15:18</td>\n",
              "      <td>2023-12-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>31 Aralık 2023 06:34 — — Güncelleme: 1 Ocak 20...</td>\n",
              "      <td>2023-12-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>31 Aralık 2023 05:34 — — Güncelleme: 11:21</td>\n",
              "      <td>2023-12-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>30 Aralık 2023 17:58</td>\n",
              "      <td>2023-12-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>30 Aralık 2023 17:40</td>\n",
              "      <td>2023-12-30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-679f1178-5a01-47a1-a52f-0f7a1440c16d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-679f1178-5a01-47a1-a52f-0f7a1440c16d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-679f1178-5a01-47a1-a52f-0f7a1440c16d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df[[\\\"date\\\",\\\"PUB_DATE\\\"]]\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"30 Aral\\u0131k 2023 17:58\",\n          \"2 Ocak 2024 04:30\",\n          \"31 Aral\\u0131k 2023 15:18\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PUB_DATE\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2023-12-30 00:00:00\",\n        \"max\": \"2024-01-02 00:00:00\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"2024-01-01 00:00:00\",\n          \"2023-12-30 00:00:00\",\n          \"2024-01-02 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ----------------------------\n",
        "# (kept) Phrase mining config (you already had this; kept for minimal change)\n",
        "# We'll still compute ORG_KEYS_FILTERED (baseline org anchor),\n",
        "# but employer anchoring will be rebuilt later (EMPLOYER_KEYS).\n",
        "# ----------------------------\n",
        "from collections import Counter\n",
        "\n",
        "CONTENT_CHARS = 800  # only scan first N chars of content for speed + relevance\n",
        "MIN_FREQ = 3         # keep phrases that appear at least this many times\n",
        "MAX_PHRASES = 5000   # cap to avoid huge ruler\n",
        "NGRAM_MIN = 2\n",
        "NGRAM_MAX = 4\n",
        "\n",
        "# Light stopwords (add more if needed)\n",
        "STOP = set(\"\"\"\n",
        "ve veya ile için gibi üzere da de ki mi mı mu mü\n",
        "işçi işçileri işçilerin işçisine işçileriyle\n",
        "toplu sözleşme toplu iş sözleşmesi sözleşme tis\n",
        "grev greve grevde grevin\n",
        "ücret zam maaş ücretler\n",
        "sendika sendikası sendikadan sendikaya direniş emekçi emekçileri\n",
        "emekçilerin ama çünkü direnişi\n",
        "\"\"\".split())\n",
        "\n",
        "def ngrams(words, n):\n",
        "    for i in range(len(words)-n+1):\n",
        "        yield tuple(words[i:i+n])\n",
        "\n",
        "def tokenize_for_phrases(text):\n",
        "    text = text.replace(\"’\",\"'\").replace(\"`\",\"'\")\n",
        "    text = re.sub(r\"[^\\w\\sçğıöşüÇĞİÖŞÜ'-]\", \" \", text)\n",
        "    words = [w for w in text.split() if w]\n",
        "    return words\n",
        "\n",
        "phrase_counts = Counter()\n",
        "\n",
        "for t, c in zip(df[\"title\"].astype(str), df[\"content\"].astype(str)):\n",
        "    text = (t + \" \" + c[:CONTENT_CHARS]).strip()\n",
        "    words = tokenize_for_phrases(text)\n",
        "\n",
        "    # Keep titlecase-ish tokens for phrase building\n",
        "    # (kept as in your approach)\n",
        "    for n in range(NGRAM_MIN, NGRAM_MAX+1):\n",
        "        for ng in ngrams(words, n):\n",
        "            s = \" \".join(ng)\n",
        "            low = s.lower()\n",
        "            if any(w.lower() in STOP for w in ng):\n",
        "                continue\n",
        "            # heuristic: must contain at least one token starting with uppercase\n",
        "            if not any(w[:1].isupper() for w in ng if w):\n",
        "                continue\n",
        "            phrase_counts[low] += 1\n",
        "\n",
        "firm_phrases = [p for p,cnt in phrase_counts.items() if cnt >= MIN_FREQ]\n",
        "firm_phrases = firm_phrases[:MAX_PHRASES]\n",
        "len(firm_phrases), firm_phrases[:20]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbe-JEhoRfjn",
        "outputId": "c4769277-ab23-420c-d7b2-4d1e81bfeb3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000,\n",
              " [\"kaybetti bartın'ın\",\n",
              "  \"bartın'ın amasra\",\n",
              "  'amasra ilçesindeki',\n",
              "  'ferdi özgün',\n",
              "  'kaybetti i̇ş',\n",
              "  'i̇ş cinayeti',\n",
              "  \"26 aralık'ta\",\n",
              "  'geldi saat',\n",
              "  'saat 16',\n",
              "  'sıkıştı mesai',\n",
              "  'mesai arkadaşları',\n",
              "  '112 acil',\n",
              "  'acil sağlık',\n",
              "  'i̇lk müdahalesinin',\n",
              "  'ambulansla bartın',\n",
              "  'bartın devlet',\n",
              "  \"devlet hastanesi'ne\",\n",
              "  \"hastanesi'ne kaldırılan\",\n",
              "  'bilkent şehir',\n",
              "  \"şehir hastanesi'ne\"])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ----------------------------\n",
        "# Filter ORG_KEYS: remove union/confed/general-institution anchors\n",
        "# ----------------------------\n",
        "\n",
        "BAD_ORG = set([\n",
        "    \"disk\", \"dsk\", \"dİsk\", \"türk iş\", \"turk is\", \"hak iş\", \"kesk\",\n",
        "    \"genel başkanı\", \"genel baskani\", \"genel başkan\", \"sube baskani\", \"şube başkanı\",\n",
        "    \"emek partisi\", \"emep\", \"sosyal guvenlik\", \"devlet hastanesi\",\n",
        "    \"organize sanayi\",\n",
        "    \"belediye\", \"bakanlık\", \"bakanligi\", \"bakan\",\n",
        "    \"haber merkezi\", \"ajans\", \"gazetesi\", \"servisi\"\n",
        "])\n",
        "\n",
        "def normalize_org(s: str) -> str:\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    s = re.sub(r\"[^\\w\\sçğıöşü0-9]\", \"\", s, flags=re.UNICODE)\n",
        "    return s\n",
        "\n",
        "def is_bad_org(k: str) -> bool:\n",
        "    s = normalize_org(k)\n",
        "    if not s:\n",
        "        return True\n",
        "    if s in BAD_ORG:\n",
        "        return True\n",
        "    # “contains” filters\n",
        "    if \"sendika\" in s or \"konfederasyon\" in s or \"şube\" in s or \"sube\" in s:\n",
        "        return True\n",
        "    if \"organize sanayi\" in s:\n",
        "        return True\n",
        "    if \"haber\" in s or \"servis\" in s or \"ajans\" in s:\n",
        "        return True\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "_CqEdlJPRiHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# ----------------------------\n",
        "# Tune knobs\n",
        "# ----------------------------\n",
        "CONTENT_CHARS = 1200      # scan first N chars for speed\n",
        "WIN_CHARS = 220           # +/- window around triggers for mining\n",
        "MAX_WINS = 8              # max windows per doc\n",
        "MIN_FREQ = 3\n",
        "MAX_PHRASES = 5000\n",
        "NGRAM_MIN = 2\n",
        "NGRAM_MAX = 5\n",
        "\n",
        "STOP = set(\"\"\"\n",
        "ve veya ile için gibi üzere da de ki mi mı mu mü\n",
        "işçi işçileri grev grevi direniş direnişi eylem açıklama basın\n",
        "sendika sendikası işçilerden işçilerin mücadele talep sözleşme toplu iş emekçi\n",
        "emekçiler emekçileri örgütlü ama fakat lakin çünkü\n",
        "\"\"\".split())\n",
        "\n",
        "# ---- your trigger regex (use your ALL_TRIG if already defined) ----\n",
        "TRIG_RE = ALL_TRIG  # assumes you already defined ALL_TRIG earlier\n",
        "\n",
        "def clean_for_phrase_mining(text: str) -> str:\n",
        "    text = (text or \"\")\n",
        "    text = text.replace(\"\\u00A0\", \" \").replace(\"’\",\"'\").replace(\"`\",\"'\")\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize_simple(text: str):\n",
        "    return re.findall(r\"[A-Za-zÇĞİÖŞÜçğıöşü0-9]+\", text)\n",
        "\n",
        "def is_titlecase_like(tok: str) -> bool:\n",
        "    if len(tok) < 2:\n",
        "        return False\n",
        "    if tok.isupper() and len(tok) >= 2:\n",
        "        return True\n",
        "    return tok[0].isupper() and any(c.islower() for c in tok[1:])\n",
        "\n",
        "def trigger_windows_text(title: str, content: str, win_chars=WIN_CHARS, max_wins=MAX_WINS):\n",
        "    t = clean_for_phrase_mining(title)\n",
        "    c = clean_for_phrase_mining(content)[:CONTENT_CHARS]\n",
        "    text = f\"{t} {c}\".strip()\n",
        "    wins = []\n",
        "    for m in TRIG_RE.finditer(text):\n",
        "        a = max(0, m.start() - win_chars)\n",
        "        b = min(len(text), m.end() + win_chars)\n",
        "        wins.append(text[a:b])\n",
        "        if len(wins) >= max_wins:\n",
        "            break\n",
        "    # if no trigger found, return empty => no phrase mining (prevents boilerplate mining)\n",
        "    return wins\n",
        "\n",
        "phrase_counts = Counter()\n",
        "\n",
        "for t, c in zip(df[\"title\"].astype(str), df[\"content\"].astype(str)):\n",
        "    wins = trigger_windows_text(t, c)\n",
        "    if not wins:\n",
        "        continue\n",
        "\n",
        "    for w in wins:\n",
        "        toks = tokenize_simple(w)\n",
        "        flags = [is_titlecase_like(tok) for tok in toks]\n",
        "\n",
        "        i = 0\n",
        "        while i < len(toks):\n",
        "            if not flags[i]:\n",
        "                i += 1\n",
        "                continue\n",
        "            j = i\n",
        "            while j < len(toks) and flags[j]:\n",
        "                j += 1\n",
        "\n",
        "            span = toks[i:j]  # consecutive titlecase-like tokens inside trigger window\n",
        "\n",
        "            for n in range(NGRAM_MIN, NGRAM_MAX + 1):\n",
        "                for k in range(0, len(span) - n + 1):\n",
        "                    ng = span[k:k+n]\n",
        "                    ng_l = [w.lower() for w in ng]\n",
        "\n",
        "                    if any(w in STOP for w in ng_l):\n",
        "                        continue\n",
        "                    if all(w.isdigit() for w in ng):\n",
        "                        continue\n",
        "\n",
        "                    phrase = \" \".join(ng)\n",
        "                    phrase_counts[phrase] += 1\n",
        "\n",
        "            i = j\n",
        "\n",
        "candidates = [(p, cnt) for p, cnt in phrase_counts.items() if cnt >= MIN_FREQ]\n",
        "candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Candidate phrases (trigger-window, freq>=MIN_FREQ):\", len(candidates))\n",
        "print(\"Top 30:\")\n",
        "for p, cnt in candidates[:30]:\n",
        "    print(cnt, \"-\", p)\n",
        "\n",
        "firm_phrases = [p for p, cnt in candidates[:MAX_PHRASES]]\n",
        "print(\"firm_phrases kept:\", len(firm_phrases))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "8wv9tzfrv0be",
        "outputId": "de6a888e-ddda-45f4-f3a0-2faf49b7c857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ALL_TRIG' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2049749972.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# ---- your trigger regex (use your ALL_TRIG if already defined) ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mTRIG_RE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_TRIG\u001b[0m  \u001b[0;31m# assumes you already defined ALL_TRIG earlier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_for_phrase_mining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ALL_TRIG' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "wUI_6GZMRlwz",
        "outputId": "4511ed2c-cac4-452f-b91a-675dc3a169fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[38;5;1m✘ No compatible package found for 'tr_core_news_md' (spaCy v3.8.11)\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[E050] Can't find model 'or tr_core_news_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3206354240.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"or tr_core_news_md\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# or tr_core_news_md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_orgs_spacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'or tr_core_news_md'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "ORG_MAX_PER_DOC = 5\n",
        "\n",
        "org_keys = []\n",
        "for t, c in zip(df[\"title\"].astype(str), df[\"content\"].astype(str)):\n",
        "    text = (t + \" \" + c[:CONTENT_CHARS]).strip()\n",
        "    doc = nlp(text)\n",
        "    ents = [normalize_org(ent.text) for ent in doc.ents if ent.label_ == \"ORG\"]\n",
        "    ents = [e for e in ents if e and (not is_bad_org(e))]\n",
        "    # de-dup preserve order\n",
        "    seen = set()\n",
        "    dedup = []\n",
        "    for e in ents:\n",
        "        if e not in seen:\n",
        "            seen.add(e)\n",
        "            dedup.append(e)\n",
        "    org_keys.append(dedup[:ORG_MAX_PER_DOC])\n",
        "\n",
        "df[\"ORG_KEYS_FILTERED\"] = org_keys\n",
        "df[[\"title\",\"ORG_KEYS_FILTERED\"]].head(10)\n"
      ],
      "metadata": {
        "id": "T8RwsEO3RnqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ----------------------------\n",
        "# Cell: Narrow to Collective Bargaining / Wage-related strikes\n",
        "# ----------------------------\n",
        "\n",
        "KEEP_PATTERNS = [\n",
        "    r\"\\btoplu sözleşme\\b\", r\"\\btis\\b\", r\"\\btoplu iş sözleşmesi\\b\",\n",
        "    r\"\\bücret\\b\", r\"\\bzam\\b\", r\"\\bmaaş\\b\", r\"\\bücret artış\\b\",\n",
        "    r\"\\bpazarlık\\b\", r\"\\bgörüşme\\b\", r\"\\bmüzakere\\b\",\n",
        "    r\"\\bgrev\\b\", r\"\\bgrevde\\b\", r\"\\bgreve çıktı\\b\",\n",
        "    r\"\\bsözleşme\\s*sürüyor\\b\", r\"\\banlaşma\\b\", r\"\\bdireniş\\b\"\n",
        "]\n",
        "\n",
        "DROP_PATTERNS = [\n",
        "    r\"\\bişten çıkar\", r\"\\bişten at\", r\"\\bişten çıkarıl\", r\"\\bkovuldu\\b\",\n",
        "    r\"\\bsendikalaş\", r\"\\bsendika üye\", r\"\\bsendika üyeli\",\n",
        "    r\"\\biş kaz\", r\"\\bölüm\\b.*\\b(ölüm|yaralı)\\b\", r\"\\bgöçük\\b\",\n",
        "    r\"\\bgözalt\", r\"\\btutuk\", r\"\\bdava\\b\", r\"\\bmahkeme\\b\",\n",
        "    r\"\\bziyaret\\b\", r\"\\bdayanışma\\b\", r\"\\banma\\b\", r\"\\bbasın açıklama\\b\",\n",
        "    r\"\\bsendikalaşma\\b\"\n",
        "]\n",
        "\n",
        "keep_re = re.compile(\"|\".join(KEEP_PATTERNS), flags=re.IGNORECASE)\n",
        "drop_re = re.compile(\"|\".join(DROP_PATTERNS), flags=re.IGNORECASE)\n",
        "\n",
        "def is_cb_wage_article(title, content):\n",
        "    text = f\"{title} {content}\"\n",
        "    if drop_re.search(text):\n",
        "        return False\n",
        "    return bool(keep_re.search(text))\n",
        "\n",
        "df[\"IS_CB_WAGE\"] = df.apply(lambda r: is_cb_wage_article(r[\"title\"], r[\"content\"]), axis=1)\n",
        "\n",
        "# Start from EVENT_PRED (general)\n",
        "df[\"EVENT_PRED_CB\"] = 0\n",
        "mask_rel = df[\"EVENT_PRED\"] == 1\n",
        "df[\"EVENT_PRED_CB\"] = ((df[\"EVENT_PRED\"] == 1) & (df[\"IS_CB_WAGE\"])).astype(int)\n",
        "\n",
        "#df.loc[mask_pred, \"EVENT_PRED_CB\"] = df.loc[mask_pred, \"text\"].apply(lambda x: int(bool(KEEP_RE.search(str(x)))))\n",
        "print(\"Pred-relevant (all):\", int((df[\"EVENT_PRED\"]==1).sum()))\n",
        "print(\"Pred-relevant (CB/Wage):\", int((df[\"EVENT_PRED_CB\"]==1).sum()))\n",
        "df[\"EVENT_PRED_CB\"].value_counts()\n"
      ],
      "metadata": {
        "id": "coSARTNjRrYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def title_tokens(title):\n",
        "    return re.findall(r\"[A-Za-zÇĞİÖŞÜçğıöşü]+\", str(title).lower())\n",
        "\n",
        "mask_cb = df[\"EVENT_PRED_CB\"] == 1\n",
        "N = int(mask_cb.sum())\n",
        "\n",
        "dfreq = Counter()\n",
        "for t in df.loc[mask_cb, \"title\"]:\n",
        "    seen = set(w for w in title_tokens(t) if len(w) >= 4)\n",
        "    for w in seen:\n",
        "        dfreq[w] += 1\n",
        "\n",
        "# keep tokens that appear in <= 5% of CB titles\n",
        "RARE_MAX = max(1, int(0.05 * max(N, 1)))\n",
        "rare_tokens = {w for w,cnt in dfreq.items() if cnt <= RARE_MAX}\n",
        "len(rare_tokens), list(sorted(list(rare_tokens)))[:30]\n"
      ],
      "metadata": {
        "id": "hSTxyDzTRsBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# ============================================================\n",
        "# UPDATED HELPERS (NEW)\n",
        "# These were missing in your notebook but are required for a \"full code\" version:\n",
        "# - encode_texts()  -> embedding encoder for linking\n",
        "# - connected_components() -> union-find\n",
        "# ============================================================\n"
      ],
      "metadata": {
        "id": "_C7gSan_Rv7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "from transformers import AutoModel\n",
        "\n",
        "# Sentence embedding encoder (mean pooling + L2 norm)\n",
        "# Minimal and stable: uses BERTurk backbone (same family as your classifier).\n",
        "_enc_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "_enc_model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "_enc_model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_texts(texts, batch_size=8, max_len=256):\n",
        "    vecs = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = [str(t) for t in texts[i:i+batch_size]]\n",
        "        enc = _enc_tokenizer(\n",
        "            batch,\n",
        "            truncation=True,\n",
        "            max_length=max_len,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        out = _enc_model(**enc)\n",
        "        last = out.last_hidden_state  # (B, T, H)\n",
        "        mask = enc[\"attention_mask\"].unsqueeze(-1).float()  # (B, T, 1)\n",
        "\n",
        "        pooled = (last * mask).sum(dim=1) / torch.clamp(mask.sum(dim=1), min=1e-6)\n",
        "        pooled = torch.nn.functional.normalize(pooled, p=2, dim=1)  # unit length\n",
        "\n",
        "        vecs.append(pooled.detach().cpu().numpy())\n",
        "    return np.vstack(vecs) if vecs else np.zeros((0, 768), dtype=np.float32)\n",
        "\n",
        "def connected_components(n, edges):\n",
        "    # Union-find\n",
        "    parent = list(range(n))\n",
        "    rank = [0]*n\n",
        "\n",
        "    def find(x):\n",
        "        while parent[x] != x:\n",
        "            parent[x] = parent[parent[x]]\n",
        "            x = parent[x]\n",
        "        return x\n",
        "\n",
        "    def union(a,b):\n",
        "        ra, rb = find(a), find(b)\n",
        "        if ra == rb:\n",
        "            return\n",
        "        if rank[ra] < rank[rb]:\n",
        "            parent[ra] = rb\n",
        "        elif rank[ra] > rank[rb]:\n",
        "            parent[rb] = ra\n",
        "        else:\n",
        "            parent[rb] = ra\n",
        "            rank[ra] += 1\n",
        "\n",
        "    for a,b in edges:\n",
        "        union(a,b)\n",
        "\n",
        "    comps = defaultdict(list)\n",
        "    for i in range(n):\n",
        "        comps[find(i)].append(i)\n",
        "\n",
        "    return list(comps.values())\n"
      ],
      "metadata": {
        "id": "wB17jWluRwaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# ============================================================\n",
        "# UPDATED LINKING (DENEME1 improvement)\n",
        "# - Uses EMPLOYER_KEYS instead of ORG_KEYS_FILTERED as the primary anchor\n",
        "# - Uses TIME DECAY (no hard cutoff) to allow long strikes\n",
        "# - Keeps your \"rare title token overlap\" gate\n",
        "# ============================================================\n"
      ],
      "metadata": {
        "id": "Xke7y-6iRyPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "def rare_title_tokens(title, min_len=4):\n",
        "    toks = re.findall(r\"[A-Za-zÇĞİÖŞÜçğıöşü]+\", str(title).lower())\n",
        "    return {t for t in toks if len(t) >= min_len and t in rare_tokens}\n",
        "\n",
        "def _safe_days_diff(d1, d2):\n",
        "    if pd.isna(d1) or pd.isna(d2):\n",
        "        return None\n",
        "    try:\n",
        "        return abs((pd.to_datetime(d1) - pd.to_datetime(d2)).days)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _time_decay(days_diff, tau_days=25.0):\n",
        "    if days_diff is None:\n",
        "        return 1.0\n",
        "    return math.exp(-float(days_diff) / float(tau_days))\n",
        "\n",
        "def assign_event_ids_hybrid(\n",
        "    df_in,\n",
        "    rel_flag_col=\"EVENT_PRED_CB\",\n",
        "    date_col=\"PUB_DATE\",\n",
        "    employer_col=\"EMPLOYER_KEYS\",\n",
        "    sim_short=0.94,\n",
        "    sim_emp=0.965,\n",
        "    tau_days=25.0,\n",
        "    max_rel=6000,\n",
        "    EMP_MAX_BUCKET=40,\n",
        "    TITLE_OVERLAP_K=2,\n",
        "    # --- NEW knobs (minimal additions) ---\n",
        "    MIN_CLUSTER_SIZE=2,                 # 2 kills phantom EVs; set to 1 if you want singletons\n",
        "    DROP_EMPTY_EMPLOYER_CLUSTERS=True   # drops clusters where nobody has employer keys\n",
        "):\n",
        "    df_out = df_in.copy()\n",
        "\n",
        "    # Normalize EVENT_ID\n",
        "    df_out[\"EVENT_ID\"] = df_out[\"EVENT_ID\"].where(df_out[\"EVENT_ID\"].notna(), np.nan)\n",
        "    df_out[\"EVENT_ID\"] = df_out[\"EVENT_ID\"].apply(lambda x: str(x).strip() if not pd.isna(x) else np.nan)\n",
        "\n",
        "    if rel_flag_col not in df_out.columns:\n",
        "        raise ValueError(f\"Missing column {rel_flag_col}. Did you create EVENT_PRED_CB first?\")\n",
        "\n",
        "    rel = df_out[df_out[rel_flag_col] == 1].copy()\n",
        "    if len(rel) == 0:\n",
        "        print(f\"No rows where {rel_flag_col} == 1. Nothing to link.\")\n",
        "        return df_out\n",
        "\n",
        "    if len(rel) > max_rel:\n",
        "        print(f\"Too many relevant rows ({len(rel)}). Increase threshold or lower corpus slice.\")\n",
        "        return df_out\n",
        "\n",
        "    # Stable ordering\n",
        "    rel[\"_has_date\"] = rel[date_col].notna()\n",
        "    rel = rel.sort_values(by=[date_col, \"_has_date\"], ascending=[True, False])\n",
        "\n",
        "    idx = list(rel.index)\n",
        "    n = len(idx)\n",
        "\n",
        "    # Embeddings once\n",
        "    texts = rel[\"text\"].tolist()\n",
        "    E = encode_texts(texts, batch_size=32 if DEVICE == \"cuda\" else 8, max_len=256)\n",
        "\n",
        "    # Precompute title token sets\n",
        "    title_tok_sets = [rare_title_tokens(t) for t in rel[\"title\"].tolist()]\n",
        "    rel_dates = rel[date_col].tolist()\n",
        "\n",
        "    # Inverted index for EMPLOYER keys\n",
        "    emp_to_pos = defaultdict(list)\n",
        "    rel_emp_lists = rel[employer_col].tolist() if employer_col in rel.columns else [None] * n\n",
        "    for pos, keys in enumerate(rel_emp_lists):\n",
        "        for k in (keys or []):\n",
        "            if k:\n",
        "                emp_to_pos[k].append(pos)\n",
        "\n",
        "    edges = set()\n",
        "\n",
        "    # A) Employer-bucket edges (high precision)\n",
        "    for emp, positions in emp_to_pos.items():\n",
        "        if len(positions) <= 1:\n",
        "            continue\n",
        "        if len(positions) > EMP_MAX_BUCKET:\n",
        "            continue\n",
        "\n",
        "        for a_i in range(len(positions)):\n",
        "            i = positions[a_i]\n",
        "            for a_j in range(a_i + 1, len(positions)):\n",
        "                j = positions[a_j]\n",
        "                base_sim = float(np.dot(E[i], E[j]))\n",
        "                dd = _safe_days_diff(rel_dates[i], rel_dates[j])\n",
        "                sim = base_sim * _time_decay(dd, tau_days=tau_days)\n",
        "\n",
        "                if sim >= sim_emp:\n",
        "                    if len(title_tok_sets[i] & title_tok_sets[j]) >= TITLE_OVERLAP_K:\n",
        "                        edges.add((min(i, j), max(i, j)))\n",
        "\n",
        "    # B) Non-employer edges (still allowed, but harder)\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            base_sim = float(np.dot(E[i], E[j]))\n",
        "            dd = _safe_days_diff(rel_dates[i], rel_dates[j])\n",
        "            sim = base_sim * _time_decay(dd, tau_days=tau_days)\n",
        "\n",
        "            if sim >= sim_short:\n",
        "                if len(title_tok_sets[i] & title_tok_sets[j]) >= TITLE_OVERLAP_K:\n",
        "                    edges.add((i, j))\n",
        "\n",
        "    comps = connected_components(n, list(edges))\n",
        "    clusters = [[idx[pos] for pos in comp] for comp in comps]\n",
        "\n",
        "    # --- NEW: filter phantom clusters BEFORE assigning EV IDs ---\n",
        "    filtered = []\n",
        "    for cluster in clusters:\n",
        "        # 1) must have at least MIN_CLUSTER_SIZE articles (since rel already filtered, this is cluster size)\n",
        "        if len(cluster) < MIN_CLUSTER_SIZE:\n",
        "            continue\n",
        "\n",
        "        # 2) optionally drop clusters where everyone has empty employer keys\n",
        "        if DROP_EMPTY_EMPLOYER_CLUSTERS and employer_col in df_out.columns:\n",
        "            emp_nonempty = df_out.loc[cluster, employer_col].apply(lambda x: isinstance(x, list) and len(x) > 0).sum()\n",
        "            if int(emp_nonempty) == 0:\n",
        "                continue\n",
        "\n",
        "        filtered.append(cluster)\n",
        "\n",
        "    clusters = filtered\n",
        "    # -----------------------------------------------------------\n",
        "\n",
        "    # Assign EVENT_IDs\n",
        "    new_counter = 1\n",
        "    for cluster in clusters:\n",
        "        existing = df_out.loc[cluster, \"EVENT_ID\"].dropna()\n",
        "        if len(existing) > 0:\n",
        "            chosen = existing.value_counts().idxmax()\n",
        "        else:\n",
        "            chosen = f\"EV{new_counter:06d}\"\n",
        "            new_counter += 1\n",
        "        df_out.loc[cluster, \"EVENT_ID\"] = chosen\n",
        "\n",
        "    return df_out\n"
      ],
      "metadata": {
        "id": "nzl3p__VR0Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# ============================================================\n",
        "# UPDATED EMPLOYER EXTRACTION (DENEME1 improvement)\n",
        "# Minimal change philosophy:\n",
        "# - Keep ORG_KEYS_FILTERED as a baseline\n",
        "# - Rebuild EMPLOYER_KEYS with trigger-window + canonicalization + alias\n",
        "# - Keep unions separately (UNION_KEYS)\n",
        "# ============================================================\n"
      ],
      "metadata": {
        "id": "WQ29KytSR2Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Canonicalization + alias (grow alias dict over time)\n",
        "LEGAL_RE = re.compile(\n",
        "    r\"\\b(a\\.?ş\\.?|aş|anonim|şirketi|şti|ltd|limited|inc|corp|co|holding|sanayi|ticaret|ve)\\b\",\n",
        "    flags=re.IGNORECASE\n",
        ")\n",
        "GENERIC_TAIL_RE = re.compile(\n",
        "    r\"\\b(fabrika(sı|si|da|de|nda|nde)?|işletme(si|de|da|nde|nda)?|tesis(leri|de|da|nde|nda)?|işyeri(nde|ne|ni)?)\\b\",\n",
        "    flags=re.IGNORECASE\n",
        ")\n",
        "\n",
        "def _fold_tr(s: str) -> str:\n",
        "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
        "        return \"\"\n",
        "    s = str(s).strip().lower()\n",
        "    s = s.replace(\"’\", \"'\").replace(\"`\", \"'\")\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def canonical_employer(name: str) -> str:\n",
        "    s = _fold_tr(name)\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    s = re.sub(r\"[^\\w\\sçğıöşü0-9'-]\", \" \", s, flags=re.UNICODE)\n",
        "    s = LEGAL_RE.sub(\" \", s)\n",
        "    s = GENERIC_TAIL_RE.sub(\" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# %%\n",
        "def recanon_list(xs):\n",
        "    if xs is None or (isinstance(xs, float) and pd.isna(xs)):\n",
        "        return []\n",
        "    if not isinstance(xs, list):\n",
        "        # if it came back as a string like \"a; b\", split\n",
        "        xs = [p.strip() for p in str(xs).split(\";\") if p.strip()]\n",
        "    out = []\n",
        "    seen = set()\n",
        "    for x in xs:\n",
        "        k = canonical_employer(x)   # NO ALIASES\n",
        "        if not k:\n",
        "            continue\n",
        "        if k not in seen:\n",
        "            seen.add(k)\n",
        "            out.append(k)\n",
        "    return out\n",
        "\n",
        "# Make ORG_KEYS_FILTERED speak the same \"language\" as EMPLOYER_KEYS\n",
        "df[\"ORG_KEYS_FILTERED_CAN\"] = df[\"ORG_KEYS_FILTERED\"].apply(recanon_list)\n",
        "\n",
        "# If you want, overwrite the old one (simplest):\n",
        "df[\"ORG_KEYS_FILTERED\"] = df[\"ORG_KEYS_FILTERED_CAN\"]\n",
        "df.drop(columns=[\"ORG_KEYS_FILTERED_CAN\"], inplace=True)\n",
        "\n",
        "print(df[\"ORG_KEYS_FILTERED\"].head())\n",
        "\n",
        "\n",
        "#EMPLOYER_ALIASES = {\n",
        "    # \"schneider elektrik\": \"schneider electric\",\n",
        "    # \"purmo metal\": \"purmo\",\n",
        "    #}\n",
        "def apply_employer_alias(k: str) -> str:\n",
        "    #k = canonical_employer(k)\n",
        "    return canonical_employer(k)\n",
        "\n",
        "# ----------------------------\n",
        "# Triggers + patterns\n",
        "# IMPORTANT: join alternatives with | inside the SAME group\n",
        "# ----------------------------\n",
        "ALL_TRIG = re.compile(\n",
        "    r\"(grev(e)? çıktı|grev başladı|iş bırak(tı|ıyor)|üretim(i)? durdur|\"\n",
        "    r\"grevde|grev sürüyor|(\\d+)\\.? ?gün(ü)?nde|\"\n",
        "    r\"anlaşma sağlandı|grev bitti|grev sona erdi|protokol imzalandı|\"\n",
        "    r\"kabul edildi|reddedildi|imza(landı)?|uzlaş(ma)?|anlaş(ma)?|\"\n",
        "    r\"direnişi sürüyor|direniş(i)?)\"\n",
        "    r\"kazanımla sonuçlandı\",\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "P_FACILITY = re.compile(\n",
        "    r\"(?P<name>[A-ZÇĞİÖŞÜ][\\w’'-.]+(?:\\s+[A-ZÇĞİÖŞÜ][\\w’'-.]+){0,6})\\s+\"\n",
        "    r\"(fabrika(sı|sinda|sında|da|de|nda|nde)?|işyeri(nde|ne|ni)?|tesis(leri|de|da|nde|nda)?)\",\n",
        "    re.UNICODE\n",
        ")\n",
        "P_WORKERS = re.compile(\n",
        "    r\"(?P<name>[A-ZÇĞİÖŞÜ][\\w’'-.]+(?:\\s+[A-ZÇĞİÖŞÜ][\\w’'-.]+){0,6})\\s+işçi(leri|ler)?\",\n",
        "    re.UNICODE\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Union + person drop lists\n",
        "# ----------------------------\n",
        "UNION_TERMS = {\n",
        "    \"disk\", \"türk iş\", \"turk is\", \"hak iş\", \"kesk\",\n",
        "    \"türk metal\", \"metal iş\", \"birleşik metal iş\", \"birlesik metal is\",\n",
        "    \"genel iş\", \"petrol iş\", \"tek gıda iş\", \"tek gida is\",\n",
        "    \"tüm bel sen\", \"tum bel sen\", \"tüm bel-sen\", \"tum bel-sen\",\n",
        "    \"birtek sen\",\n",
        "}\n",
        "\n",
        "# NOTE: best practice is to store *names only* (not role phrases),\n",
        "# because role phrases are already handled by DROP_IF_CONTAINS.\n",
        "PERSON_TERMS = {\n",
        "   \"hilal tok istanbul\", \"ramis sağlam içerik\", \"genel başkan özkan atar\",\n",
        "   \"özkan atar\", \"bölge temsilcisi hayrettin çakmak\", \"izbb başkanı cemil tugay\",\n",
        "   \"hilal tok\", \"hasret gültekin kozan\", \"sevda karaca\", \"iskender bayhan\"\n",
        "}\n",
        "\n",
        "DROP_IF_CONTAINS = [\n",
        "    # roles / bylines / org-like non-employers\n",
        "    \"genel başkan\", \"genel baskan\", \"başkanı\", \"baskani\", \"şube başkanı\", \"sube baskani\",\n",
        "    \"belediyesi\", \"çalışan\", \"asgari\", \"vade\",\n",
        "    \"partisi\", \"chp\", \"akp\", \"mhp\", \"emep\",\n",
        "    \"servisi\", \"muhabir\", \"haber merkezi\", \"gazetesi\", \"ajans\", \"ücret\", \"zam\",\n",
        "    \"tis\", \"sözleşme\", \"grev\", \"işçi\"\n",
        "]\n",
        "\n",
        "# Add these near your DROP lists\n",
        "STARTER_BAD = {\n",
        "    \"yapılan\", \"yapilan\", \"önünde\", \"onunde\", \"grevdeki\",\n",
        "    \"sosyal\", \"sabah\", \"aynı\", \"ayni\", \"en\", \"daha\", \"gece\",\n",
        "    \"temel\", \"kamu\", \"türkiye\", \"turkiye\", \"açıklamada\", \"aciklamada\",\n",
        "    \"geçtiğimiz\", \"gectigimiz\", \"günlerde\", \"gunlerde\", \"çok\"\n",
        "}\n",
        "\n",
        "# If a key is basically a sentence starter phrase, reject\n",
        "def looks_like_sentence_starter(k: str) -> bool:\n",
        "    kk = _fold_tr(k)\n",
        "    if not kk:\n",
        "        return True\n",
        "    toks = kk.split()\n",
        "    if not toks:\n",
        "        return True\n",
        "    # reject if first token is a known starter\n",
        "    if toks[0] in STARTER_BAD:\n",
        "        return True\n",
        "    # reject very generic bigrams like \"yapılan açıklamada\"\n",
        "    if len(toks) >= 2 and (toks[0] in STARTER_BAD or toks[1] in STARTER_BAD):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Morphology-ish heuristics that are very common in junk:\n",
        "# -deki/-daki adjectives, -inde/-ında locative phrases, etc.\n",
        "def looks_like_non_entity_phrase(k: str) -> bool:\n",
        "    kk = _fold_tr(k)\n",
        "    if not kk:\n",
        "        return True\n",
        "    if kk.endswith((\"deki\", \"daki\", \"teki\", \"taki\")):\n",
        "        return True\n",
        "    if any(w in kk for w in [\"açıklamada\", \"aciklamada\", \"saatlerinde\", \"günlerde\", \"gunlerde\"]):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def looks_like_union(k: str) -> bool:\n",
        "    return _fold_tr(k) in UNION_TERMS\n",
        "\n",
        "def looks_like_person(k: str) -> bool:\n",
        "    # exact name match OR contains a known name (covers \"X Y içerik\" cases)\n",
        "    kk = _fold_tr(k)\n",
        "    if not kk:\n",
        "        return False\n",
        "    if kk in PERSON_TERMS:\n",
        "        return True\n",
        "    return any(name in kk for name in PERSON_TERMS)\n",
        "\n",
        "def should_drop_orgish(k: str) -> bool:\n",
        "    kk = _fold_tr(k)\n",
        "    if not kk or len(kk) < 3:\n",
        "        return True\n",
        "    if looks_like_union(kk) or looks_like_person(kk):\n",
        "        return True\n",
        "    if looks_like_sentence_starter(kk) or looks_like_non_entity_phrase(kk):\n",
        "        return True\n",
        "    for bad in DROP_IF_CONTAINS:\n",
        "        if bad in kk:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _as_list(x):\n",
        "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
        "        return []\n",
        "    if isinstance(x, list):\n",
        "        return [str(i).strip() for i in x if str(i).strip()]\n",
        "    s = str(x).strip()\n",
        "    if not s:\n",
        "        return []\n",
        "    return [p.strip() for p in re.split(r\"[;|,]\\s*\", s) if p.strip()]\n",
        "\n",
        "def trigger_windows(text: str, window_chars: int = 180, max_wins: int = 6):\n",
        "    t = text or \"\"\n",
        "    wins = []\n",
        "    for m in ALL_TRIG.finditer(t):\n",
        "        a = max(0, m.start() - window_chars)\n",
        "        b = min(len(t), m.end() + window_chars)\n",
        "        wins.append(t[a:b])\n",
        "        if len(wins) >= max_wins:\n",
        "            break\n",
        "    return wins\n",
        "\n",
        "def extract_employer_candidates_from_text(title: str, content: str, org_list=None):\n",
        "    \"\"\"\n",
        "    Only keep employer candidates that:\n",
        "    1) appear near strike triggers\n",
        "    2) AND already exist in ORG_KEYS_FILTERED\n",
        "\n",
        "    This prevents sentence fragments from becoming employers.\n",
        "    \"\"\"\n",
        "    txt = (str(title or \"\") + \" \" + str(content or \"\")).strip()\n",
        "    wins = trigger_windows(txt, window_chars=180, max_wins=6)\n",
        "\n",
        "    emp, uni = [], []\n",
        "\n",
        "    org_list = org_list or []\n",
        "    org_can = {apply_employer_alias(o) for o in org_list}\n",
        "\n",
        "    for w in wins:\n",
        "        for pat in (P_FACILITY, P_WORKERS):\n",
        "            for m in pat.finditer(w):\n",
        "                raw = m.group(\"name\").strip()\n",
        "                k = apply_employer_alias(raw)\n",
        "                if not k:\n",
        "                    continue\n",
        "\n",
        "                # HARD GATE: must already be an ORG\n",
        "                if k not in org_can:\n",
        "                    continue\n",
        "\n",
        "                if looks_like_union(k):\n",
        "                    uni.append(k)\n",
        "                    continue\n",
        "                if looks_like_person(k):\n",
        "                    continue\n",
        "                if should_drop_orgish(k):\n",
        "                    continue\n",
        "\n",
        "                emp.append(k)\n",
        "\n",
        "    def dedup(seq):\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for x in seq:\n",
        "            if x and x not in seen:\n",
        "                seen.add(x)\n",
        "                out.append(x)\n",
        "        return out\n",
        "\n",
        "    return dedup(emp), dedup(uni)\n",
        "\n",
        "\n",
        "\n",
        "def build_employer_and_union_keys(df_in, org_col=\"ORG_KEYS_FILTERED\", max_emp_per_doc=3):\n",
        "    df_out = df_in.copy()\n",
        "    employer_keys = []\n",
        "    union_keys = []\n",
        "\n",
        "    for t, c, orgs in zip(\n",
        "        df_out[\"title\"].astype(str),\n",
        "        df_out[\"content\"].astype(str),\n",
        "        df_out[org_col] if org_col in df_out.columns else [None] * len(df_out)\n",
        "    ):\n",
        "        org_list = _as_list(orgs)\n",
        "\n",
        "        emp_from_org, uni_from_org = [], []\n",
        "        for o in org_list:\n",
        "            k = apply_employer_alias(o)\n",
        "            if not k:\n",
        "                continue\n",
        "\n",
        "            if looks_like_union(k):\n",
        "                uni_from_org.append(k)\n",
        "                continue\n",
        "            if looks_like_person(k):\n",
        "                continue\n",
        "            if should_drop_orgish(k):\n",
        "                continue\n",
        "\n",
        "            emp_from_org.append(k)\n",
        "\n",
        "        emp_mined, uni_mined = extract_employer_candidates_from_text(t, c, org_list)\n",
        "\n",
        "        def dedup(seq):\n",
        "            seen = set()\n",
        "            out = []\n",
        "            for x in seq:\n",
        "                if x and x not in seen:\n",
        "                    seen.add(x)\n",
        "                    out.append(x)\n",
        "            return out\n",
        "\n",
        "        emp = dedup(emp_from_org + emp_mined)[:max_emp_per_doc]\n",
        "        uni = dedup(uni_from_org + uni_mined)\n",
        "\n",
        "        employer_keys.append(emp)\n",
        "        union_keys.append(uni)\n",
        "\n",
        "    df_out[\"EMPLOYER_KEYS\"] = employer_keys\n",
        "    df_out[\"UNION_KEYS\"] = union_keys\n",
        "    return df_out\n"
      ],
      "metadata": {
        "id": "k_v5j-ULR48z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ---- Run linking FIRST (wave-level), then rebuild EMPLOYER_KEYS, then firm-splitting ----\n",
        "\n",
        "df_linked = df.copy()\n",
        "\n",
        "# placeholder (EMPLOYER_KEYS used by linker; build now so linker can use it)\n",
        "df_linked = build_employer_and_union_keys(df_linked, org_col=\"ORG_KEYS_FILTERED\", max_emp_per_doc=3)\n",
        "\n",
        "df_linked = assign_event_ids_hybrid(\n",
        "    df_linked,\n",
        "    rel_flag_col=\"EVENT_PRED_CB\",\n",
        "    date_col=\"PUB_DATE\",\n",
        "    employer_col=\"EMPLOYER_KEYS\",\n",
        "    sim_short=0.94,\n",
        "    sim_emp=0.965,\n",
        "    tau_days=25.0,     # key knob: higher = more tolerant for long strikes\n",
        "    EMP_MAX_BUCKET=40,\n",
        "    TITLE_OVERLAP_K=2\n",
        ")\n",
        "\n",
        "mask_rel = df_linked[\"EVENT_PRED_CB\"] == 1\n",
        "print(\"Pred-relevant (CB/Wage):\", int(mask_rel.sum()))\n",
        "print(\"Unique EVENT_ID among predicted relevant:\", int(df_linked.loc[mask_rel, \"EVENT_ID\"].nunique()))\n",
        "df_linked.loc[mask_rel].groupby(\"EVENT_ID\").size().describe()\n"
      ],
      "metadata": {
        "id": "y32oIUiCR8_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "from collections import Counter\n",
        "\n",
        "mask_cb = df_linked[\"EVENT_PRED_CB\"] == 1\n",
        "\n",
        "# Top employers after filtering/mining\n",
        "c_emp = Counter()\n",
        "for ks in df_linked.loc[mask_cb, \"EMPLOYER_KEYS\"]:\n",
        "    for k in (ks or []):\n",
        "        c_emp[k] += 1\n",
        "\n",
        "print(\"\\nTop 30 EMPLOYER_KEYS:\")\n",
        "for k,v in c_emp.most_common(30):\n",
        "    print(v, \"-\", k)\n",
        "\n",
        "print(\"\\nShare of CB/Wage docs with NO employer key:\",\n",
        "      float((df_linked.loc[mask_cb, \"EMPLOYER_KEYS\"].apply(len) == 0).mean()))\n"
      ],
      "metadata": {
        "id": "P-_4UGTiR9no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "from collections import Counter\n",
        "\n",
        "mask_rel = df_linked[\"EVENT_PRED_CB\"] == 1\n",
        "\n",
        "c = Counter()\n",
        "for ks in df_linked.loc[mask_rel, \"ORG_KEYS_FILTERED\"]:\n",
        "    if ks is None or (isinstance(ks, float) and pd.isna(ks)):\n",
        "        continue\n",
        "    if isinstance(ks, list):\n",
        "        for k in ks:\n",
        "            k = str(k).strip()\n",
        "            if k:\n",
        "                c[k] += 1\n",
        "    else:\n",
        "        # if already a string\n",
        "        for k in str(ks).split(\";\"):\n",
        "            k = k.strip()\n",
        "            if k:\n",
        "                c[k] += 1\n",
        "\n",
        "print(\"Top 30 ORG_KEYS_FILTERED (among predicted CB events):\")\n",
        "for k, v in c.most_common(30):\n",
        "    print(f\"{v:>4} - {k}\")\n"
      ],
      "metadata": {
        "id": "R_25LXuxrW8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# ----------------------------\n",
        "# Split each wave-level EVENT_ID into firm-level strike IDs using EMPLOYER_KEYS\n",
        "# Requires df_linked[\"EMPLOYER_KEYS\"] to exist (built above).\n",
        "# ----------------------------\n",
        "\n",
        "def split_wave_into_employers(\n",
        "    df_in,\n",
        "    wave_col=\"EVENT_ID\",\n",
        "    emp_col=\"EMPLOYER_KEYS\",\n",
        "    min_emp_mentions=1,\n",
        "    multi_firm_mode=True\n",
        "):\n",
        "    out = df_in.copy()\n",
        "    out[\"EVENT_ID_FIRM\"] = None\n",
        "\n",
        "    for wave_id, g in out.groupby(wave_col):\n",
        "        # count employer keys within wave\n",
        "        counter = Counter()\n",
        "        for ks in g[emp_col]:\n",
        "            for k in (ks or []):\n",
        "                counter[k] += 1\n",
        "\n",
        "        keep_emps = {k for k,v in counter.items() if v >= min_emp_mentions}\n",
        "\n",
        "        for idx, row in g.iterrows():\n",
        "            keys = [k for k in (row.get(emp_col) or []) if k in keep_emps]\n",
        "\n",
        "            if len(keys) == 0:\n",
        "                out.loc[idx, \"EVENT_ID_FIRM\"] = f\"{wave_id}_UNK\"\n",
        "                continue\n",
        "\n",
        "            if multi_firm_mode and len(keys) > 1:\n",
        "                # keep first (or join) — simplest\n",
        "                out.loc[idx, \"EVENT_ID_FIRM\"] = f\"{wave_id}__{keys[0]}\"\n",
        "            else:\n",
        "                out.loc[idx, \"EVENT_ID_FIRM\"] = f\"{wave_id}__{keys[0]}\"\n",
        "\n",
        "    return out\n",
        "\n",
        "df_linked = split_wave_into_employers(\n",
        "    df_linked,\n",
        "    wave_col=\"EVENT_ID\",\n",
        "    emp_col=\"EMPLOYER_KEYS\",\n",
        "    min_emp_mentions=1,\n",
        "    multi_firm_mode=True\n",
        ")\n",
        "\n",
        "mask = df_linked[\"EVENT_PRED_CB\"] == 1\n",
        "print(\"CB/Wage articles:\", int(mask.sum()))\n",
        "print(\"Unique firm-level strike events:\", df_linked.loc[mask, \"EVENT_ID_FIRM\"].nunique())\n",
        "df_linked.loc[mask].groupby(\"EVENT_ID_FIRM\").size().describe()\n"
      ],
      "metadata": {
        "id": "kH_C_ApYR_x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# ============================================================\n",
        "# UPDATED UNK ABSORPTION (DENEME1 improvement)\n",
        "# You asked: no hard date cutoff. This uses a time-decay factor instead.\n",
        "# ============================================================\n"
      ],
      "metadata": {
        "id": "p7g3OizbSCj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "def absorb_unk_into_employers(\n",
        "    df_in,\n",
        "    wave_col=\"EVENT_ID\",\n",
        "    firm_col=\"EVENT_ID_FIRM\",\n",
        "    date_col=\"PUB_DATE\",\n",
        "    text_col=\"text\",\n",
        "    sim_thresh=0.90,\n",
        "    tau_days=25.0\n",
        "):\n",
        "    out = df_in.copy()\n",
        "    out[date_col] = pd.to_datetime(out[date_col], errors=\"coerce\")\n",
        "\n",
        "    base = out[out[\"EVENT_PRED_CB\"]==1].copy()\n",
        "\n",
        "    for wave_id, g in base.groupby(wave_col):\n",
        "        firm = g[~g[firm_col].str.endswith(\"_UNK\", na=False)].copy()\n",
        "        unk  = g[g[firm_col].str.endswith(\"_UNK\", na=False)].copy()\n",
        "\n",
        "        if len(firm) == 0 or len(unk) == 0:\n",
        "            continue\n",
        "\n",
        "        firm_texts = firm[text_col].tolist()\n",
        "        unk_texts  = unk[text_col].tolist()\n",
        "\n",
        "        E_firm = encode_texts(firm_texts, batch_size=32 if DEVICE==\"cuda\" else 8, max_len=256)\n",
        "        E_unk  = encode_texts(unk_texts,  batch_size=32 if DEVICE==\"cuda\" else 8, max_len=256)\n",
        "\n",
        "        firm_dates = firm[date_col].tolist()\n",
        "        unk_dates  = unk[date_col].tolist()\n",
        "        firm_ids   = firm[firm_col].tolist()\n",
        "\n",
        "        for u_i, row_idx in enumerate(unk.index):\n",
        "            ud = unk_dates[u_i]\n",
        "            best_score = -1.0\n",
        "            best_firm = None\n",
        "\n",
        "            for f_i in range(len(firm)):\n",
        "                fd = firm_dates[f_i]\n",
        "\n",
        "                base_sim = float(np.dot(E_unk[u_i], E_firm[f_i]))\n",
        "                dd = _safe_days_diff(ud, fd)\n",
        "                score = base_sim * _time_decay(dd, tau_days=tau_days)\n",
        "\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_firm = firm_ids[f_i]\n",
        "\n",
        "            if best_firm is not None and best_score >= sim_thresh:\n",
        "                out.loc[row_idx, firm_col] = best_firm\n",
        "\n",
        "    return out\n",
        "\n",
        "df_linked = absorb_unk_into_employers(\n",
        "    df_linked,\n",
        "    sim_thresh=0.90,\n",
        "    tau_days=25.0\n",
        ")\n",
        "\n",
        "mask_cb = df_linked[\"EVENT_PRED_CB\"]==1\n",
        "print(\"UNK count after absorption:\", int(df_linked.loc[mask_cb, \"EVENT_ID_FIRM\"].str.endswith(\"_UNK\", na=False).sum()))\n",
        "print(\"Unique firm events:\", df_linked.loc[mask_cb, \"EVENT_ID_FIRM\"].nunique())\n",
        "df_linked.loc[mask_cb].groupby(\"EVENT_ID_FIRM\").size().describe()\n"
      ],
      "metadata": {
        "id": "G11n0LQkSHIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# ============================================================\n",
        "# REPLACEMENT EXPORT CELL — 2-sheet Excel in your format\n",
        "# Sheet 1: Firm_Level_Strikes (one row per EVENT_ID_FIRM)\n",
        "# Sheet 2: Articles_By_Firm_Event (article-level mapping)\n",
        "# NOW includes: EMPLOYER_KEYS, UNION_KEYS in Sheet 2\n",
        "# ============================================================\n",
        "\n",
        "from openpyxl import Workbook\n",
        "import pandas as pd\n",
        "\n",
        "# pick your final dataframe variable\n",
        "try:\n",
        "    df_out\n",
        "except NameError:\n",
        "    df_out = df_linked  # fallback if you use df_linked\n",
        "\n",
        "# choose mask if not already defined\n",
        "try:\n",
        "    mask\n",
        "except NameError:\n",
        "    mask = df_out[\"EVENT_PRED_CB\"] == 1  # default\n",
        "\n",
        "def list_to_str(x):\n",
        "    if isinstance(x, list):\n",
        "        return \"; \".join([str(i) for i in x if str(i).strip()])\n",
        "    if pd.isna(x):\n",
        "        return \"\"\n",
        "    return str(x)\n",
        "\n",
        "def flatten_unique_keys(series_of_lists):\n",
        "    \"\"\"\n",
        "    Works for list[str] cells OR already-joined strings.\n",
        "    Returns semicolon-joined unique keys.\n",
        "    \"\"\"\n",
        "    s = set()\n",
        "    for ks in series_of_lists:\n",
        "        if ks is None or (isinstance(ks, float) and pd.isna(ks)):\n",
        "            continue\n",
        "        if isinstance(ks, list):\n",
        "            for k in ks:\n",
        "                k = str(k).strip()\n",
        "                if k:\n",
        "                    s.add(k)\n",
        "        else:\n",
        "            for k in str(ks).split(\";\"):\n",
        "                k = k.strip()\n",
        "                if k:\n",
        "                    s.add(k)\n",
        "    return \"; \".join(sorted(s))\n",
        "\n",
        "# ---- Sheet 1: firm-level events ----\n",
        "events_firm = (\n",
        "    df_out.loc[mask]\n",
        "    .groupby(\"EVENT_ID_FIRM\", dropna=False)\n",
        "    .agg(\n",
        "        start=(\"PUB_DATE\", \"min\"),\n",
        "        end=(\"PUB_DATE\", \"max\"),\n",
        "        duration=(\"PUB_DATE\", lambda x: (x.max() - x.min()).days + 1 if x.notna().any() else \"\"),\n",
        "        n_articles=(\"title\", \"count\"),\n",
        "        firms=(\"ORG_KEYS_FILTERED\", flatten_unique_keys),\n",
        "        employers=(\"EMPLOYER_KEYS\", flatten_unique_keys),\n",
        "        unions=(\"UNION_KEYS\", flatten_unique_keys),\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "for c in [\"start\", \"end\"]:\n",
        "    events_firm[c] = pd.to_datetime(events_firm[c], errors=\"coerce\")\n",
        "\n",
        "wb = Workbook()\n",
        "ws1 = wb.active\n",
        "ws1.title = \"Firm_Level_Strikes\"\n",
        "ws1.append(list(events_firm.columns))\n",
        "\n",
        "for _, row in events_firm.iterrows():\n",
        "    ws1.append([list_to_str(v) for v in row.tolist()])\n",
        "\n",
        "# ---- Sheet 2: article-level mapping ----\n",
        "ws2 = wb.create_sheet(\"Articles_By_Firm_Event\")\n",
        "cols = [\n",
        "    \"EVENT_ID_FIRM\",\n",
        "    \"PUB_DATE\",\n",
        "    \"title\",\n",
        "    \"ORG_KEYS_FILTERED\",\n",
        "    \"EMPLOYER_KEYS\",\n",
        "    \"UNION_KEYS\",\n",
        "    \"EVENT_ID\",\n",
        "    \"EVENT_RELEVANT\",\n",
        "]\n",
        "ws2.append(cols)\n",
        "\n",
        "tmp = df_out.loc[mask, cols].copy()\n",
        "tmp[\"PUB_DATE\"] = pd.to_datetime(tmp[\"PUB_DATE\"], errors=\"coerce\")\n",
        "for col in [\"ORG_KEYS_FILTERED\", \"EMPLOYER_KEYS\", \"UNION_KEYS\"]:\n",
        "    tmp[col] = tmp[col].apply(list_to_str)\n",
        "\n",
        "for _, r in tmp.iterrows():\n",
        "    ws2.append([list_to_str(v) for v in r.tolist()])\n",
        "\n",
        "# Save\n",
        "path = \"firm_level_strikes_7.xlsx\"\n",
        "wb.save(path)\n",
        "print(\"Saved:\", path)\n"
      ],
      "metadata": {
        "id": "TcgPkgBESKKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Optional quick sanity sampling\n",
        "mask_named = (df_linked[\"EVENT_PRED_CB\"]==1) & (df_linked[\"EVENT_ID_FIRM\"].notna()) & (~df_linked[\"EVENT_ID_FIRM\"].str.endswith(\"_UNK\", na=False))\n",
        "df_linked.loc[mask_named].groupby(\"EVENT_ID_FIRM\").head(2)[[\"EVENT_ID_FIRM\",\"PUB_DATE\",\"title\"]].sample(20, random_state=42)\n"
      ],
      "metadata": {
        "id": "MNbWVhTnSKxn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}