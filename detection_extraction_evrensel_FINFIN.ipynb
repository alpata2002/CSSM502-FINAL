{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 - Event Detection & Linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, i wrote a script for event detection & linking for the content we scraped. \n",
    "\n",
    "My first idea was to extract the qualities we desired (such as union presence, number of workers on strike etc.) automatically from the articles. In this endeavour, i attempted to use sequence linking for both training the detection (using spans of a sentence in json format) and event linking. After producing very low evaluation scores (see sequence_linking_deneme.ipynb), i decided to manually annotate the spans using doccano. Unfortunately, doccano works terribly with Turkish; so that was a waste. I then gave up on automatic extraction completely, instead opting to manually label the relevant values for our linked events.\n",
    "\n",
    "My next idea was annotating 738 of our articles by hand for their relevance (1 for relevant and 0 for irrelevant) and training the model on this set, with the majority of them being labeled negative (160 positive, 578 negative) as to reflect the pool of articles. Although training was fine, this approach lacked in event linking; producing huge clusters (see event detection & linking).\n",
    "\n",
    "To overcome this, we attempted to use rule-based clustering for cross-document linking. However, because of normalization, tuning and rule errors; the model was not able to effectively link events (see detection_extraction_deneme3.ipynb)\n",
    "\n",
    "Then, we tried to add a NER package to do the linking, but the Turkish depository could not be accessed (see detection_extraction_deneme4.ipynb).\n",
    "\n",
    "\n",
    "\n",
    "Finally, after all these trials, we arrived at our last model. The details of each cell will be explained on the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "50yEujdJfCGM"
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from collections import Counter, defaultdict\n",
    "from openpyxl import Workbook\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import precision_recall_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xH3rdUk_fKcM"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8zx6A_eofRcS"
   },
   "outputs": [],
   "source": [
    "!pip -q install transformers accelerate evaluate openpyxl scikit-learn pandas numpy torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "LkqJAofifh_h",
    "outputId": "e5eab443-4b4c-4db7-e50a-2a3328691c26"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip -q install \"transformers>=4.38\"\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After working on jupyter notebook up to this point, i decided to switch to google collab since the existence of GPU's allowed for much shorter training & prediction durations when compared to using only my PC's CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zl-sJ-EDfCo9",
    "outputId": "39f1fe81-e1f4-49cc-8648-e9dc92752580"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9186, 15),\n",
       " ['title',\n",
       "  'date',\n",
       "  'link',\n",
       "  'content',\n",
       "  'EVENT_RELEVANT',\n",
       "  'EVENT_ID',\n",
       "  'Unnamed: 6',\n",
       "  'error',\n",
       "  'Unnamed: 8',\n",
       "  'Unnamed: 9',\n",
       "  'Unnamed: 10',\n",
       "  'Unnamed: 11',\n",
       "  'Unnamed: 12',\n",
       "  650,\n",
       "  160])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_XLSX = \"evrensel_isci_sendika_2024_dec2025_clean_fin_uncorrupted_real.xlsx\"  \n",
    "df = pd.read_excel(PATH_XLSX)\n",
    "\n",
    "required_cols = [\"EVENT_RELEVANT\", \"EVENT_ID\", \"title\", \"content\",\"date\"]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in XLSX: {missing}\")\n",
    "TITLE_COL = \"title\"\n",
    "CONTENT_COL = \"content\"\n",
    "TEXT_COL = \"text\"\n",
    "DATE_COL = \"date\"    \n",
    "MANUAL_COL = \"EVENT_RELEVANT\"  \n",
    "EVENT_ID_COL = \"EVENT_ID\"      \n",
    "EVENT_ID = EVENT_ID_COL\n",
    "\n",
    "LINK = \"link\"\n",
    "\n",
    "df.shape, df.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the following part, we dip our toes into some text naturalization. This will be a huge problem for us throughout the script. After that, we correct the date parsing. They were written previously as \"13 Ağustos 2024\", we turned them to \"13.08.2024\". This will be important later for event linking, since days between article dates will be one of our constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8IhxRmImh60t",
    "outputId": "16a78dc2-54de-47ec-8d88-8258bc827453"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed DATE_DT non-null: 9167 / 9186\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "SINGLE_LETTER_TOKEN_RE = re.compile(r\"\\b[^\\W\\d_]\\b\", flags=re.UNICODE)\n",
    "\n",
    "TR_MONTHS = {\n",
    "    \"ocak\": 1, \"şubat\": 2, \"subat\": 2, \"mart\": 3, \"nisan\": 4, \"mayıs\": 5, \"mayis\": 5,\n",
    "    \"haziran\": 6, \"temmuz\": 7, \"ağustos\": 8, \"agustos\": 8, \"eylül\": 9, \"eylul\": 9,\n",
    "    \"ekim\": 10, \"kasım\": 11, \"kasim\": 11, \"aralık\": 12, \"aralik\": 12\n",
    "}\n",
    "\n",
    "def parse_tr_datetime(s):\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return pd.NaT\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return pd.NaT\n",
    "\n",
    "    # Example: \"8 Şubat 2024 13:43\" (sometimes with extra whitespace)\n",
    "    m = re.search(r\"(\\d{1,2})\\s+([A-Za-zÇĞİÖŞÜçğıöşü]+)\\s+(\\d{4})(?:\\s+(\\d{1,2}):(\\d{2}))?\", s)\n",
    "    if not m:\n",
    "        return pd.NaT\n",
    "\n",
    "    day = int(m.group(1))\n",
    "    month_name = m.group(2).lower().replace(\"ı\", \"i\")  # helps \"mayıs/kasım\" normalization\n",
    "    # But keep Turkish chars too:\n",
    "    month_name = m.group(2).lower()\n",
    "    month = TR_MONTHS.get(month_name, None)\n",
    "    if month is None:\n",
    "        # try ascii fallback\n",
    "        month = TR_MONTHS.get(month_name.replace(\"ş\",\"s\").replace(\"ğ\",\"g\").replace(\"ü\",\"u\").replace(\"ö\",\"o\").replace(\"ç\",\"c\").replace(\"ı\",\"i\"), None)\n",
    "    if month is None:\n",
    "        return pd.NaT\n",
    "\n",
    "    year = int(m.group(3))\n",
    "    hh = int(m.group(4)) if m.group(4) else 0\n",
    "    mm = int(m.group(5)) if m.group(5) else 0\n",
    "\n",
    "    return pd.Timestamp(year=year, month=month, day=day, hour=hh, minute=mm)\n",
    "\n",
    "# Create a clean datetime column and use it everywhere downstream\n",
    "df[\"DATE_DT\"] = df[DATE_COL].apply(parse_tr_datetime)\n",
    "\n",
    "print(\"Parsed DATE_DT non-null:\", int(df[\"DATE_DT\"].notna().sum()), \"/\", len(df))\n",
    "df.loc[df[\"DATE_DT\"].isna(), [DATE_COL]].head(10)\n",
    "\n",
    "DATE_COL = \"DATE_DT\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ho18TH6-fTzY",
    "outputId": "e517853a-cadb-4da9-94f4-128a84a5bc46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape: (9186, 16)\n",
      "Columns ok: False\n"
     ]
    }
   ],
   "source": [
    "print(\"df shape:\", df.shape)\n",
    "print(\"Columns ok:\", all(c in df.columns for c in [TITLE_COL, CONTENT_COL, TEXT_COL, DATE_COL]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Columns ok: False\" seems worrying but is actually not. This will be solved when we create the \"text\" column in the next cell. Now, we will create that cell and apply some more normalization by clearing weird spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "id": "ZPo6aU_afxym",
    "outputId": "5c3cc9b5-f679-4024-dfaa-241901c7368e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df[[\\\"title\\\",\\\"content\\\",\\\"text\\\"]]\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Bu soygun d\\u00fczeni de\\u011fi\\u015fmeli\",\n          \"Bart\\u0131n'da Hema'ya ait maden oca\\u011f\\u0131nda vagonlar\\u0131n aras\\u0131na s\\u0131k\\u0131\\u015fan i\\u015f\\u00e7i hayat\\u0131n\\u0131 kaybetti\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Pendik Marmara E\\u011fitim ve Ara\\u015ft\\u0131rma Hastanesinden bir sa\\u011fl\\u0131k emek\\u00e7isi Devlet memurlar\\u0131n\\u0131n \\u015fu s\\u0131ra en b\\u00fcy\\u00fck g\\u00fcndemlerinden birisi vergi dilimi meselesi. \\u00c7\\u00fcnk\\u00fc ekonomik ko\\u015fullar daha da k\\u00f6t\\u00fcye giderken, her g\\u00fcn al\\u0131m g\\u00fcc\\u00fcm\\u00fcz d\\u00fc\\u015ferken, bu ya\\u015fam standartlar\\u0131nda maa\\u015flar\\u0131m\\u0131z\\u0131n insanca ya\\u015fayabilecek bir d\\u00fczeyden uzak olmas\\u0131 yetmezmi\\u015f gibi, artan vergi kesintileriyle kayb\\u0131m\\u0131z daha da b\\u00fcy\\u00fcyor. \\u00d6nceki d\\u00f6nemlerde de \\u00e7ok da adaletli olmayan bu vergi sistemi, bug\\u00fcn\\u00fcn ekonomik ko\\u015fullar\\u0131nda biz emek\\u00e7iler i\\u00e7in tam bir kambur haline geldi. H\\u00fck\\u00fcmetin izledi\\u011fi ekonomi politikalar\\u0131 sonucunda olu\\u015fan a\\u00e7\\u0131k, halihaz\\u0131rda ge\\u00e7inmekte zorlanan biz emek\\u00e7ilerden \\u00e7\\u0131kar\\u0131lacak. \\u00c7\\u00fcnk\\u00fc sermayeye asla dokunamaz, dokunmazlar. Mecliste torba yasalarla sermaye gruplar\\u0131 her t\\u00fcrl\\u00fc ihaleleri al\\u0131rken, devletten ald\\u0131klar\\u0131 taahh\\u00fctlerle kasalar\\u0131n\\u0131 doldururken, milyarl\\u0131k vergi bor\\u00e7lar\\u0131 silinirken, AKP iktidar\\u0131n\\u0131n krizin faturas\\u0131n\\u0131 biz emek\\u00e7ilerden \\u00e7\\u0131karmamas\\u0131n\\u0131 beklemek abes olur. H\\u00fck\\u00fcmetin T\\u00dc\\u0130K\\u2019in yalan enflasyon rakamlar\\u0131yla alaca\\u011f\\u0131m\\u0131z zamlar\\u0131 \\u00fc\\u00e7 kuru\\u015fa d\\u00fc\\u015f\\u00fcrerek k\\u00fcfreder gibi hak etti\\u011fimizden \\u00e7ok uzak rakamlarla \\u00e7al\\u0131\\u015ft\\u0131rmas\\u0131 yetmezmi\\u015f gibi 3 kuru\\u015fluk zamm\\u0131n yar\\u0131s\\u0131na da elimize ge\\u00e7meden el koyuluyor. 2024 b\\u00fct\\u00e7esinin b\\u00fcy\\u00fck bir k\\u0131sm\\u0131 biz emek\\u00e7ilerden toplanaca\\u011f\\u0131 ilan edilmi\\u015fken, \\u2018K\\u00f6t\\u00fc g\\u00fcnleri geride b\\u0131rakt\\u0131k, s\\u0131rada daha k\\u00f6t\\u00fc g\\u00fcnler var\\u2019 repli\\u011fi ger\\u00e7e\\u011fe b\\u00fcr\\u00fcnm\\u00fc\\u015ft\\u00fcr. K\\u00f6t\\u00fc g\\u00fcnleri yaratanlar, bizlere insanca ya\\u015fayacak bir \\u00fccreti \\u00e7ok g\\u00f6renler, televizyonlarda ve gazetelerde \\u015fu kadar zam yapaca\\u011f\\u0131z deyip maa\\u015f elimize ge\\u00e7meden mafya gibi \\u00e7\\u00f6kenler bu soygun d\\u00fczenini de\\u011fi\\u015ftirmeyecekler, tabi bizler h\\u00fck\\u00fcmeti bunu yapmaya zorlamazsak. \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor...\",\n          \"Bart\\u0131n'\\u0131n Amasra il\\u00e7esindeki Hema Enerji \\u015firketine ait maden oca\\u011f\\u0131nda vagonlar\\u0131n aras\\u0131na s\\u0131k\\u0131\\u015fan i\\u015f\\u00e7i Ferdi \\u00d6zg\\u00fcn (33) Ankara'da tedavi g\\u00f6rd\\u00fc\\u011f\\u00fc hastanede 1 hafta sonra hayat\\u0131n\\u0131 kaybetti. \\u0130\\u015f cinayeti, 26 Aral\\u0131k'ta, merkeze ba\\u011fl\\u0131 Tarlaa\\u011fz\\u0131 k\\u00f6y\\u00fcndeki Hema Enerji \\u015firketine ait maden oca\\u011f\\u0131nda meydana geldi. Saat 16.00- 24.00 vardiyas\\u0131nda eksi 500 kotunda \\u00e7al\\u0131\\u015fan Ferdi \\u00d6zg\\u00fcn, k\\u00f6m\\u00fcr galerisine mekanize direkler ta\\u015f\\u0131nd\\u0131\\u011f\\u0131 s\\u0131rada vagonlar\\u0131n aras\\u0131na s\\u0131k\\u0131\\u015ft\\u0131. Mesai arkada\\u015flar\\u0131, durumu 112 Acil Sa\\u011fl\\u0131k ekiplerine haber verdi. \\u0130lk m\\u00fcdahalesinin ard\\u0131ndan ambulansla Bart\\u0131n Devlet Hastanesi'ne kald\\u0131r\\u0131lan \\u00d6zg\\u00fcn, buradan da Ankara Bilkent \\u015eehir Hastanesi'ne sevk edildi. Ferdi \\u00d6zg\\u00fcn, hastanede bu sabah hayat\\u0131n\\u0131 kaybetti.(DHA) \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... Elif Ekin Salt\\u0131k \\u0130\\u00e7erik y\\u00fckleniyor...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Bu soygun d\\u00fczeni de\\u011fi\\u015fmeli\\n\\nPendik Marmara E\\u011fitim ve Ara\\u015ft\\u0131rma Hastanesinden bir sa\\u011fl\\u0131k emek\\u00e7isi Devlet memurlar\\u0131n\\u0131n \\u015fu s\\u0131ra en b\\u00fcy\\u00fck g\\u00fcndemlerinden birisi vergi dilimi meselesi. \\u00c7\\u00fcnk\\u00fc ekonomik ko\\u015fullar daha da k\\u00f6t\\u00fcye giderken, her g\\u00fcn al\\u0131m g\\u00fcc\\u00fcm\\u00fcz d\\u00fc\\u015ferken, bu ya\\u015fam standartlar\\u0131nda maa\\u015flar\\u0131m\\u0131z\\u0131n insanca ya\\u015fayabilecek bir d\\u00fczeyden uzak olmas\\u0131 yetmezmi\\u015f gibi, artan vergi kesintileriyle kayb\\u0131m\\u0131z daha da b\\u00fcy\\u00fcyor. \\u00d6nceki d\\u00f6nemlerde de \\u00e7ok da adaletli olmayan bu vergi sistemi, bug\\u00fcn\\u00fcn ekonomik ko\\u015fullar\\u0131nda biz emek\\u00e7iler i\\u00e7in tam bir kambur haline geldi. H\\u00fck\\u00fcmetin izledi\\u011fi ekonomi politikalar\\u0131 sonucunda olu\\u015fan a\\u00e7\\u0131k, halihaz\\u0131rda ge\\u00e7inmekte zorlanan biz emek\\u00e7ilerden \\u00e7\\u0131kar\\u0131lacak. \\u00c7\\u00fcnk\\u00fc sermayeye asla dokunamaz, dokunmazlar. Mecliste torba yasalarla sermaye gruplar\\u0131 her t\\u00fcrl\\u00fc ihaleleri al\\u0131rken, devletten ald\\u0131klar\\u0131 taahh\\u00fctlerle kasalar\\u0131n\\u0131 doldururken, milyarl\\u0131k vergi bor\\u00e7lar\\u0131 silinirken, AKP iktidar\\u0131n\\u0131n krizin faturas\\u0131n\\u0131 biz emek\\u00e7ilerden \\u00e7\\u0131karmamas\\u0131n\\u0131 beklemek abes olur. H\\u00fck\\u00fcmetin T\\u00dc\\u0130K\\u2019in yalan enflasyon rakamlar\\u0131yla alaca\\u011f\\u0131m\\u0131z zamlar\\u0131 \\u00fc\\u00e7 kuru\\u015fa d\\u00fc\\u015f\\u00fcrerek k\\u00fcfreder gibi hak etti\\u011fimizden \\u00e7ok uzak rakamlarla \\u00e7al\\u0131\\u015ft\\u0131rmas\\u0131 yetmezmi\\u015f gibi 3 kuru\\u015fluk zamm\\u0131n yar\\u0131s\\u0131na da elimize ge\\u00e7meden el koyuluyor. 2024 b\\u00fct\\u00e7esinin b\\u00fcy\\u00fck bir k\\u0131sm\\u0131 biz emek\\u00e7ilerden toplanaca\\u011f\\u0131 ilan edilmi\\u015fken, \\u2018K\\u00f6t\\u00fc g\\u00fcnleri geride b\\u0131rakt\\u0131k, s\\u0131rada daha k\\u00f6t\\u00fc g\\u00fcnler var\\u2019 repli\\u011fi ger\\u00e7e\\u011fe b\\u00fcr\\u00fcnm\\u00fc\\u015ft\\u00fcr. K\\u00f6t\\u00fc g\\u00fcnleri yaratanlar, bizlere insanca ya\\u015fayacak bir \\u00fccreti \\u00e7ok g\\u00f6renler, televizyonlarda ve gazetelerde \\u015fu kadar zam yapaca\\u011f\\u0131z deyip maa\\u015f elimize ge\\u00e7meden mafya gibi \\u00e7\\u00f6kenler bu soygun d\\u00fczenini de\\u011fi\\u015ftirmeyecekler, tabi bizler h\\u00fck\\u00fcmeti bunu yapmaya zorlamazsak. \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor...\",\n          \"Bart\\u0131n'da Hema'ya ait maden oca\\u011f\\u0131nda vagonlar\\u0131n aras\\u0131na s\\u0131k\\u0131\\u015fan i\\u015f\\u00e7i hayat\\u0131n\\u0131 kaybetti\\n\\nBart\\u0131n'\\u0131n Amasra il\\u00e7esindeki Hema Enerji \\u015firketine ait maden oca\\u011f\\u0131nda vagonlar\\u0131n aras\\u0131na s\\u0131k\\u0131\\u015fan i\\u015f\\u00e7i Ferdi \\u00d6zg\\u00fcn (33) Ankara'da tedavi g\\u00f6rd\\u00fc\\u011f\\u00fc hastanede 1 hafta sonra hayat\\u0131n\\u0131 kaybetti. \\u0130\\u015f cinayeti, 26 Aral\\u0131k'ta, merkeze ba\\u011fl\\u0131 Tarlaa\\u011fz\\u0131 k\\u00f6y\\u00fcndeki Hema Enerji \\u015firketine ait maden oca\\u011f\\u0131nda meydana geldi. Saat 16.00- 24.00 vardiyas\\u0131nda eksi 500 kotunda \\u00e7al\\u0131\\u015fan Ferdi \\u00d6zg\\u00fcn, k\\u00f6m\\u00fcr galerisine mekanize direkler ta\\u015f\\u0131nd\\u0131\\u011f\\u0131 s\\u0131rada vagonlar\\u0131n aras\\u0131na s\\u0131k\\u0131\\u015ft\\u0131. Mesai arkada\\u015flar\\u0131, durumu 112 Acil Sa\\u011fl\\u0131k ekiplerine haber verdi. \\u0130lk m\\u00fcdahalesinin ard\\u0131ndan ambulansla Bart\\u0131n Devlet Hastanesi'ne kald\\u0131r\\u0131lan \\u00d6zg\\u00fcn, buradan da Ankara Bilkent \\u015eehir Hastanesi'ne sevk edildi. Ferdi \\u00d6zg\\u00fcn, hastanede bu sabah hayat\\u0131n\\u0131 kaybetti.(DHA) \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... \\u0130\\u00e7erik y\\u00fckleniyor... Elif Ekin Salt\\u0131k \\u0130\\u00e7erik y\\u00fckleniyor...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-559715fd-6ac8-4d45-b08b-fa4817e8bfce\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bartın'da Hema'ya ait maden ocağında vagonları...</td>\n",
       "      <td>Bartın'ın Amasra ilçesindeki Hema Enerji şirke...</td>\n",
       "      <td>Bartın'da Hema'ya ait maden ocağında vagonları...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bu soygun düzeni değişmeli</td>\n",
       "      <td>Pendik Marmara Eğitim ve Araştırma Hastanesind...</td>\n",
       "      <td>Bu soygun düzeni değişmeli\\n\\nPendik Marmara E...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-559715fd-6ac8-4d45-b08b-fa4817e8bfce')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-559715fd-6ac8-4d45-b08b-fa4817e8bfce button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-559715fd-6ac8-4d45-b08b-fa4817e8bfce');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Bartın'da Hema'ya ait maden ocağında vagonları...   \n",
       "1                         Bu soygun düzeni değişmeli   \n",
       "\n",
       "                                             content  \\\n",
       "0  Bartın'ın Amasra ilçesindeki Hema Enerji şirke...   \n",
       "1  Pendik Marmara Eğitim ve Araştırma Hastanesind...   \n",
       "\n",
       "                                                text  \n",
       "0  Bartın'da Hema'ya ait maden ocağında vagonları...  \n",
       "1  Bu soygun düzeni değişmeli\\n\\nPendik Marmara E...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "def normalize_text(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    x = str(x).replace(\"\\u00A0\", \" \")  # non-breaking space\n",
    "    x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    return x\n",
    "\n",
    "df[\"title\"] = df[\"title\"].apply(normalize_text)\n",
    "df[\"content\"] = df[\"content\"].apply(normalize_text)\n",
    "\n",
    "# final text fed into the model\n",
    "df[\"text\"] = (df[\"title\"].astype(str) + \"\\n\\n\" + df[\"content\"].astype(str)).str.strip()\n",
    "df[[\"title\",\"content\",\"text\"]].head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the excel sheet, EVENT_RELEVANT values are recorded as both numeric and string variables by mistake. We will normalize them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "De-dqcZIfzTA",
    "outputId": "dc313cad-9961-4925-db9e-eec2d35f053a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 9186\n",
      "Labeled rows: 738\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"EVENT_RELEVANT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LABEL_CLEAN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-c28da34d-bdf6-41ae-a6f1-c98766b44c70\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EVENT_RELEVANT</th>\n",
       "      <th>LABEL_CLEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c28da34d-bdf6-41ae-a6f1-c98766b44c70')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-c28da34d-bdf6-41ae-a6f1-c98766b44c70 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-c28da34d-bdf6-41ae-a6f1-c98766b44c70');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   EVENT_RELEVANT  LABEL_CLEAN\n",
       "0             0.0          0.0\n",
       "1             0.0          0.0\n",
       "2             0.0          0.0\n",
       "3             0.0          0.0\n",
       "4             0.0          0.0\n",
       "5             0.0          0.0\n",
       "6             0.0          0.0\n",
       "7             0.0          0.0\n",
       "8             0.0          0.0\n",
       "9             0.0          0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_label(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "\n",
    "  \n",
    "    if isinstance(x, (int, np.integer, float, np.floating)):\n",
    "        if x == 0 or x == 0.0:\n",
    "            return 0\n",
    "        if x == 1 or x == 1.0:\n",
    "            return 1\n",
    "        return np.nan\n",
    "\n",
    "    s = str(x).strip().lower()\n",
    "    if s in {\"0\", \"0.0\", \"no\", \"n\", \"false\"}:\n",
    "        return 0\n",
    "    if s in {\"1\", \"1.0\", \"yes\", \"y\", \"true\"}:\n",
    "        return 1\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "df[\"LABEL_CLEAN\"] = df[\"EVENT_RELEVANT\"].apply(normalize_label)\n",
    "labeled_mask = df[\"LABEL_CLEAN\"].notna()\n",
    "\n",
    "print(\"Total rows:\", len(df))\n",
    "print(\"Labeled rows:\", int(labeled_mask.sum()))\n",
    "df.loc[labeled_mask, [\"EVENT_RELEVANT\",\"LABEL_CLEAN\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally start training. A classic 20/80 test/train split will be used. The model will be trained on the 738 manually labeled rows as said before. We can observe that both the training and testing sets are balanced with respect to the distribution of 0/1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZ7v8fetf0nj",
    "outputId": "09058b71-7dc1-494b-a918-6d212c9e3176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 590 Val size: 148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(label\n",
       " 0    0.783051\n",
       " 1    0.216949\n",
       " Name: proportion, dtype: float64,\n",
       " label\n",
       " 0    0.783784\n",
       " 1    0.216216\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_labeled = df.loc[labeled_mask].copy()\n",
    "\n",
    "df_labeled[\"label\"] = df_labeled[\"LABEL_CLEAN\"].astype(int)\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df_labeled,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_labeled[\"label\"]\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_df), \"Val size:\", len(val_df))\n",
    "train_df[\"label\"].value_counts(normalize=True), val_df[\"label\"].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a final check before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iG_pNviUf17K",
    "outputId": "e61eeb24-c1a1-4bc2-92d6-eecd01781f4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: float64\n",
      "non-null count: 738\n",
      "unique values sample (up to 50): [0. 1.]\n",
      "stringified sample (up to 50): ['0.0', '1.0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s = df[\"EVENT_RELEVANT\"]\n",
    "\n",
    "print(\"dtype:\", s.dtype)\n",
    "print(\"non-null count:\", s.notna().sum())\n",
    "\n",
    "\n",
    "u = s.dropna().unique()\n",
    "print(\"unique values sample (up to 50):\", u[:50])\n",
    "\n",
    "\n",
    "u_str = pd.Series(u).astype(str).str.strip()\n",
    "print(\"stringified sample (up to 50):\", u_str.head(50).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set. As specified in the midterm report, this project will not train a model from scratch; but modify and fine-tune BERTurk for strike detection purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XbpQ6_4f4UI",
    "outputId": "fcdf430c-bb48-4c73-de59-d5a85ec0b8db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_NAME = \"dbmdz/bert-base-turkish-cased\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "MAX_LEN = 384 \n",
    "\n",
    "class TextClsDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=384):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = None if labels is None else list(labels)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[i],\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=False\n",
    "        )\n",
    "        item = {k: torch.tensor(v) for k, v in enc.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(int(self.labels[i]))\n",
    "        return item\n",
    "\n",
    "train_ds = TextClsDataset(train_df[\"text\"], train_df[\"label\"], tokenizer, MAX_LEN)\n",
    "val_ds   = TextClsDataset(val_df[\"text\"],   val_df[\"label\"],   tokenizer, MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above error is fine, since the public model still can be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SpfhncOBf524",
    "outputId": "1414b1a7-7b5f-4e52-ede0-c45da83a5407"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 3.6094], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(DEVICE)\n",
    "\n",
    "pos = int(train_df[\"label\"].sum())\n",
    "neg = int(len(train_df) - pos)\n",
    "\n",
    "# More weight to positive class if positives are rare\n",
    "class_weights = torch.tensor([1.0, (neg / max(pos, 1))], dtype=torch.float, device=DEVICE)\n",
    "class_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train the model on 3 epochs. We will implement the ROC-AUC score, since this is a binary classification task. We will also weigh our trainer; since we value recall highly and relevant strikes are far less common then irrelevant ones in the article database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "IQv65Nslf7Ph",
    "outputId": "e77a0e46-81dd-4c18-ad75-723570dcfa68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3381214140.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='222' max='222' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [222/222 00:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.647700</td>\n",
       "      <td>0.366318</td>\n",
       "      <td>0.938578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.392300</td>\n",
       "      <td>0.458118</td>\n",
       "      <td>0.942080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>0.420896</td>\n",
       "      <td>0.945043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=222, training_loss=0.32484422932873974, metrics={'train_runtime': 25.5963, 'train_samples_per_second': 69.151, 'train_steps_per_second': 8.673, 'total_flos': 349279925990400.0, 'train_loss': 0.32484422932873974, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import inspect\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()[:, 1]\n",
    "    return {\n",
    "        \"roc_auc\": float(roc_auc_score(labels, probs)) if len(np.unique(labels)) > 1 else float(\"nan\")\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Weighted loss\n",
    "def custom_loss_fn(model, inputs, return_outputs=False):\n",
    "    labels = inputs.pop(\"labels\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    loss = loss_fct(logits, labels)\n",
    "    return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Some transformers versions support compute_loss in Trainer; fallback safely\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        return custom_loss_fn(model, inputs, return_outputs=return_outputs)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"berturk_event_detect\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8 if DEVICE==\"cuda\" else 4,\n",
    "    per_device_eval_batch_size=16 if DEVICE==\"cuda\" else 8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"roc_auc\",\n",
    "    greater_is_better=True,\n",
    "    fp16=(DEVICE==\"cuda\"),\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC AUC score is highly promising, although the model is also somewhat prone to overfitting according to the validation loss over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "CN4WUUdof8fD",
    "outputId": "404ee43a-2e8a-450f-b823-1723330d014e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.9450431034482759\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.957     0.957     0.957       116\n",
      "           1      0.844     0.844     0.844        32\n",
      "\n",
      "    accuracy                          0.932       148\n",
      "   macro avg      0.900     0.900     0.900       148\n",
      "weighted avg      0.932     0.932     0.932       148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "val_out = trainer.predict(val_ds)\n",
    "val_logits = val_out.predictions\n",
    "val_labels = val_out.label_ids\n",
    "\n",
    "val_probs = torch.softmax(torch.tensor(val_logits), dim=-1).numpy()[:, 1]\n",
    "val_preds = (val_probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"ROC-AUC:\", roc_auc_score(val_labels, val_probs) if len(np.unique(val_labels)) > 1 else \"NA\")\n",
    "print(classification_report(val_labels, val_preds, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our scores for 1's are lower than 0's, although they are still workable. This is likely because we have quite a small support in the validation set for 1's, with only 32 of them. Now, it is prediction time. To not lose too many articles (since we prioritize recall as explained before), we will pick a threshold that yields a minimum recall of 0.85. This will most likely be a low threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "61oZU08rf-qr",
    "outputId": "cc6c031d-87b8-4a42-f869-aac661a1d322"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen threshold (min_recall=0.85): 0.04676847159862518\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EVENT_PRED</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "EVENT_PRED\n",
       "0    7331\n",
       "1    1855\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "full_ds = TextClsDataset(df[\"text\"], labels=None, tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "full_out = trainer.predict(full_ds)\n",
    "full_logits = full_out.predictions\n",
    "full_probs = torch.softmax(torch.tensor(full_logits), dim=-1).numpy()[:, 1]\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def pick_threshold_min_recall(y_true, y_prob, min_recall=0.85):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    best_t = 0.0\n",
    "    best_p = -1.0\n",
    "\n",
    "    for t, p, r in zip(thresholds, precision[1:], recall[1:]):\n",
    "        if r >= min_recall and p > best_p:\n",
    "            best_p = p\n",
    "            best_t = float(t)\n",
    "\n",
    "\n",
    "    if best_p < 0:\n",
    "      \n",
    "        best_t = float(thresholds[np.argmax(recall[1:])])\n",
    "\n",
    "    return best_t\n",
    "\n",
    "\n",
    "BEST_T = pick_threshold_min_recall(val_labels, val_probs, min_recall=0.85)\n",
    "print(\"Chosen threshold (min_recall=0.85):\", BEST_T)\n",
    "\n",
    "df[\"EVENT_PRED\"] = (full_probs >= BEST_T).astype(int)\n",
    "df[\"EVENT_PRED\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 9186 articles, our model has predicted 1855 of them to be strike relevant. Since this is such an important part of our model, we will re-check it just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "SZGjAR5pgEYA",
    "outputId": "a61ff68d-acca-4ed8-a241-f9ae638b7a13",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT_PRED\n",
      "0    7331\n",
      "1    1855\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class TextClsDataset(torch.utils.data.Dataset):\n",
    "    # Keep compatible with your training dataset structure:\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=384):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = None if labels is None else list(labels)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=False,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "full_ds = TextClsDataset(df[TEXT_COL], labels=None, tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "full_out = trainer.predict(full_ds)\n",
    "full_logits = full_out.predictions\n",
    "full_probs = torch.softmax(torch.tensor(full_logits), dim=-1).numpy()[:, 1]\n",
    "\n",
    "df[\"EVENT_PROB\"] = full_probs\n",
    "df[\"EVENT_PRED\"] = (df[\"EVENT_PROB\"] >= BEST_T).astype(int)\n",
    "\n",
    "print(df[\"EVENT_PRED\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All is good. We can finally move onto the linking part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that we have 1855 strike-relevant articles. However, due to heavy semantic similarities, BERTurk cannot specifically detect wage/collective bargaining based strike events. Instead, it just detects \"strike events\" in general. We will implement some rules to narrow these 1855 articles down to wage/collective bargaining relevant ones.\n",
    "\n",
    "At first, we had only CB_KEEP but later decided to split it into two parts: CB_KEEP_STRONG and CB_KEEP_WEAK. The reason is simple; CB_EXCLUDE was getting rid of too many articles (false negatives). You can find this version as a comment in the code. So, we had to relax our narrowing a bit. That is why we also added CB_DISMISSAL; we are not interested in strikes occuring because of dismissals but we do not want to lose the wage/collective bargaining strikes which also have workers dismissed. The CB_KEEP_STRONG and CB_KEEP_WEAK lists are self explanatory (relating to wage/collective bargaining. So is the CB_EXCLUDE, especially considering Evrensel's reporting tendencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48WL6LkegExg",
    "outputId": "be96c0a3-2be6-4f2b-9369-22e4800cc5ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT_PRED: 1855\n",
      "EVENT_PRED_CB: 1150\n"
     ]
    }
   ],
   "source": [
    "CB_KEEP = re.compile(\n",
    "    r\"(toplu\\s+i[şs]\\s+sözleşme|toplu\\s+sözleşme|tis\\b|\"\n",
    "    r\"ücret|zam|sözleşme|protokol|arabulucu|\"\n",
    "    r\"grev\\s+karar|grev\\s+başla|grev\\s+çıktı|iş\\s+bırak)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Strong CB / bargaining signals (high precision)\n",
    "CB_KEEP_STRONG = re.compile(\n",
    "    r\"(?i)\\b(\"\n",
    "    r\"toplu\\s+i[şs]\\s+sözleşme|toplu\\s+sözleşme|\\btis\\b|\"\n",
    "    r\"arabulucu|arabuluculuk|\"\n",
    "    r\"görüşme(leri)?|müzakere|protokol|\"\n",
    "    r\"imza(landı|sı|lamak)|sözleşme\\s+imza|\"\n",
    "    r\"zam\\s+oranı|ücret\\s+artış|\"\n",
    "    r\"ikramiye|sosyal\\s+hak(lar)?\"\n",
    "    r\")\\b\"\n",
    ")\n",
    "# Weaker wage-related signals (higher recall, lower precision)\n",
    "CB_KEEP_WEAK = re.compile(\n",
    "    r\"(?i)\\b(ücret|maaş|zam|\"\n",
    "    r\"ikramiye|prim|\"\n",
    "    r\"yemek\\s+ücreti|yol\\s+ücreti|\"\n",
    "    r\"tazminat|kıdem|ihbar|\"\n",
    "    r\"asgari\\s+ücret)\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "CB_EXCLUDE = re.compile(\n",
    "    r\"(?i)\\b(\"\n",
    "    r\"iş\\s+cinayet|kaza|yaralandı|öl(ü|u)m|\"\n",
    "    r\"enflasyon|hayat\\s+pahalılığı|pahalılık|\"\n",
    "    r\"genel\\s+değerlendirme|genel\\s+yorum|\"\n",
    "    r\"seçim|oy\\s+ver|sandık|miting|\"\n",
    "    r\"sendikalaş|\"\n",
    "    r\"mülteci|deprem|sel|yangın|\"\n",
    "    r\"iş\\s+kaz|göçük|gözalt|tutuk|\"\n",
    "    r\"dava|mahkeme|ziyaret|dayanışma|anma|basın\\s+açıklama\"\n",
    "    r\")\\b\"\n",
    ")\n",
    "\n",
    "CB_DISMISSAL = re.compile(\n",
    "    r\"(?i)\\b(\"\n",
    "    r\"işten\\s+(çıkar(ıl|ma)|at(ıl|ma)|çıkarıldı|atıldı)|\"\n",
    "    r\"işten\\s+çıkarmalar?|toplu\\s+çıkarma|\"\n",
    "    r\"kod\\s*29|kod\\s*46|\"\n",
    "    r\"tazminat(sız|siz)|\"\n",
    "    r\"ücretsiz\\s+izin\"\n",
    "    r\")\\b\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def cb_filter_soft(text: str) -> int:\n",
    "    t = str(text or \"\")\n",
    "\n",
    "    strong = bool(CB_KEEP_STRONG.search(t))\n",
    "    weak   = bool(CB_KEEP_WEAK.search(t))\n",
    "    dism   = bool(CB_DISMISSAL.search(t))\n",
    "    excl   = bool(CB_EXCLUDE.search(t))\n",
    "\n",
    "\n",
    "    if strong:\n",
    "        return 1\n",
    "\n",
    "\n",
    "    if weak and not excl:\n",
    "        return 1\n",
    "\n",
    "   \n",
    "    if dism and weak and not excl:\n",
    "        return 1\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "#def cb_filter(text: str) -> int:\n",
    "#    t = str(text or \"\")\n",
    "#    if not CB_KEEP.search(t):\n",
    "#        return 0\n",
    "#    if CB_EXCLUDE.search(t):\n",
    "#        return 0\n",
    "#    return 1\n",
    "\n",
    "df[\"EVENT_PRED_CB\"] = df.apply(\n",
    "    lambda r: int(r[\"EVENT_PRED\"] == 1 and cb_filter_soft(r[TEXT_COL]) == 1),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"EVENT_PRED:\", int(df[\"EVENT_PRED\"].sum()))\n",
    "print(\"EVENT_PRED_CB:\", int(df[\"EVENT_PRED_CB\"].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have reduced the wage/collective bargaining relevant articles to 1150 from the initial candidates of 1855."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, first we will implement some canonicalization (dropping suffixes, firm indicators etc) while also trying to fix broken encoding like \"i ş\" (this attempt will not work). Since we will be linking through employer/firm names, we must make sure that unwanted recurring tokens like union names, people names, any other junk and common Turkish words are not present. Although this list is not bad, as we will see, the issue of peope names are very, very hard to overcome in Evrensel since for some reason; they record their reporters inside the text itself instead of having them as authors. Since no comprehensive list of reporters can be found (and since weird tokenizations mix some parts of the reporters' names with a part of the text), this part was appended by hand through trial and error and observing the most common results (this is actually the case for all of the lists but is especially true for person terms.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "tqF1JX7BgHjI"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "\n",
    "LEGAL_RE = re.compile(\n",
    "    r\"\\b(a\\.?ş\\.?|aş|anonim|şirketi|şti|ltd|limited|inc|corp|co|holding|sanayi|ticaret|ve)\\b\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "GENERIC_TAIL_RE = re.compile(\n",
    "    r\"\\b(fabrika(sı|si|da|de|nda|nde)?|işletme(si|de|da|nde|nda)?|tesis(leri|de|da|nde|nda)?|işyeri(nde|ne|ni)?)\\b\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "def _fold_tr(s: str) -> str:\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "\n",
    "    s = s.replace(\"’\", \"'\").replace(\"`\", \"'\")\n",
    "\n",
    "\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "\n",
    "    s = re.sub(r\"\\b([a-zçğıöşü])\\s+([a-zçğıöşü])\\b\", r\"\\1\\2\", s)\n",
    "    s = re.sub(r\"\\b([a-zçğıöşü])\\s+([a-zçğıöşü])\\b\", r\"\\1\\2\", s)  # run twice\n",
    "\n",
    "    return s\n",
    "\n",
    "def canonical_employer(name: str) -> str:\n",
    "    s = _fold_tr(name)\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = re.sub(r\"[^\\w\\sçğıöşü0-9'-]\", \" \", s, flags=re.UNICODE)\n",
    "    s = LEGAL_RE.sub(\" \", s)\n",
    "    s = GENERIC_TAIL_RE.sub(\" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "UNION_TERMS = {\n",
    "    \"disk\", \"türk iş\", \"turk is\", \"hak iş\", \"kesk\",\n",
    "    \"türk metal\", \"metal iş\", \"birleşik metal iş\", \"birlesik metal is\",\n",
    "    \"genel iş\", \"petrol iş\", \"tek gıda iş\", \"tek gida is\",\n",
    "    \"tüm bel sen\", \"tum bel sen\", \"tüm bel-sen\", \"tum bel-sen\",\n",
    "    \"birtek sen\", \"birleşik metal\", \"tüm bel\",\"eğitim sen\", \"iş gebze\",\n",
    "    \"iş aliağa\",\n",
    "}\n",
    "PERSON_TERMS = {\n",
    "    \"hilal tok\", \"ramis sağlam\", \"ramis saglam\", \"özkan atar\", \"ozkan atar\",\n",
    "    \"hayrettin çakmak\", \"hayrettin cakmak\", \"cemil tugay\",\n",
    "    \"hasret gültekin kozan\", \"hasret gultekin kozan\",\n",
    "    \"sevda karaca\", \"iskender bayhan\", \"nazlıer çerik\",\n",
    "    \"andac aydın arıduru\", \"volkan pekal\", \"özer akdemir\",\n",
    "    \"duygu ayber gültekin\", \"dudu selçuk\", \"michael roberts\", \"çerik\",\n",
    "    \"murat özveri\", \"andaç aydın\", \"ayça pektaş\", \"cem turan\", \"mehmet güngör\",\n",
    "    \"recep tayyip\", \"emirhan durmaz\", \"ergün atalay\", \"ergun atalay\", \"vedat ışıkhan\"\n",
    "    \"mehmet türkmen\", \"aydın arıduru\", \"ercan gül\", \"duygu ayber\",\"aydın ariduru i stanbul\",\n",
    "    \"ali çeltek\", \"hasret gültekin\", \"gültekin kozan gebze\", \"arzu erkan\",\"ali rıza\",\n",
    "    \"eda aktaş\", \"halil i mrek\",\n",
    "\n",
    "}\n",
    "\n",
    "DROP_IF_CONTAINS = [\n",
    "    \"genel başkan\", \"genel baskan\", \"başkanı\", \"baskani\", \"genel sekreter\",\n",
    "    \"genel sekreteri\",\n",
    "    \"şube başkanı\", \"sube baskani\", \"başkan yardımcısı\", \"baskan yardimcisi\"\n",
    "    \"belediyesi\", \"organize sanayi\", \"osb\",\n",
    "    \"partisi\", \"chp\", \"akp\", \"mhp\", \"emep\",\n",
    "    \"servisi\", \"muhabir\", \"haber merkezi\", \"gazetesi\", \"ajans\",\n",
    "    # generic wage/strike words are NOT employers:\n",
    "    \"ücret\", \"ucret\", \"zam\", \"tis\", \"sözleşme\", \"sozlesme\", \"grev\", \"işçi\", \"isci\",\n",
    "     \"merhaba evrensel\", \"organize\", \"çalışma bakanlığı\", \"şube\", \"izmir şube\",\n",
    "    \"yönetim kurulu\", \"genel merkez\", \"genel merkezi\",\n",
    "    \"milletvekili\", \"bakan\", \"bakanı\", \"bakanlığı\", \"bakanligi\",\n",
    "     \"valilik\", \"tbmm\", \"buyuksehir\",\n",
    "    \"parti\", \"partisi\", \"dem parti\", \"i zmir\", \"di sk\", \"i şi\", \"il örgütü\",\"hizmet sektörü\",\n",
    "    \"i şçi sendi ka servi si\",\n",
    "]\n",
    "\n",
    "STARTER_BAD = {\n",
    "    \"yapılan\", \"yapilan\", \"önünde\", \"onunde\",\n",
    "    \"sosyal\", \"sabah\", \"aynı\", \"ayni\", \"en\", \"daha\", \"gece\",\n",
    "    \"kamu\", \"türkiye\", \"turkiye\", \"açıklamada\", \"aciklamada\",\n",
    "    \"geçtiğimiz\", \"gectigimiz\", \"günlerde\", \"gunlerde\", \"çok\", \"cok\"\n",
    "}\n",
    "\n",
    "UNION_TERMS = {_fold_tr(x) for x in UNION_TERMS}\n",
    "PERSON_TERMS = {_fold_tr(x) for x in PERSON_TERMS}\n",
    "DROP_IF_CONTAINS = [_fold_tr(x) for x in DROP_IF_CONTAINS]\n",
    "STARTER_BAD = {_fold_tr(x) for x in STARTER_BAD}\n",
    "\n",
    "def looks_like_union(k: str) -> bool:\n",
    "    kk = _fold_tr(k)\n",
    "    if not kk:\n",
    "        return False\n",
    "\n",
    "    kk_ns = kk.replace(\" \", \"\")\n",
    "\n",
    "    # 1) normal substring match (space robust)\n",
    "    for u in UNION_TERMS:\n",
    "        uu = _fold_tr(u)\n",
    "        if uu and (uu in kk or uu.replace(\" \", \"\") in kk_ns):\n",
    "            return True\n",
    "\n",
    "\n",
    "    toks = kk.split()\n",
    "    s = set(toks)\n",
    "\n",
    "    if (\"iş\" in s or \"is\" in s) and (\"genel\" in s or \"metal\" in s or \"petrol\" in s or \"özçelik\" in s or \"ozcelik\" in s):\n",
    "        return True\n",
    "\n",
    "\n",
    "    if (\"birleşik\" in s or \"birlesik\" in s) and (\"metal\" in s):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "def looks_like_person(k: str) -> bool:\n",
    "    kk = _fold_tr(k)\n",
    "    if not kk:\n",
    "        return False\n",
    "    kk_ns = kk.replace(\" \", \"\")\n",
    "    for p in PERSON_TERMS:\n",
    "        pp = _fold_tr(p)\n",
    "        if pp and (pp in kk or pp.replace(\" \", \"\") in kk_ns):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def looks_like_sentence_starter(k: str) -> bool:\n",
    "    kk = _fold_tr(k)\n",
    "    toks = kk.split()\n",
    "    if not toks:\n",
    "        return True\n",
    "    if toks[0] in STARTER_BAD:\n",
    "        return True\n",
    "    if len(toks) >= 2 and (toks[0] in STARTER_BAD or toks[1] in STARTER_BAD):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def looks_like_non_entity_phrase(k: str) -> bool:\n",
    "    kk = _fold_tr(k)\n",
    "    if not kk:\n",
    "        return True\n",
    "    if kk.endswith((\"deki\", \"daki\", \"teki\", \"taki\")):\n",
    "        return True\n",
    "    if any(w in kk for w in [\"açıklamada\", \"aciklamada\", \"saatlerinde\", \"günlerde\", \"gunlerde\"]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def should_drop_orgish(k: str) -> bool:\n",
    "    kk = _fold_tr(k)\n",
    "    if not kk or len(kk) < 3:\n",
    "        return True\n",
    "    if looks_like_union(kk) or looks_like_person(kk):\n",
    "        return True\n",
    "    if looks_like_sentence_starter(kk) or looks_like_non_entity_phrase(kk):\n",
    "        return True\n",
    "    for bad in DROP_IF_CONTAINS:\n",
    "        if bad in kk:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our lists, we can start the phrase mining process for firm names. We will build a candidate firm list by searching through the first 1200 characters of each text, using trigger and stop terms. Then, we will drop all the candidates that exist in our created lists from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZt1MLMBgxNL",
    "outputId": "308ed9a2-cc6a-4777-9bf3-2cf30cb94a9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate phrases: 2049\n",
      "firm_phrases kept: 951\n",
      "Top 30 candidates:\n",
      "299 - Genel İş\n",
      "270 - Birleşik Metal\n",
      "269 - Metal İş\n",
      "266 - Petrol İş\n",
      "262 - Birleşik Metal İş\n",
      "159 - Şube Başkanı\n",
      "137 - Özçelik İş\n",
      "123 - Organize Sanayi\n",
      "114 - Emek Partisi\n",
      "111 - BİRTEK SEN\n",
      "103 - İş İzmir\n",
      "102 - Genel Başkanı\n",
      "95 - Temel Conta\n",
      "89 - İş Genel\n",
      "80 - Genel İş İzmir\n",
      "78 - Türk İş\n",
      "77 - DİSK Genel\n",
      "70 - DİSK Genel İş\n",
      "69 - Sanayi Bölgesi\n",
      "68 - İş İstanbul\n",
      "65 - Özak Tekstil\n",
      "64 - Organize Sanayi Bölgesi\n",
      "63 - İzmir Büyükşehir\n",
      "63 - İstanbul Anadolu\n",
      "62 - Anadolu Yakası\n",
      "61 - İş İstanbul Anadolu\n",
      "60 - Genel İş İstanbul\n",
      "60 - İstanbul Anadolu Yakası\n",
      "60 - Genel İş İstanbul Anadolu\n",
      "59 - İş İstanbul Anadolu Yakası\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ALL_TRIG = re.compile(\n",
    "    r\"(grev(e)? çıktı|grev başladı|iş bırak(tı|ıyor)|üretim(i)? durdur|\"\n",
    "    r\"grevde|grev sürüyor|(\\d+)\\.? ?gün(ü)?nde|\"\n",
    "    r\"anlaşma sağlandı|grev bitti|grev sona erdi|protokol imzalandı|\"\n",
    "    r\"kabul edildi|reddedildi|imza(landı)?|uzlaş(ma)?|anlaş(ma)?|\"\n",
    "    r\"direnişi sürüyor|direniş(i)?)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "CONTENT_CHARS = 1200\n",
    "WIN_CHARS = 220\n",
    "MAX_WINS = 8\n",
    "MIN_FREQ = 3\n",
    "MAX_PHRASES = 5000\n",
    "NGRAM_MIN = 2\n",
    "NGRAM_MAX = 5\n",
    "\n",
    "STOP = set(\"\"\"\n",
    "ve veya ile için gibi üzere da de ki mi mı mu mü\n",
    "işçi işçileri grev grevi direniş direnişi eylem açıklama basın\n",
    "sendika sendikası işçilerden işçilerin mücadele talep sözleşme toplu iş emekçi\n",
    "emekçiler emekçileri örgütlü ama fakat lakin çünkü\n",
    "\"\"\".split())\n",
    "\n",
    "EMPLOYER_DROP_TERMS = [\n",
    "    \"iş sözleşmesi\", \"is sozlesmesi\", \"toplu iş sözleşme\", \"toplu is sozlesme\",\n",
    "    \"sözleşme\", \"sozlesme\", \"protokol\", \"arabulucu\", \"uzlaşma\", \"uzlasma\",\n",
    "    \"zam\", \"ücret\", \"ucret\", \"talepleri\", \"görüşme\", \"gorusme\",\n",
    "]\n",
    "\n",
    "def should_drop_employer_key(k: str) -> bool:\n",
    "    kk = _fold_tr(k)\n",
    "    if should_drop_orgish(kk):  \n",
    "        return True\n",
    "    for bad in EMPLOYER_DROP_TERMS:\n",
    "        if bad in kk:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def clean_for_phrase_mining(text: str) -> str:\n",
    "    text = (text or \"\")\n",
    "    text = text.replace(\"\\u00A0\", \" \").replace(\"’\",\"'\").replace(\"`\",\"'\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_simple(text: str):\n",
    "    return re.findall(r\"[A-Za-zÇĞİÖŞÜçğıöşü0-9]+\", text)\n",
    "\n",
    "def is_titlecase_like(tok: str) -> bool:\n",
    "    if len(tok) < 2:\n",
    "        return False\n",
    "    if tok.isupper() and len(tok) >= 2:\n",
    "        return True\n",
    "    return tok[0].isupper() and any(c.islower() for c in tok[1:])\n",
    "\n",
    "def trigger_windows_text(title: str, content: str, win_chars=WIN_CHARS, max_wins=MAX_WINS):\n",
    "    t = clean_for_phrase_mining(title)\n",
    "    c = clean_for_phrase_mining(content)[:CONTENT_CHARS]\n",
    "    text = f\"{t} {c}\".strip()\n",
    "    wins = []\n",
    "    for m in ALL_TRIG.finditer(text):\n",
    "        a = max(0, m.start() - win_chars)\n",
    "        b = min(len(text), m.end() + win_chars)\n",
    "        wins.append(text[a:b])\n",
    "        if len(wins) >= max_wins:\n",
    "            break\n",
    "    return wins\n",
    "\n",
    "phrase_counts = Counter()\n",
    "\n",
    "\n",
    "mask_mine = df[\"EVENT_PRED_CB\"] == 1\n",
    "\n",
    "for t, c in zip(df.loc[mask_mine, TITLE_COL].astype(str), df.loc[mask_mine, CONTENT_COL].astype(str)):\n",
    "    wins = trigger_windows_text(t, c)\n",
    "    if not wins:\n",
    "        continue\n",
    "    for w in wins:\n",
    "        toks = tokenize_simple(w)\n",
    "        flags = [is_titlecase_like(tok) for tok in toks]\n",
    "\n",
    "        i = 0\n",
    "        while i < len(toks):\n",
    "            if not flags[i]:\n",
    "                i += 1\n",
    "                continue\n",
    "            j = i\n",
    "            while j < len(toks) and flags[j]:\n",
    "                j += 1\n",
    "\n",
    "            span = toks[i:j]\n",
    "            for n in range(NGRAM_MIN, NGRAM_MAX + 1):\n",
    "                for k in range(0, len(span) - n + 1):\n",
    "                    ng = span[k:k+n]\n",
    "                    ng_l = [w.lower() for w in ng]\n",
    "                    if any(w in STOP for w in ng_l):\n",
    "                        continue\n",
    "                    if all(w.isdigit() for w in ng):\n",
    "                        continue\n",
    "                    phrase = \" \".join(ng)\n",
    "                    phrase_counts[phrase] += 1\n",
    "\n",
    "            i = j\n",
    "\n",
    "candidates = [(p, cnt) for p, cnt in phrase_counts.items() if cnt >= MIN_FREQ]\n",
    "candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def is_plausible_firm_phrase(p: str) -> bool:\n",
    "    k = canonical_employer(p)\n",
    "    if not k:\n",
    "        return False\n",
    "\n",
    "    if len(k.split()) < 2 or len(k) < 6:\n",
    "        return False\n",
    "\n",
    "    if looks_like_union(k) or looks_like_person(k) or should_drop_employer_key(k):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "firm_phrases = [p for p, cnt in candidates if is_plausible_firm_phrase(p)]\n",
    "\n",
    "\n",
    "print(\"Candidate phrases:\", len(candidates))\n",
    "print(\"firm_phrases kept:\", len(firm_phrases))\n",
    "print(\"Top 30 candidates:\")\n",
    "for p, cnt in candidates[:30]:\n",
    "    print(cnt, \"-\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 2049 candidate phrases to keeping 951 firm phrases, we have done quite well. Let us now try to go even further and use a Turkish spaCy pipeline to extract canonicalized org_keys from our articles and to filter our organization keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hInmDbj7hKvD",
    "outputId": "144f1836-ba20-4ad4-f5a3-aa348435d0ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs with >=1 ORG_KEY before: 4304\n",
      "Docs with >=1 ORG_KEY after : 4304\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp_ruler = spacy.blank(\"tr\")\n",
    "ruler = nlp_ruler.add_pipe(\"entity_ruler\")\n",
    "\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": p} for p in firm_phrases]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "def extract_org_keys_ruler(title: str, content: str):\n",
    "    txt = f\"{title} {content[:CONTENT_CHARS]}\"\n",
    "    doc = nlp_ruler(txt)\n",
    "    orgs = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ != \"ORG\":\n",
    "            continue\n",
    "        k = canonical_employer(ent.text)\n",
    "        if not k:\n",
    "            continue\n",
    "        orgs.append(k)\n",
    "    # dedup preserve order\n",
    "    return list(dict.fromkeys(orgs))\n",
    "\n",
    "df[\"ORG_KEYS\"] = [\n",
    "    extract_org_keys_ruler(t, c)\n",
    "    for t, c in zip(df[TITLE_COL].astype(str), df[CONTENT_COL].astype(str))\n",
    "]\n",
    "\n",
    "# filter\n",
    "def is_good_org_key(k: str) -> bool:\n",
    "    k2 = canonical_employer(k)\n",
    "    if not k2:\n",
    "        return False\n",
    "    if len(k2.split()) < 2:\n",
    "        return False\n",
    "    if len(k2) < 6:\n",
    "        return False\n",
    "    if should_drop_employer_key(k2):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "df[\"ORG_KEYS_FILTERED\"] = df[\"ORG_KEYS\"].apply(lambda ks: [canonical_employer(k) for k in (ks or []) if is_good_org_key(k)])\n",
    "\n",
    "print(\"Docs with >=1 ORG_KEY before:\", int((df[\"ORG_KEYS\"].apply(lambda x: len(x or [])) > 0).sum()))\n",
    "print(\"Docs with >=1 ORG_KEY after :\", int((df[\"ORG_KEYS_FILTERED\"].apply(lambda x: len(x or [])) > 0).sum()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No docs were deleted, but this is not necessarily a bad thing. It likely helped with cleaning inside the articles. We now build regular expression patterns to specifize employer and union keys seperately instead of the \"org_keys\" we had before, which included both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qX0z-es-hRLq",
    "outputId": "197c4beb-ea07-4c54-e154-af0c997f1eac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs with >=1 ORG_KEY before: 4304\n",
      "Docs with >=1 ORG_KEY after : 4304\n"
     ]
    }
   ],
   "source": [
    "P_FACILITY = re.compile(\n",
    "    r\"(?P<name>[A-ZÇĞİÖŞÜ][\\w’'-.]+(?:\\s+[A-ZÇĞİÖŞÜ][\\w’'-.]+){0,6})\\s+\"\n",
    "    r\"(fabrika(sı|sinda|sında|da|de|nda|nde)?|işyeri(nde|ne|ni)?|tesis(leri|de|da|nde|nda)?)\",\n",
    "    re.UNICODE\n",
    ")\n",
    "P_WORKERS = re.compile(\n",
    "    r\"(?P<name>[A-ZÇĞİÖŞÜ][\\w’'-.]+(?:\\s+[A-ZÇĞİÖŞÜ][\\w’'-.]+){0,6})\\s+işçi(leri|ler)?\",\n",
    "    re.UNICODE\n",
    ")\n",
    "\n",
    "def _as_list(x):\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return [str(i).strip() for i in x if str(i).strip()]\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    return [p.strip() for p in re.split(r\"[;|,]\\s*\", s) if p.strip()]\n",
    "\n",
    "def trigger_windows(text: str, window_chars: int = 180, max_wins: int = 6):\n",
    "    t = text or \"\"\n",
    "    wins = []\n",
    "    for m in ALL_TRIG.finditer(t):\n",
    "        a = max(0, m.start() - window_chars)\n",
    "        b = min(len(t), m.end() + window_chars)\n",
    "        wins.append(t[a:b])\n",
    "        if len(wins) >= max_wins:\n",
    "            break\n",
    "    return wins\n",
    "\n",
    "def extract_employer_candidates_from_text(title: str, content: str, org_list=None):\n",
    "    \"\"\"\n",
    "    Keep mined candidates only if they already exist in ORG_KEYS_FILTERED for the doc.\n",
    "    \"\"\"\n",
    "    txt = (str(title or \"\") + \" \" + str(content or \"\")).strip()\n",
    "    wins = trigger_windows(txt, window_chars=180, max_wins=6)\n",
    "\n",
    "    emp, uni = [], []\n",
    "    org_list = org_list or []\n",
    "    org_can = {_fold_tr(canonical_employer(o)) for o in org_list}\n",
    "\n",
    "    for w in wins:\n",
    "        for pat in (P_FACILITY, P_WORKERS):\n",
    "            for m in pat.finditer(w):\n",
    "                raw = m.group(\"name\").strip()\n",
    "                k = _fold_tr(canonical_employer(raw))\n",
    "                if not k:\n",
    "                    continue\n",
    "\n",
    "                if k not in org_can:\n",
    "                    continue\n",
    "\n",
    "                if looks_like_union(k):\n",
    "                    uni.append(k)\n",
    "                    continue\n",
    "                if looks_like_person(k):\n",
    "                    continue\n",
    "                if should_drop_employer_key(k):\n",
    "                    continue\n",
    "\n",
    "                emp.append(k)\n",
    "\n",
    "    def dedup(seq):\n",
    "        seen, out = set(), []\n",
    "        for x in seq:\n",
    "            if x and x not in seen:\n",
    "                seen.add(x)\n",
    "                out.append(x)\n",
    "        return out\n",
    "\n",
    "    return dedup(emp), dedup(uni)\n",
    "\n",
    "def build_employer_and_union_keys(df_in, org_col=\"ORG_KEYS_FILTERED\", max_emp_per_doc=3):\n",
    "    df_out = df_in.copy()\n",
    "    employer_keys, union_keys = [], []\n",
    "\n",
    "    for t, c, orgs in zip(\n",
    "        df_out[TITLE_COL].astype(str),\n",
    "        df_out[CONTENT_COL].astype(str),\n",
    "        df_out[org_col] if org_col in df_out.columns else [None] * len(df_out)\n",
    "    ):\n",
    "        org_list = _as_list(orgs)\n",
    "\n",
    "        emp_from_org, uni_from_org = [], []\n",
    "        for o in org_list:\n",
    "            k = _fold_tr(canonical_employer(o))\n",
    "            if not k:\n",
    "                continue\n",
    "            if looks_like_union(k):\n",
    "                uni_from_org.append(k)\n",
    "                continue\n",
    "            if looks_like_person(k):\n",
    "                continue\n",
    "            if should_drop_employer_key(k):\n",
    "                continue\n",
    "            emp_from_org.append(k)\n",
    "\n",
    "        emp_mined, uni_mined = extract_employer_candidates_from_text(t, c, org_list)\n",
    "\n",
    "        def dedup(seq):\n",
    "            seen, out = set(), []\n",
    "            for x in seq:\n",
    "                if x and x not in seen:\n",
    "                    seen.add(x)\n",
    "                    out.append(x)\n",
    "            return out\n",
    "\n",
    "        emp_all = dedup(emp_from_org + emp_mined)\n",
    "\n",
    "\n",
    "        emp_clean = []\n",
    "        for k in emp_all:\n",
    "            kk = canonical_employer(k)   \n",
    "            kk = _fold_tr(kk)            \n",
    "            if not kk:\n",
    "                continue\n",
    "            if looks_like_union(kk):\n",
    "                continue\n",
    "            if looks_like_person(kk):\n",
    "                continue\n",
    "            if should_drop_employer_key(kk):\n",
    "                continue\n",
    "            emp_clean.append(kk)\n",
    "        if not emp_clean and emp_from_org:\n",
    "               emp_clean = emp_from_org[:1]\n",
    "\n",
    "        emp = []\n",
    "        seen = set()\n",
    "        for x in emp_clean:\n",
    "            if x not in seen:\n",
    "                seen.add(x)\n",
    "                emp.append(x)\n",
    "\n",
    "        emp = emp[:max_emp_per_doc]\n",
    "\n",
    "        uni = dedup(uni_from_org + uni_mined)\n",
    "\n",
    "        employer_keys.append(emp)\n",
    "        union_keys.append(uni)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df_out[\"EMPLOYER_KEYS\"] = employer_keys\n",
    "    df_out[\"UNION_KEYS\"] = union_keys\n",
    "    return df_out\n",
    "\n",
    "df_linked = df.copy()\n",
    "df_linked = build_employer_and_union_keys(df_linked, org_col=\"ORG_KEYS_FILTERED\", max_emp_per_doc=3)\n",
    "\n",
    "\n",
    "print(\"Docs with >=1 ORG_KEY before:\", int((df[\"ORG_KEYS\"].apply(lambda x: len(x or [])) > 0).sum()))\n",
    "print(\"Docs with >=1 ORG_KEY after :\", int((df[\"ORG_KEYS_FILTERED\"].apply(lambda x: len(x or [])) > 0).sum()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o4ZPOGkG-TGi",
    "outputId": "8c8f985a-42ee-4753-8279-a9235778ee9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 EMPLOYER_KEYS:\n",
      "43 - büyükşehir belediyesi\n",
      "36 - temel conta\n",
      "29 - ge grid solutions\n",
      "27 - büyükşehir belediyesine\n",
      "25 - i stanbul anadolu yakası\n",
      "25 - karşıyaka belediyesi\n",
      "25 - buca belediyesi\n",
      "24 - i şçi sendi ka\n",
      "23 - iş sendikasının\n",
      "20 - yolbulan metal\n",
      "20 - hitachi energy\n",
      "17 - as plastik\n",
      "17 - schneider elektrik\n",
      "17 - gültekin kozan\n",
      "17 - i smail cem şimşek\n",
      "16 - sen genel\n",
      "15 - green transfo\n",
      "14 - metal sanayicileri\n",
      "14 - mehmet türkmen\n",
      "14 - büyükşehir belediyesinde\n",
      "13 - pamukkale üniversitesi\n",
      "13 - genel müdürlüğü\n",
      "12 - arıtaş kriyojenik\n",
      "12 - iş yeri temsilcisi\n",
      "12 - toros tarım\n",
      "12 - kartal belediyesi\n",
      "11 - iş sendikasına\n",
      "11 - schneider electric\n",
      "11 - iş sendikasında\n",
      "11 - buca belediyesi i mar\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "for ks in df_linked.loc[df_linked[\"EVENT_PRED_CB\"]==1, \"EMPLOYER_KEYS\"]:\n",
    "    for k in (ks or []):\n",
    "        c[k] += 1\n",
    "\n",
    "print(\"Top 30 EMPLOYER_KEYS:\")\n",
    "for k,v in c.most_common(30):\n",
    "    print(v, \"-\", k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are our top employer keys. Even though quite a lot of actual firms are present (temel conta, green transfo, hitachi energy); there are still person names (mehmet türkmen), canonicalization issues (schneider electric and schneider elektrik), nonsense unions (iş sendikasında) and broken tokenizations ( i şçi sendi ka) we could not get rid of.\n",
    "\n",
    "Now, we will load a multilingual sentence-transformer model to produce contextual embeddings to check for semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ZakmqCA8hRnG"
   },
   "outputs": [],
   "source": [
    "EMB_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "emb_tokenizer = AutoTokenizer.from_pretrained(EMB_MODEL_NAME)\n",
    "emb_model = AutoModel.from_pretrained(EMB_MODEL_NAME).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "emb_model.eval()\n",
    "\n",
    "def mean_pool(last_hidden, attention_mask):\n",
    "    mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "    summed = (last_hidden * mask).sum(dim=1)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_texts(texts, batch_size=32, max_len=256):\n",
    "    dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = emb_tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(dev)\n",
    "        out = emb_model(**enc)\n",
    "        v = mean_pool(out.last_hidden_state, enc[\"attention_mask\"])\n",
    "        v = torch.nn.functional.normalize(v, p=2, dim=1)\n",
    "        vecs.append(v.detach().cpu().numpy())\n",
    "    return np.vstack(vecs)\n",
    "\n",
    "def connected_components(n, edges):\n",
    "    parent = list(range(n))\n",
    "    rank = [0]*n\n",
    "\n",
    "    def find(x):\n",
    "        while parent[x] != x:\n",
    "            parent[x] = parent[parent[x]]\n",
    "            x = parent[x]\n",
    "        return x\n",
    "\n",
    "    def union(a, b):\n",
    "        ra, rb = find(a), find(b)\n",
    "        if ra == rb:\n",
    "            return\n",
    "        if rank[ra] < rank[rb]:\n",
    "            parent[ra] = rb\n",
    "        elif rank[ra] > rank[rb]:\n",
    "            parent[rb] = ra\n",
    "        else:\n",
    "            parent[rb] = ra\n",
    "            rank[ra] += 1\n",
    "\n",
    "    for a, b in edges:\n",
    "        union(a, b)\n",
    "\n",
    "    comps = defaultdict(list)\n",
    "    for i in range(n):\n",
    "        comps[find(i)].append(i)\n",
    "    return list(comps.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will build rare tokens for titles for extra linking evidence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UKkg2N6rhV1k"
   },
   "outputs": [],
   "source": [
    "def build_rare_tokens(titles, min_len=4, max_df=0.05):\n",
    "    toks_all = []\n",
    "    for t in titles:\n",
    "        toks = re.findall(r\"[A-Za-zÇĞİÖŞÜçğıöşü]+\", str(t).lower())\n",
    "        toks = [x for x in toks if len(x) >= min_len]\n",
    "        toks_all.extend(set(toks))\n",
    "    c = Counter(toks_all)\n",
    "    n = len(titles)\n",
    "    return {tok for tok, dfreq in c.items() if (dfreq / max(n,1)) <= max_df}\n",
    "\n",
    "rare_tokens = build_rare_tokens(df_linked[TITLE_COL].tolist(), min_len=4, max_df=0.05)\n",
    "\n",
    "def rare_title_tokens(title, min_len=4):\n",
    "    toks = re.findall(r\"[A-Za-zÇĞİÖŞÜçğıöşü]+\", str(title).lower())\n",
    "    return {t for t in toks if len(t) >= min_len and t in rare_tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time for linking. We will use a safe time decay (to not completely miss strikes lasting a long time and hardly ever being reported on), employer buckets, similarity thresholds, rare token overlaps and trying for minimal clustering. The thresholds for our variables have been picked through many trial and errors in our re-runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sqBCp4oshZ2k",
    "outputId": "c892d824-9028-4952-ccec-df8ec9093fdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant n: 1150\n",
      "Unique employer keys among relevant: 441\n",
      "Edges after employer bucket (A): 613\n",
      "Pred-relevant (CB/Wage): 1150\n",
      "Unique EVENT_ID among predicted relevant: 550\n",
      "count    550.000000\n",
      "mean       2.090909\n",
      "std        8.820767\n",
      "min        1.000000\n",
      "25%        1.000000\n",
      "50%        1.000000\n",
      "75%        1.000000\n",
      "max      158.000000\n",
      "dtype: float64\n",
      "count    550.000000\n",
      "mean       2.090909\n",
      "std        8.820767\n",
      "min        1.000000\n",
      "25%        1.000000\n",
      "50%        1.000000\n",
      "75%        1.000000\n",
      "max      158.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def _safe_days_diff(d1, d2):\n",
    "    if pd.isna(d1) or pd.isna(d2):\n",
    "        return None\n",
    "    try:\n",
    "        return abs((pd.to_datetime(d1) - pd.to_datetime(d2)).days)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _time_decay(days_diff, tau_days=25.0):\n",
    "    if days_diff is None:\n",
    "        return 1.0\n",
    "    return math.exp(-float(days_diff) / float(tau_days))\n",
    "\n",
    "def assign_event_ids_hybrid(\n",
    "    df_in,\n",
    "    rel_flag_col=\"EVENT_PRED_CB\",\n",
    "    date_col=DATE_COL,\n",
    "    employer_col=\"EMPLOYER_KEYS\",\n",
    "    sim_short=0.92,\n",
    "    sim_emp=0.935,\n",
    "    tau_days=80.0,\n",
    "    max_rel=6000,\n",
    "    EMP_MAX_BUCKET=40,\n",
    "    TITLE_OVERLAP_K=2,\n",
    "    MIN_CLUSTER_SIZE=2,\n",
    "    DROP_EMPTY_EMPLOYER_CLUSTERS=True,\n",
    "):\n",
    "    df_out = df_in.copy()\n",
    "\n",
    "    if EVENT_ID_COL not in df_out.columns:\n",
    "        df_out[EVENT_ID_COL] = np.nan\n",
    "\n",
    "    df_out[EVENT_ID_COL] = df_out[EVENT_ID_COL].where(df_out[EVENT_ID_COL].notna(), np.nan)\n",
    "    df_out[EVENT_ID_COL] = df_out[EVENT_ID_COL].apply(lambda x: str(x).strip() if not pd.isna(x) else np.nan)\n",
    "\n",
    "    rel = df_out[df_out[rel_flag_col] == 1].copy()\n",
    "    if len(rel) == 0:\n",
    "        print(f\"No rows where {rel_flag_col} == 1.\")\n",
    "        return df_out\n",
    "    if len(rel) > max_rel:\n",
    "        print(f\"Too many relevant rows ({len(rel)}).\")\n",
    "        return df_out\n",
    "\n",
    "    rel[\"_has_date\"] = rel[date_col].notna()\n",
    "    rel = rel.sort_values(by=[date_col, \"_has_date\"], ascending=[True, False])\n",
    "\n",
    "    idx = list(rel.index)\n",
    "    n = len(idx)\n",
    "\n",
    "    E = encode_texts(rel[TEXT_COL].tolist(), batch_size=32 if torch.cuda.is_available() else 8, max_len=256)\n",
    "\n",
    "    title_tok_sets = [rare_title_tokens(t) for t in rel[TITLE_COL].tolist()]\n",
    "    rel_dates = rel[date_col].tolist()\n",
    "\n",
    "    emp_to_pos = defaultdict(list)\n",
    "    rel_emp_lists = rel[employer_col].tolist() if employer_col in rel.columns else [None] * n\n",
    "    for pos, keys in enumerate(rel_emp_lists):\n",
    "        for k in (keys or []):\n",
    "            if k:\n",
    "                emp_to_pos[k].append(pos)\n",
    "\n",
    "    edges = set()\n",
    "\n",
    "    print(\"Relevant n:\", n)\n",
    "    print(\"Unique employer keys among relevant:\", len(emp_to_pos))\n",
    "\n",
    "\n",
    "    EMP_MAX_GAP_DAYS = 60          \n",
    "    EMP_SIM_MIN = 0.75             \n",
    "    USE_EMP_SIM = True             \n",
    "\n",
    "    for emp, positions in emp_to_pos.items():\n",
    "        if looks_like_union(emp) or should_drop_employer_key(emp):\n",
    "            continue\n",
    "        if len(emp) < 3:\n",
    "            continue\n",
    "        if len(positions) <= 1 or len(positions) > EMP_MAX_BUCKET:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        def _pos_date(p):\n",
    "            d = rel_dates[p]\n",
    "            try:\n",
    "                return pd.to_datetime(d) if not pd.isna(d) else pd.Timestamp.min\n",
    "            except Exception:\n",
    "                return pd.Timestamp.min\n",
    "\n",
    "        positions = sorted(positions, key=_pos_date)\n",
    "\n",
    "        for a in range(1, len(positions)):\n",
    "            i = positions[a - 1]\n",
    "            j = positions[a]\n",
    "            dd = _safe_days_diff(rel_dates[i], rel_dates[j])\n",
    "            if dd is None or dd > EMP_MAX_GAP_DAYS:\n",
    "                continue\n",
    "\n",
    "            if USE_EMP_SIM:\n",
    "                base_sim = float(np.dot(E[i], E[j]))\n",
    "                if base_sim < EMP_SIM_MIN:\n",
    "                    continue\n",
    "\n",
    "            edges.add((min(i, j), max(i, j)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    edges_after_A = len(edges)\n",
    "    print(\"Edges after employer bucket (A):\", edges_after_A)\n",
    "\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "\n",
    "            ei = rel_emp_lists[i] if i < len(rel_emp_lists) else None\n",
    "            ej = rel_emp_lists[j] if j < len(rel_emp_lists) else None\n",
    "            if (not ei) and (not ej):\n",
    "                continue\n",
    "\n",
    "            base_sim = float(np.dot(E[i], E[j]))\n",
    "            dd = _safe_days_diff(rel_dates[i], rel_dates[j])\n",
    "            sim = base_sim * _time_decay(dd, tau_days=tau_days)\n",
    "            if sim >= sim_short:\n",
    "                if len(title_tok_sets[i] & title_tok_sets[j]) >= 1:\n",
    "                  edges.add((min(i, j), max(i, j)))\n",
    "\n",
    "\n",
    "    comps = connected_components(n, list(edges))\n",
    "    clusters = [[idx[pos] for pos in comp] for comp in comps]\n",
    "\n",
    "\n",
    "    filtered = []\n",
    "    for cluster in clusters:\n",
    "        if len(cluster) < MIN_CLUSTER_SIZE:\n",
    "            continue\n",
    "        if DROP_EMPTY_EMPLOYER_CLUSTERS and employer_col in df_out.columns:\n",
    "            emp_nonempty = df_out.loc[cluster, employer_col].apply(lambda x: isinstance(x, list) and len(x) > 0).sum()\n",
    "            if int(emp_nonempty) == 0:\n",
    "                continue\n",
    "        filtered.append(cluster)\n",
    "    clusters = filtered\n",
    "\n",
    "\n",
    "    new_counter = 1\n",
    "    for cluster in clusters:\n",
    "        existing = df_out.loc[cluster, EVENT_ID_COL].dropna()\n",
    "        if len(existing) > 0:\n",
    "            chosen = existing.value_counts().idxmax()\n",
    "        else:\n",
    "            chosen = f\"EV{new_counter:06d}\"\n",
    "            new_counter += 1\n",
    "        df_out.loc[cluster, EVENT_ID_COL] = chosen\n",
    "    mask_rel = df_out[rel_flag_col] == 1\n",
    "    unassigned = df_out.index[mask_rel & df_out[EVENT_ID_COL].isna()].tolist()\n",
    "\n",
    "\n",
    "    used = set(df_out.loc[mask_rel, EVENT_ID_COL].dropna().astype(str))\n",
    "\n",
    "    while f\"EV{new_counter:06d}\" in used:\n",
    "        new_counter += 1\n",
    "\n",
    "    for ix in unassigned:\n",
    "        eid = f\"EV{new_counter:06d}\"\n",
    "        while eid in used:\n",
    "            new_counter += 1\n",
    "            eid = f\"EV{new_counter:06d}\"\n",
    "        df_out.at[ix, EVENT_ID_COL] = eid\n",
    "        used.add(eid)\n",
    "        new_counter += 1\n",
    "\n",
    "\n",
    "    return df_out\n",
    "\n",
    "df_linked = assign_event_ids_hybrid(\n",
    "    df_linked,\n",
    "    rel_flag_col=\"EVENT_PRED_CB\",\n",
    "    date_col=DATE_COL,\n",
    "    employer_col=\"EMPLOYER_KEYS\",\n",
    "    sim_short=0.85,\n",
    "    sim_emp=0.87,\n",
    "    tau_days=100.0,\n",
    "    EMP_MAX_BUCKET=40,\n",
    "    TITLE_OVERLAP_K=1,\n",
    "    MIN_CLUSTER_SIZE=2,\n",
    "    DROP_EMPTY_EMPLOYER_CLUSTERS=True\n",
    ")\n",
    "\n",
    "mask_rel = df_linked[\"EVENT_PRED_CB\"] == 1\n",
    "print(\"Pred-relevant (CB/Wage):\", int(mask_rel.sum()))\n",
    "print(\"Unique EVENT_ID among predicted relevant:\",\n",
    "      int(df_linked.loc[mask_rel, EVENT_ID_COL].nunique()))\n",
    "print(df_linked.loc[mask_rel].groupby(EVENT_ID_COL).size().describe())\n",
    "print(df_linked.loc[mask_rel].groupby(\"EVENT_ID\").size().describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1150 articles, spread over 550 clusters with 441 unique emplyer keys. Although there is still one mega cluster, this will be solved when we firm split now. This way, the events (articles) containing more than one firm will not fall under the same EVENT_ID now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RI-sBVtyhc4g",
    "outputId": "1624a12a-df2f-4a85-8244-181ca85f42e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2094209805.py:37: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'E0001_i şçi sendi ka' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_out.at[idx_row, \"EVENT_ID_FIRM\"] = f\"{ev}_{primary}\"\n"
     ]
    }
   ],
   "source": [
    "def pick_primary_employer(emps):\n",
    "    good = []\n",
    "    for e in (emps or []):\n",
    "        k = canonical_employer(e)\n",
    "        if not k or should_drop_employer_key(k) or looks_like_union(k) or looks_like_person(k):\n",
    "            continue\n",
    "        toks = k.split()\n",
    "        score = 0\n",
    "        score += 2 if len(toks) >= 2 else 0\n",
    "        score += 1 if len(k) >= 10 else 0\n",
    "        score += min(len(k), 30) / 30.0  \n",
    "        good.append((score, k))\n",
    "    if not good:\n",
    "        return None\n",
    "    good.sort(reverse=True)\n",
    "    return good[0][1]\n",
    "\n",
    "\n",
    "\n",
    "def firm_split_event_ids(df_in, rel_flag_col=\"EVENT_PRED_CB\", event_col=\"EVENT_ID\", employer_col=\"EMPLOYER_KEYS\"):\n",
    "    df_out = df_in.copy()\n",
    "    df_out[\"EVENT_ID_FIRM\"] = np.nan\n",
    "\n",
    "    rel = df_out[df_out[rel_flag_col] == 1].copy()\n",
    "    for ev, g in rel.groupby(event_col):\n",
    "        # assign per employer; if no employer, keep one bucket\n",
    "        for idx_row in g.index:\n",
    "            emps = df_out.at[idx_row, employer_col]\n",
    "            if not isinstance(emps, list) or len(emps) == 0:\n",
    "                df_out.at[idx_row, \"EVENT_ID_FIRM\"] = f\"{ev}_F0\"\n",
    "            else:\n",
    "                # pick first employer as primary key for firm-level split\n",
    "                primary = pick_primary_employer(emps)\n",
    "                if primary is None:\n",
    "                     df_out.at[idx_row, \"EVENT_ID_FIRM\"] = f\"{ev}_F0\"\n",
    "                else:\n",
    "                    df_out.at[idx_row, \"EVENT_ID_FIRM\"] = f\"{ev}_{primary}\"\n",
    "\n",
    "\n",
    "    return df_out\n",
    "\n",
    "df_linked = firm_split_event_ids(df_linked, rel_flag_col=\"EVENT_PRED_CB\", event_col=\"EVENT_ID\", employer_col=\"EMPLOYER_KEYS\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will just save these outputs to an excel file. Notice how the firm-split values are still not showing. This is not the case in the excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8g-La75hgTF",
    "outputId": "1cdcbfc8-30d2-4784-e3fd-f8115867b0d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing among export-critical cols: []\n",
      "Pred-relevant (CB/Wage): 1150\n",
      "Unique EVENT_ID among predicted relevant: 550\n",
      "count    550.000000\n",
      "mean       2.090909\n",
      "std        8.820767\n",
      "min        1.000000\n",
      "25%        1.000000\n",
      "50%        1.000000\n",
      "75%        1.000000\n",
      "max      158.000000\n",
      "dtype: float64\n",
      "count    550.000000\n",
      "mean       2.090909\n",
      "std        8.820767\n",
      "min        1.000000\n",
      "25%        1.000000\n",
      "50%        1.000000\n",
      "75%        1.000000\n",
      "max      158.000000\n",
      "dtype: float64\n",
      "Saved: firm_level_strikes_trigger_window.xlsx\n"
     ]
    }
   ],
   "source": [
    "def list_to_str(x):\n",
    "    if isinstance(x, list):\n",
    "        return \"; \".join([str(i) for i in x if str(i).strip()])\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return str(x)\n",
    "\n",
    "def flatten_unique_keys(series_of_lists):\n",
    "    s = set()\n",
    "    for ks in series_of_lists:\n",
    "        if ks is None or (isinstance(ks, float) and pd.isna(ks)):\n",
    "            continue\n",
    "        if isinstance(ks, list):\n",
    "            for k in ks:\n",
    "                k = str(k).strip()\n",
    "                if k:\n",
    "                    s.add(k)\n",
    "        else:\n",
    "            for k in str(ks).split(\";\"):\n",
    "                k = k.strip()\n",
    "                if k:\n",
    "                    s.add(k)\n",
    "    return \"; \".join(sorted(s))\n",
    "\n",
    "mask = df_linked[\"EVENT_PRED_CB\"] == 1\n",
    "\n",
    "events_firm = (\n",
    "    df_linked.loc[mask]\n",
    "    .groupby(\"EVENT_ID_FIRM\", dropna=False)\n",
    "    .agg(\n",
    "        start=(DATE_COL, \"min\"),\n",
    "        end=(DATE_COL, \"max\"),\n",
    "        duration=(DATE_COL, lambda x: (pd.to_datetime(x).max() - pd.to_datetime(x).min()).days + 1 if x.notna().any() else \"\"),\n",
    "        n_articles=(TITLE_COL, \"count\"),\n",
    "        firms=(\"ORG_KEYS_FILTERED\", flatten_unique_keys),\n",
    "        employers=(\"EMPLOYER_KEYS\", flatten_unique_keys),\n",
    "        unions=(\"UNION_KEYS\", flatten_unique_keys),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "for c in [\"start\", \"end\"]:\n",
    "    events_firm[c] = pd.to_datetime(events_firm[c], errors=\"coerce\")\n",
    "\n",
    "\n",
    "if \"EVENT_PROB\" not in df_linked.columns:\n",
    "    if \"EVENT_PROB\" in df.columns:\n",
    "        df_linked = df_linked.join(df[[\"EVENT_PROB\"]], how=\"left\")\n",
    "    else:\n",
    "        df_linked[\"EVENT_PROB\"] = np.nan\n",
    "\n",
    "for col in [\"EVENT_PRED\", \"EVENT_PRED_CB\"]:\n",
    "    if col not in df_linked.columns:\n",
    "        if col in df.columns:\n",
    "            df_linked = df_linked.join(df[[col]], how=\"left\")\n",
    "        else:\n",
    "            df_linked[col] = 0\n",
    "\n",
    "\n",
    "if MANUAL_COL not in df_linked.columns:\n",
    "    df_linked[MANUAL_COL] = np.nan\n",
    "\n",
    "\n",
    "print(\"Missing among export-critical cols:\",\n",
    "      [c for c in [\"EVENT_PROB\",\"EVENT_PRED\",\"EVENT_PRED_CB\",MANUAL_COL] if c not in df_linked.columns])\n",
    "\n",
    "\n",
    "wb = Workbook()\n",
    "ws1 = wb.active\n",
    "ws1.title = \"Firm_Level_Strikes\"\n",
    "ws1.append(list(events_firm.columns))\n",
    "\n",
    "for _, row in events_firm.iterrows():\n",
    "    ws1.append([list_to_str(v) for v in row.tolist()])\n",
    "\n",
    "ws2 = wb.create_sheet(\"Articles_By_Firm_Event\")\n",
    "cols = [\"EVENT_ID_FIRM\", DATE_COL, TITLE_COL, LINK, \"ORG_KEYS_FILTERED\", \"EMPLOYER_KEYS\", \"UNION_KEYS\", \"EVENT_ID\", MANUAL_COL, \"EVENT_PRED\", \"EVENT_PRED_CB\", \"EVENT_PROB\"]\n",
    "ws2.append(cols)\n",
    "\n",
    "tmp = df_linked.loc[mask, cols].copy()\n",
    "tmp[DATE_COL] = pd.to_datetime(tmp[DATE_COL], errors=\"coerce\")\n",
    "for col in [\"ORG_KEYS_FILTERED\", \"EMPLOYER_KEYS\", \"UNION_KEYS\"]:\n",
    "    tmp[col] = tmp[col].apply(list_to_str)\n",
    "\n",
    "for _, r in tmp.iterrows():\n",
    "    ws2.append([list_to_str(v) for v in r.tolist()])\n",
    "\n",
    "path = \"firm_level_strikes_trigger_window.xlsx\"\n",
    "wb.save(path)\n",
    "print(\"Saved:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSIjGPNahkK_",
    "outputId": "2526970b-f3df-4e47-a2a4-66568e7545f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 ORG_KEYS_FILTERED:\n",
      "44 - büyükşehir belediyesi\n",
      "38 - temel conta\n",
      "37 - arıtaş kriyojenik\n",
      "32 - green transfo\n",
      "31 - ge grid solutions\n",
      "31 - i stanbul anadolu yakası\n",
      "27 - büyükşehir belediyesine\n",
      "25 - i şçi sendi ka\n",
      "25 - karşıyaka belediyesi\n",
      "25 - buca belediyesi\n",
      "24 - hitachi energy\n",
      "23 - iş sendikasının\n",
      "21 - yolbulan metal\n",
      "21 - schneider elektrik\n",
      "21 - schneider electric\n",
      "20 - gültekin kozan\n",
      "18 - i smail cem şimşek\n",
      "17 - as plastik\n",
      "17 - mehmet türkmen\n",
      "16 - sen genel\n",
      "15 - kartal belediyesi\n",
      "14 - iş yeri temsilcisi\n",
      "14 - metal sanayicileri\n",
      "14 - maltepe belediyesi\n",
      "14 - büyükşehir belediyesinde\n",
      "13 - iş sendikasına\n",
      "13 - toros tarım\n",
      "13 - pamukkale üniversitesi\n",
      "13 - genel müdürlüğü\n",
      "12 - seyit aslan\n",
      "\n",
      "Top 30 EMPLOYER_KEYS:\n",
      "43 - büyükşehir belediyesi\n",
      "36 - temel conta\n",
      "29 - ge grid solutions\n",
      "27 - büyükşehir belediyesine\n",
      "25 - i stanbul anadolu yakası\n",
      "25 - karşıyaka belediyesi\n",
      "25 - buca belediyesi\n",
      "24 - i şçi sendi ka\n",
      "23 - iş sendikasının\n",
      "20 - yolbulan metal\n",
      "20 - hitachi energy\n",
      "17 - as plastik\n",
      "17 - schneider elektrik\n",
      "17 - gültekin kozan\n",
      "17 - i smail cem şimşek\n",
      "16 - sen genel\n",
      "15 - green transfo\n",
      "14 - metal sanayicileri\n",
      "14 - mehmet türkmen\n",
      "14 - büyükşehir belediyesinde\n",
      "13 - pamukkale üniversitesi\n",
      "13 - genel müdürlüğü\n",
      "12 - arıtaş kriyojenik\n",
      "12 - iş yeri temsilcisi\n",
      "12 - toros tarım\n",
      "12 - kartal belediyesi\n",
      "11 - iş sendikasına\n",
      "11 - schneider electric\n",
      "11 - iş sendikasında\n",
      "11 - buca belediyesi i mar\n"
     ]
    }
   ],
   "source": [
    "def top30_listcol(df_in, col, mask):\n",
    "    c = Counter()\n",
    "    for ks in df_in.loc[mask, col]:\n",
    "        if ks is None or (isinstance(ks, float) and pd.isna(ks)):\n",
    "            continue\n",
    "        if isinstance(ks, list):\n",
    "            for k in ks:\n",
    "                k = str(k).strip()\n",
    "                if k:\n",
    "                    c[k] += 1\n",
    "        else:\n",
    "            for k in str(ks).split(\";\"):\n",
    "                k = k.strip()\n",
    "                if k:\n",
    "                    c[k] += 1\n",
    "    return c.most_common(30)\n",
    "\n",
    "print(\"Top 30 ORG_KEYS_FILTERED:\")\n",
    "for k, v in top30_listcol(df_linked, \"ORG_KEYS_FILTERED\", df_linked[\"EVENT_PRED_CB\"]==1):\n",
    "    print(v, \"-\", k)\n",
    "\n",
    "print(\"\\nTop 30 EMPLOYER_KEYS:\")\n",
    "for k, v in top30_listcol(df_linked, \"EMPLOYER_KEYS\", df_linked[\"EVENT_PRED_CB\"]==1):\n",
    "    print(v, \"-\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will check how our detection performs with respect to our manually labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3yAQy4zMKGP",
    "outputId": "aaa9b3f3-817b-4edc-ec94-d56440d9e482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual CB/Wage: 160\n",
      "Predicted CB/Wage: 1855\n",
      "Overlap: 155\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mask_manual = df_linked[MANUAL_COL] == 1         \n",
    "mask_pred = df_linked[\"EVENT_PRED\"] == 1\n",
    "\n",
    "print(\"Manual CB/Wage:\", mask_manual.sum())\n",
    "print(\"Predicted CB/Wage:\", mask_pred.sum())\n",
    "print(\"Overlap:\", (mask_manual & mask_pred).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rlPkB9QxMQDz",
    "outputId": "8ed11787-6a59-46d1-dcef-9ccedfc77a46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.96875\n",
      "Precision: 0.08355795148247978\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall:\", (mask_manual & mask_pred).sum() / mask_manual.sum())\n",
    "print(\"Precision:\", (mask_manual & mask_pred).sum() / mask_pred.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, for wage/collective bargaining detection; we have extremely high recall but low precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6wKfzqKPsOw",
    "outputId": "0f44756f-588f-4252-b5e1-d78f99880cc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual: 160\n",
      "\n",
      "MODEL ONLY (EVENT_PRED)\n",
      "Pred: 1855\n",
      "Overlap: 155\n",
      "\n",
      "MODEL + RULE GATE (EVENT_PRED_CB)\n",
      "Pred: 1150\n",
      "Overlap: 116\n"
     ]
    }
   ],
   "source": [
    "mask_manual = df_linked[MANUAL_COL] == 1\n",
    "\n",
    "mask_pred_raw = df_linked[\"EVENT_PRED\"] == 1\n",
    "mask_pred_cb   = df_linked[\"EVENT_PRED_CB\"] == 1     \n",
    "\n",
    "print(\"Manual:\", mask_manual.sum())\n",
    "\n",
    "print(\"\\nMODEL ONLY (EVENT_PRED)\")\n",
    "print(\"Pred:\", mask_pred_raw.sum())\n",
    "print(\"Overlap:\", (mask_manual & mask_pred_raw).sum())\n",
    "\n",
    "print(\"\\nMODEL + RULE GATE (EVENT_PRED_CB)\")\n",
    "print(\"Pred:\", mask_pred_cb.sum())\n",
    "print(\"Overlap:\", (mask_manual & mask_pred_cb).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hfmbNS-SQEDm",
    "outputId": "ea4621f4-5d5d-4e49-e558-ecfe3a4ab8c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cb_reason\n",
      "HIT_EXCLUDE    25\n",
      "PASS           14\n",
      "FAIL_KEEP       5\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"display(fn[cols]\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"Sosyal Yard\\u0131mla\\u015fma Vakf\\u0131 i\\u015f\\u00e7ilerinin grevi s\\u00fcr\\u00fcyor\",\n          \"\\u0130\\u015f\\u00e7ilerin tepkisine ra\\u011fmen patron makine ta\\u015f\\u0131d\\u0131\",\n          \"\\u2018Sefalet zamm\\u0131n\\u0131 kabul etmiyoruz\\u2019\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"https://www.evrensel.net/haber/568242/sosyal-yardimlasma-vakfi-iscilerinin-grevi-devam-ediyor\",\n          \"https://www.evrensel.net/haber/538165/temel-contada-grev-kiriciligi-iscilerin-tepkisine-ragmen-patron-makine-tasidi\",\n          \"https://www.evrensel.net/haber/560325/dokuz-eylul-ve-ege-universitesi-iscileri-is-birakti-sefalet-zammini-kabul-etmiyoruz\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cb_reason\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"HIT_EXCLUDE\",\n          \"PASS\",\n          \"FAIL_KEEP\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-3610ea32-7a45-45ca-b5e6-873c7af9bdd8\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>cb_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>İslahiye OSB'de Key Mensucat işçilerinin diren...</td>\n",
       "      <td>https://www.evrensel.net/haber/510329/islahiye...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>1 Mayıs’ı birliğimizi, kararlılığımızı gösterm...</td>\n",
       "      <td>https://www.evrensel.net/haber/517237/mega-pol...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>Grevdeki Mersen işçileri Fransız Konsolosluğu ...</td>\n",
       "      <td>https://www.evrensel.net/haber/519324/grevdeki...</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Eğitim Sen grevdeki Purmo işçilerini ziyaret etti</td>\n",
       "      <td>https://www.evrensel.net/haber/520439/egitim-s...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>Grevdeki Kristal Yağ işçilerinden birlik çağrı...</td>\n",
       "      <td>https://www.evrensel.net/haber/522517/grevdeki...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>Kristal Yağ işçilerinin grevi bir ayı geride b...</td>\n",
       "      <td>https://www.evrensel.net/haber/522871/kristal-...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Eti Krom eylemi 14’üncü günü geride bıraktı</td>\n",
       "      <td>https://www.evrensel.net/haber/523213/eti-krom...</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>İzBB’de çalışan emekçiler belediyenin zamsız t...</td>\n",
       "      <td>https://www.evrensel.net/haber/524499/izbbde-c...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Hatay'da grevdeki Yolbulan ve Befesa işçilerin...</td>\n",
       "      <td>https://www.evrensel.net/haber/525461/hatayda-...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>EMEP'li Bayhan, CarrefourSA işçilerinin direni...</td>\n",
       "      <td>https://www.evrensel.net/haber/526679/emepli-b...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>EMEP’li Karaca, Yolbulan işçilerine seslendi: ...</td>\n",
       "      <td>https://www.evrensel.net/haber/526602/emepli-k...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>As Plastik’te grev sürüyor: Devlet yanımızda d...</td>\n",
       "      <td>https://www.evrensel.net/haber/529499/as-plast...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>BMİS Genel Başkanı Atar: Grev kararlılıkla sür...</td>\n",
       "      <td>https://www.evrensel.net/haber/537080/bmis-gen...</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>Araştırmacı ve akademisyenlerden ortak açıklam...</td>\n",
       "      <td>https://www.evrensel.net/haber/537071/arastirm...</td>\n",
       "      <td>FAIL_KEEP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>“İşçiler tek adamın yasağıyla grev hakkından v...</td>\n",
       "      <td>https://www.evrensel.net/haber/537271/emek-par...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>İşçilerin tepkisine rağmen patron makine taşıdı</td>\n",
       "      <td>https://www.evrensel.net/haber/538165/temel-co...</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>Sirene Marine’de sandıktan grev kararı çıktı</td>\n",
       "      <td>https://www.evrensel.net/haber/540115/sirene-m...</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>EMEP Bursa İl Örgütü: Sirena Marine işçilerini...</td>\n",
       "      <td>https://www.evrensel.net/haber/540645/emep-bur...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>Özkaplan Halı'da direniş kazanımla sonlandı</td>\n",
       "      <td>https://www.evrensel.net/haber/542516/antepte-...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Toros Tarım işçileri: Ay sonunu getiremiyoruz,...</td>\n",
       "      <td>https://www.evrensel.net/haber/555009/toros-ta...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>GTÜ öğrencilerinden işçi grevlerine destek</td>\n",
       "      <td>https://www.evrensel.net/haber/555649/gtu-ogre...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>Cemil Tugay: Yalan söylemeyi bırakın</td>\n",
       "      <td>https://www.evrensel.net/haber/556020/izbb-bas...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>Kamu işçileri bugün de ülkenin kaderinde etkil...</td>\n",
       "      <td>https://www.evrensel.net/haber/559918/binlerce...</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7177</th>\n",
       "      <td>‘Sefalet zammını kabul etmiyoruz’</td>\n",
       "      <td>https://www.evrensel.net/haber/560325/dokuz-ey...</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7182</th>\n",
       "      <td>Grevdeki TPI işçileri 6 Temmuz direnişinin yıl...</td>\n",
       "      <td>https://www.evrensel.net/haber/560280/grevdeki...</td>\n",
       "      <td>FAIL_KEEP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7415</th>\n",
       "      <td>İşçiler greve devam dedi</td>\n",
       "      <td>https://www.evrensel.net/haber/562684/tpida-pa...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7942</th>\n",
       "      <td>“Fidan incir fidanı mıymış?”</td>\n",
       "      <td>https://www.evrensel.net/haber/567703/isciler-...</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>Sosyal Yardımlaşma Vakfı işçilerinin grevi sür...</td>\n",
       "      <td>https://www.evrensel.net/haber/568242/sosyal-y...</td>\n",
       "      <td>HIT_EXCLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8063</th>\n",
       "      <td>Kütahya Şeker Fabrikasında grev sona erdi</td>\n",
       "      <td>https://www.evrensel.net/haber/569366/kutahya-...</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8190</th>\n",
       "      <td>Ücretleri gasbedilen Buca Belediyesi işçilerin...</td>\n",
       "      <td>https://www.evrensel.net/haber/571376/ucretler...</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3610ea32-7a45-45ca-b5e6-873c7af9bdd8')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-3610ea32-7a45-45ca-b5e6-873c7af9bdd8 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-3610ea32-7a45-45ca-b5e6-873c7af9bdd8');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "276   İslahiye OSB'de Key Mensucat işçilerinin diren...   \n",
       "283   1 Mayıs’ı birliğimizi, kararlılığımızı gösterm...   \n",
       "287   Grevdeki Mersen işçileri Fransız Konsolosluğu ...   \n",
       "297   Eğitim Sen grevdeki Purmo işçilerini ziyaret etti   \n",
       "302   Grevdeki Kristal Yağ işçilerinden birlik çağrı...   \n",
       "305   Kristal Yağ işçilerinin grevi bir ayı geride b...   \n",
       "309         Eti Krom eylemi 14’üncü günü geride bıraktı   \n",
       "313   İzBB’de çalışan emekçiler belediyenin zamsız t...   \n",
       "318   Hatay'da grevdeki Yolbulan ve Befesa işçilerin...   \n",
       "322   EMEP'li Bayhan, CarrefourSA işçilerinin direni...   \n",
       "324   EMEP’li Karaca, Yolbulan işçilerine seslendi: ...   \n",
       "329   As Plastik’te grev sürüyor: Devlet yanımızda d...   \n",
       "340   BMİS Genel Başkanı Atar: Grev kararlılıkla sür...   \n",
       "341   Araştırmacı ve akademisyenlerden ortak açıklam...   \n",
       "344   “İşçiler tek adamın yasağıyla grev hakkından v...   \n",
       "347     İşçilerin tepkisine rağmen patron makine taşıdı   \n",
       "349        Sirene Marine’de sandıktan grev kararı çıktı   \n",
       "350   EMEP Bursa İl Örgütü: Sirena Marine işçilerini...   \n",
       "359         Özkaplan Halı'da direniş kazanımla sonlandı   \n",
       "365   Toros Tarım işçileri: Ay sonunu getiremiyoruz,...   \n",
       "373          GTÜ öğrencilerinden işçi grevlerine destek   \n",
       "375                Cemil Tugay: Yalan söylemeyi bırakın   \n",
       "7168  Kamu işçileri bugün de ülkenin kaderinde etkil...   \n",
       "7177                  ‘Sefalet zammını kabul etmiyoruz’   \n",
       "7182  Grevdeki TPI işçileri 6 Temmuz direnişinin yıl...   \n",
       "7415                           İşçiler greve devam dedi   \n",
       "7942                       “Fidan incir fidanı mıymış?”   \n",
       "7998  Sosyal Yardımlaşma Vakfı işçilerinin grevi sür...   \n",
       "8063          Kütahya Şeker Fabrikasında grev sona erdi   \n",
       "8190  Ücretleri gasbedilen Buca Belediyesi işçilerin...   \n",
       "\n",
       "                                                   link    cb_reason  \n",
       "276   https://www.evrensel.net/haber/510329/islahiye...  HIT_EXCLUDE  \n",
       "283   https://www.evrensel.net/haber/517237/mega-pol...  HIT_EXCLUDE  \n",
       "287   https://www.evrensel.net/haber/519324/grevdeki...         PASS  \n",
       "297   https://www.evrensel.net/haber/520439/egitim-s...  HIT_EXCLUDE  \n",
       "302   https://www.evrensel.net/haber/522517/grevdeki...  HIT_EXCLUDE  \n",
       "305   https://www.evrensel.net/haber/522871/kristal-...  HIT_EXCLUDE  \n",
       "309   https://www.evrensel.net/haber/523213/eti-krom...         PASS  \n",
       "313   https://www.evrensel.net/haber/524499/izbbde-c...  HIT_EXCLUDE  \n",
       "318   https://www.evrensel.net/haber/525461/hatayda-...  HIT_EXCLUDE  \n",
       "322   https://www.evrensel.net/haber/526679/emepli-b...  HIT_EXCLUDE  \n",
       "324   https://www.evrensel.net/haber/526602/emepli-k...  HIT_EXCLUDE  \n",
       "329   https://www.evrensel.net/haber/529499/as-plast...  HIT_EXCLUDE  \n",
       "340   https://www.evrensel.net/haber/537080/bmis-gen...         PASS  \n",
       "341   https://www.evrensel.net/haber/537071/arastirm...    FAIL_KEEP  \n",
       "344   https://www.evrensel.net/haber/537271/emek-par...  HIT_EXCLUDE  \n",
       "347   https://www.evrensel.net/haber/538165/temel-co...         PASS  \n",
       "349   https://www.evrensel.net/haber/540115/sirene-m...         PASS  \n",
       "350   https://www.evrensel.net/haber/540645/emep-bur...  HIT_EXCLUDE  \n",
       "359   https://www.evrensel.net/haber/542516/antepte-...  HIT_EXCLUDE  \n",
       "365   https://www.evrensel.net/haber/555009/toros-ta...  HIT_EXCLUDE  \n",
       "373   https://www.evrensel.net/haber/555649/gtu-ogre...  HIT_EXCLUDE  \n",
       "375   https://www.evrensel.net/haber/556020/izbb-bas...  HIT_EXCLUDE  \n",
       "7168  https://www.evrensel.net/haber/559918/binlerce...         PASS  \n",
       "7177  https://www.evrensel.net/haber/560325/dokuz-ey...         PASS  \n",
       "7182  https://www.evrensel.net/haber/560280/grevdeki...    FAIL_KEEP  \n",
       "7415  https://www.evrensel.net/haber/562684/tpida-pa...  HIT_EXCLUDE  \n",
       "7942  https://www.evrensel.net/haber/567703/isciler-...         PASS  \n",
       "7998  https://www.evrensel.net/haber/568242/sosyal-y...  HIT_EXCLUDE  \n",
       "8063  https://www.evrensel.net/haber/569366/kutahya-...         PASS  \n",
       "8190  https://www.evrensel.net/haber/571376/ucretler...         PASS  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cb_filter_reason(text: str):\n",
    "    t = str(text or \"\")\n",
    "    if not CB_KEEP.search(t):\n",
    "        return \"FAIL_KEEP\"\n",
    "    if CB_EXCLUDE.search(t):\n",
    "        return \"HIT_EXCLUDE\"\n",
    "    return \"PASS\"\n",
    "\n",
    "\n",
    "fn = df_linked[(df_linked[MANUAL_COL]==1) & (df_linked[\"EVENT_PRED_CB\"]==0)].copy()\n",
    "\n",
    "fn[\"cb_reason\"] = fn[TEXT_COL].apply(cb_filter_reason)\n",
    "\n",
    "print(fn[\"cb_reason\"].value_counts())\n",
    "\n",
    "\n",
    "cols = [TITLE_COL, LINK, \"cb_reason\"]\n",
    "display(fn[cols].head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CvC5j6x-R0EM",
    "outputId": "3703959a-ba28-453e-bf68-3138cb6b9a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual: 160\n",
      "Pred_CB: 1150\n",
      "Overlap: 116\n",
      "Recall: 0.725\n",
      "Precision: 0.10086956521739131\n"
     ]
    }
   ],
   "source": [
    "mask_manual = df_linked[MANUAL_COL] == 1\n",
    "mask_pred_cb = df_linked[\"EVENT_PRED_CB\"] == 1\n",
    "\n",
    "print(\"Manual:\", mask_manual.sum())\n",
    "print(\"Pred_CB:\", mask_pred_cb.sum())\n",
    "print(\"Overlap:\", (mask_manual & mask_pred_cb).sum())\n",
    "print(\"Recall:\", (mask_manual & mask_pred_cb).sum() / mask_manual.sum())\n",
    "print(\"Precision:\", (mask_manual & mask_pred_cb).sum() / mask_pred_cb.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems our blocking rules are still too strong, increasing precision slightly for a relatively big fall in recall. However, this is a necessary evil. In the end, i was left with 816 unique events encompassing 1151 articles. Manually, i was able to identify 134 unique events. So, our linking was still pretty subpar. For these unique events, i manually labeled \"EVENT_ID\",\"FIRM\",\"WORKER_TOTAL\",\"WORKER_STRIKE\", \"STRIKING_WORKER_RATIO\",\"SECTOR\",\"STRIKE_DURATION (DAYS)\", \"UNION_PRESENCE\",\"LEGAL_STRIKE\" and \"RESULT\". Both files can be seen at \"firm_level_strikes_trigger_window(20)\" and \"Evrensel_2024_2025_found_strikes.xlsx\" respectively. The columns are self explanatory ( \"UNION_PRESENCE\",\"LEGAL_STRIKE\" and \"RESULT\" are binary variables) except when \"RESULT\" is blank, it means that the strike is ongoing/result unknown, and in this case \"STRIKE_DURATION (DAYS)\" indicates the strike duration up until the last date the strike was mentioned.\n",
    "\n",
    "Now, we will use this file for quantitative analysis.\n",
    "\n",
    "Go to file \"discrete_time_hazard_strikes_FIN.ipynb\" for the next part. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
