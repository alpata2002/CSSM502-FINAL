{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111948b8-9a48-4ead-8075-17cb42f9d76f",
   "metadata": {},
   "source": [
    "In this final project, i will attempt to measure the effects variables like duration and the legality of strikes have on both strike survival (in a survival analysis manner) and strike success in wage/collective bargaining based strike events, from strike events extracted from newspaper articles. So, this project will consist of three parts: Web scraping, event extraction & detection and survival analysis as well as logit analysis for success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a809b5-30da-4a58-8e25-3a88524215b0",
   "metadata": {},
   "source": [
    "PART 1 - WEBSCRAPING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10050443-cf6e-4d9f-80df-bd5012a8318f",
   "metadata": {},
   "source": [
    "I will be scraping all of the article contents, titles, website links and dates of articles in the Evrensel İşçi-Sendika category from the start of 2024 to December. This is a pretty standard webscraping script, which crawls through each page and first scrapes the title, data, link of each article. Then, it opens each article link and extracts the contents. One interesting thing is that i had to use the playwright api for scraping since it gets around Evrensel's firewall protection really well by \"acting\" as a real user. The downside is that webscraping in this manner takes a long time. After some light text cleaning, i export the results to an excel file.\n",
    "\n",
    "One small thing: i had to delete the outputs since the file became too big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c9b65-6dff-4731-9725-e194c8c2ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "BASE_URL = \"https://www.evrensel.net/kategori/2/isci-sendika\"\n",
    "START_PAGE = 403\n",
    "END_PAGE = 22 \n",
    "\n",
    "MAX_CONCURRENT_ARTICLES = 6\n",
    "ARTICLE_TIMEOUT_MS = 30_000\n",
    "\n",
    "\n",
    "def make_category_url(page_num: int) -> str:\n",
    " \n",
    "    return f\"{BASE_URL}/s/{page_num}#haberler\"\n",
    "\n",
    "\n",
    "async def scrape_category_page(page, url):\n",
    "    \"\"\"\n",
    "    Scrapes one Evrensel category page.\n",
    "    Returns list of dicts with title, date, link.\n",
    "    \"\"\"\n",
    "    await page.goto(url, wait_until=\"networkidle\")\n",
    "\n",
    "    news_items = []\n",
    "    items = page.locator(\"div.kategoriHaberler > span\")\n",
    "    count = await items.count()\n",
    "\n",
    "    for i in range(count):\n",
    "        item = items.nth(i)\n",
    "\n",
    "        title_loc = item.locator(\"div.title\")\n",
    "        date_loc = item.locator(\"div.tarih\")\n",
    "        link_loc = item.locator(\"a\")\n",
    "\n",
    "        if await title_loc.count() == 0 or await date_loc.count() == 0 or await link_loc.count() == 0:\n",
    "            continue\n",
    "\n",
    "        title = (await title_loc.inner_text()).strip()\n",
    "        date = (await date_loc.inner_text()).strip()\n",
    "\n",
    "        link = await link_loc.get_attribute(\"href\")\n",
    "        if not link:\n",
    "            continue\n",
    "        if link.startswith(\"/\"):\n",
    "            link = \"https://www.evrensel.net\" + link\n",
    "\n",
    "        news_items.append({\"title\": title, \"date\": date, \"link\": link})\n",
    "\n",
    "    return news_items\n",
    "\n",
    "\n",
    "\n",
    "async def extract_article_text(page):\n",
    "    \"\"\"\n",
    "    Extracts clean Evrensel article text:\n",
    "    ONLY grabs paragraphs inside div.news-content\n",
    "    \"\"\"\n",
    "    paragraphs = page.locator(\"div.news-content p\")\n",
    "    count = await paragraphs.count()\n",
    "\n",
    "    if count == 0:\n",
    "        return None\n",
    "\n",
    "    parts = []\n",
    "    for i in range(count):\n",
    "        t = (await paragraphs.nth(i).inner_text()).strip()\n",
    "        if t:\n",
    "            parts.append(t)\n",
    "\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "async def scrape_article(context, url, semaphore):\n",
    "    async with semaphore:\n",
    "        page = await context.new_page()\n",
    "        try:\n",
    "            await page.goto(url, wait_until=\"domcontentloaded\", timeout=ARTICLE_TIMEOUT_MS)\n",
    "            await page.wait_for_selector(\"div.news-content\", timeout=10_000)\n",
    "\n",
    "            content = await extract_article_text(page)\n",
    "            return {\"link\": url, \"content\": content}\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"link\": url, \"content\": None, \"error\": str(e)}\n",
    "\n",
    "        finally:\n",
    "            await page.close()\n",
    "\n",
    "\n",
    "\n",
    "async def crawl_evrensel():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "\n",
    "\n",
    "        all_articles = []\n",
    "        for page_num in range(START_PAGE, END_PAGE - 1, -1):\n",
    "            url = make_category_url(page_num)\n",
    "            print(f\"Scraping category page {page_num} → {url}\")\n",
    "\n",
    "            rows = await scrape_category_page(page, url)\n",
    "\n",
    "            if not rows:\n",
    "                print(f\"0 items on page {page_num}. Stopping early.\")\n",
    "                break\n",
    "\n",
    "            all_articles.extend(rows)\n",
    "\n",
    "\n",
    "        seen = set()\n",
    "        deduped = []\n",
    "        for r in all_articles:\n",
    "            if r[\"link\"] not in seen:\n",
    "                seen.add(r[\"link\"])\n",
    "                deduped.append(r)\n",
    "        all_articles = deduped\n",
    "\n",
    "   \n",
    "        semaphore = asyncio.Semaphore(MAX_CONCURRENT_ARTICLES)\n",
    "        links = [row[\"link\"] for row in all_articles]\n",
    "\n",
    "        print(f\"\\nScraping contents for {len(links)} articles...\\n\")\n",
    "\n",
    "        tasks = [scrape_article(context, link, semaphore) for link in links]\n",
    "        article_texts = await asyncio.gather(*tasks)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "\n",
    "    df_meta = pd.DataFrame(all_articles)\n",
    "    df_text = pd.DataFrame(article_texts)\n",
    "    df = df_meta.merge(df_text, on=\"link\", how=\"left\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = await crawl_evrensel()\n",
    "df.head(), len(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f91ed0d-452d-488a-9203-34146a4e40e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"date\"] = df[\"date\"].str.replace(\"\\n\", \" \", regex=False).str.strip()\n",
    "\n",
    "\n",
    "df[\"date\"] = df[\"date\"].str.replace(\n",
    "    \"Güncelleme\",\n",
    "    \"<span style='color:red; font-weight:bold;'>Güncelleme</span>\",\n",
    "    regex=False\n",
    ")\n",
    "\n",
    "# 3. Render HTML in DataFrame\n",
    "from IPython.display import HTML, display\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.html.use_mathjax', False)\n",
    "\n",
    "display(HTML(df.to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25dcb13-07a7-49f4-9935-0feb87b31ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_date(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    \n",
    "bb\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    \n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "  \n",
    "    text = text.replace(\"Güncelleme:\", \"— Güncelleme:\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "df[\"date\"] = df[\"date\"].apply(clean_date)\n",
    "\n",
    "\n",
    "df.to_csv(\"evrensel_isci_sendika_2024_dec2025_clean_fin.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75524b9-ab1c-45d5-ad61-189ba4880ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_date(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    \n",
    "\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    \n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "  \n",
    "    text = text.replace(\"Güncelleme:\", \"— Güncelleme:\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "df[\"date\"] = df[\"date\"].apply(clean_date)\n",
    "\n",
    "\n",
    "df.to_csv(\"evrensel_isci_sendika_2024_dec2025_clean_fin.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18000781-dc87-490a-863a-36a00e28bbef",
   "metadata": {},
   "source": [
    "Go to file \"detection_extraction_evrensel_FINFIN.ipynb\" for the next part. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
